{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dryad2dataverse - translate, transfer and track \u00b6 \u00b6 Introduction \u00b6 dryad2dataverse is an oddly specific Python package/library which comes with the highly-sought after console application which allows easier transfer of metadata and data from a Dryad data repository (ie, https://datadryad.org ) to a Dataverse repository. The app is probably what you want \u00b6 If you\u2019re interested in moving Dryad studies to Dataverse, being notified on changes and largely automating everything, the dryadd application can do all of these things without requiring any knowledge of Python or programming. All you need is basic command line knowledge and a platform with enough storage to (temporarily) hold the contents of a few Dryad records. Quick install \u00b6 pip install dryad2dataverse or, if you want to stay clear of anything related to a command line, download a compiled version of the migrator dryadd only for your computer system at the releases page . Note that binary releases are not available for all platforms, and if you want the most recent version you\u2019re better off using pip . I know how to program in Python. What does this do? \u00b6 With these tools it\u2019s possible to: a) Serialize Dryad metadata to Dataverse JSON b) Transfer Dryad studies to Dataverse without any knowledge of the somewhat complex Dataverse API c) Monitor changes in status So why would I need this? \u00b6 You are a researcher and you wish to deposit via API into Dataverse repository. You can use the tools with this package to that without needing to understand the APIs and formats of two separate platforms. You\u2019ve used Dryad, but the Dataverse JSON and API is unfamiliar and complex . You want to write your Dryad JSON and have it convert automatically to Dataverse\u2019s much more complex one. Your institution has researchers who have deposited data into Dryad and you wish to copy them into the Dataverse repository which contains the bulk of your institution\u2019s research data (for example, the Dataverse repository at https://borealisdata.ca ). And on top of that, you don\u2019t want to keep checking to see if there were any updates, so you wish to automate the process. Basic usage \u00b6 Converting JSON \u00b6 >>> #Convert Dryad JSON to Dataverse JSON and save to a file >>> import dryad2dataverse.serializer >>> i_heart_dryad = dryad2dataverse.serializer.Serializer('doi:10.5061/dryad.2rbnzs7jp') >>> with open('dataverse_json.json', 'w') as f: f.write(f'{i_heart_dryad.dvJson}') >>> #Or just view it this way in a Python session >>> i_heard_dryad.dvJson Transferring data \u00b6 Note: a number of variables must be set [correctly] for this to work, such as your target dataverse. This example continues with the Serializer instance above. >>> import dryad2dataverse.transfer >>> dv = dryad2dataverse.transfer.Transfer(i_heart_dryad) >>> # Files must first be downloaded; there is no direct transfer >>> dv.download_files() >>> # 'dryad' is the short name of the target dataverse >>> # Yours may be different >>> # First, create the study metadata >>> dv.upload_study(targetDv='dryad') >>> # Then upload the files >>> dv.upload_files() Change monitoring \u00b6 Because monitoring the status of something over time requires persistence, the dryad2dataverse.monitor.Monitor object uses an SQLite3 database, which has the enormous advantage of being a single file that is portable between systems. This allows monitoring without laborious database configuration on a host system, and updates can be run on any system that has sufficient storage space to act as an intermediary between Dryad and Dataverse. This is quite a simple database, as the documentation on its structure shows. If you need to change systems just swap the database to the new system. In theory you could run it from a Raspberry Pi Zero that you have in a desk drawer, although that may not be the wisest idea. Maybe use your cell phone. Monitoring changes requires both the Serializer and Transfer objects from above. >>> # Create the Monitor instance >>> monitor = dryad2dataverse.monitor.Monitor() >>> # Check status of your serializer object >>> monitor.status(i_heart_dryad) {'status': 'new', 'dvpid': None} >>> # imagine, now that i_still_heart_dryad is a study >>> # that was uploaded previously >>> monitor.status(i_still_heart_dryad) {'status': 'unchanged', 'dvpid': 'doi:99.99999/FK2/FAKER'} >>> #Check the difference in files >>> monitor.diff_files(i_still_heart_dryad) {} >>> # After the transfer dv above: >>> monitor.update(transfer) >>> # And then, to make your life easier, update the last time you checked Dryad >>> monitor.set_timestamp() That\u2019s great! I\u2019m going to use this for my very important data for which I have no backup. \u00b6 The dryad2dataverse library is free and open source, released under the MIT license. It\u2019s also not written by anyone with a degree in computer science, so as the MIT license says: Software is provided \"as is\", without warranty of any kind","title":"Overview"},{"location":"#dryad2dataverse-translate-transfer-and-track","text":"","title":"dryad2dataverse - translate, transfer and track"},{"location":"#_1","text":"","title":""},{"location":"#introduction","text":"dryad2dataverse is an oddly specific Python package/library which comes with the highly-sought after console application which allows easier transfer of metadata and data from a Dryad data repository (ie, https://datadryad.org ) to a Dataverse repository.","title":"Introduction"},{"location":"#the-app-is-probably-what-you-want","text":"If you\u2019re interested in moving Dryad studies to Dataverse, being notified on changes and largely automating everything, the dryadd application can do all of these things without requiring any knowledge of Python or programming. All you need is basic command line knowledge and a platform with enough storage to (temporarily) hold the contents of a few Dryad records.","title":"The app is probably what you want"},{"location":"#quick-install","text":"pip install dryad2dataverse or, if you want to stay clear of anything related to a command line, download a compiled version of the migrator dryadd only for your computer system at the releases page . Note that binary releases are not available for all platforms, and if you want the most recent version you\u2019re better off using pip .","title":"Quick install"},{"location":"#i-know-how-to-program-in-python-what-does-this-do","text":"With these tools it\u2019s possible to: a) Serialize Dryad metadata to Dataverse JSON b) Transfer Dryad studies to Dataverse without any knowledge of the somewhat complex Dataverse API c) Monitor changes in status","title":"I know how to program in Python. What does this do?"},{"location":"#so-why-would-i-need-this","text":"You are a researcher and you wish to deposit via API into Dataverse repository. You can use the tools with this package to that without needing to understand the APIs and formats of two separate platforms. You\u2019ve used Dryad, but the Dataverse JSON and API is unfamiliar and complex . You want to write your Dryad JSON and have it convert automatically to Dataverse\u2019s much more complex one. Your institution has researchers who have deposited data into Dryad and you wish to copy them into the Dataverse repository which contains the bulk of your institution\u2019s research data (for example, the Dataverse repository at https://borealisdata.ca ). And on top of that, you don\u2019t want to keep checking to see if there were any updates, so you wish to automate the process.","title":"So why would I need this?"},{"location":"#basic-usage","text":"","title":"Basic usage"},{"location":"#converting-json","text":">>> #Convert Dryad JSON to Dataverse JSON and save to a file >>> import dryad2dataverse.serializer >>> i_heart_dryad = dryad2dataverse.serializer.Serializer('doi:10.5061/dryad.2rbnzs7jp') >>> with open('dataverse_json.json', 'w') as f: f.write(f'{i_heart_dryad.dvJson}') >>> #Or just view it this way in a Python session >>> i_heard_dryad.dvJson","title":"Converting JSON"},{"location":"#transferring-data","text":"Note: a number of variables must be set [correctly] for this to work, such as your target dataverse. This example continues with the Serializer instance above. >>> import dryad2dataverse.transfer >>> dv = dryad2dataverse.transfer.Transfer(i_heart_dryad) >>> # Files must first be downloaded; there is no direct transfer >>> dv.download_files() >>> # 'dryad' is the short name of the target dataverse >>> # Yours may be different >>> # First, create the study metadata >>> dv.upload_study(targetDv='dryad') >>> # Then upload the files >>> dv.upload_files()","title":"Transferring data"},{"location":"#change-monitoring","text":"Because monitoring the status of something over time requires persistence, the dryad2dataverse.monitor.Monitor object uses an SQLite3 database, which has the enormous advantage of being a single file that is portable between systems. This allows monitoring without laborious database configuration on a host system, and updates can be run on any system that has sufficient storage space to act as an intermediary between Dryad and Dataverse. This is quite a simple database, as the documentation on its structure shows. If you need to change systems just swap the database to the new system. In theory you could run it from a Raspberry Pi Zero that you have in a desk drawer, although that may not be the wisest idea. Maybe use your cell phone. Monitoring changes requires both the Serializer and Transfer objects from above. >>> # Create the Monitor instance >>> monitor = dryad2dataverse.monitor.Monitor() >>> # Check status of your serializer object >>> monitor.status(i_heart_dryad) {'status': 'new', 'dvpid': None} >>> # imagine, now that i_still_heart_dryad is a study >>> # that was uploaded previously >>> monitor.status(i_still_heart_dryad) {'status': 'unchanged', 'dvpid': 'doi:99.99999/FK2/FAKER'} >>> #Check the difference in files >>> monitor.diff_files(i_still_heart_dryad) {} >>> # After the transfer dv above: >>> monitor.update(transfer) >>> # And then, to make your life easier, update the last time you checked Dryad >>> monitor.set_timestamp()","title":"Change monitoring"},{"location":"#thats-great-im-going-to-use-this-for-my-very-important-data-for-which-i-have-no-backup","text":"The dryad2dataverse library is free and open source, released under the MIT license. It\u2019s also not written by anyone with a degree in computer science, so as the MIT license says: Software is provided \"as is\", without warranty of any kind","title":"That&rsquo;s great! I&rsquo;m going to use this for my very important data for which I have no backup."},{"location":"api_reference/","text":"API Reference \u00b6 dryad2dataverse \u00b6 Dryad to Dataverse utilities. No modules are loaded by default, so >>> import dryad2dataverse will work, but will have no effect. Modules included: dryad2dataverse.constants : \u201cConstants\u201d for all modules. URLs, API keys, etc are all here. dryad2dataverse.serializer : Download and serialize Dryad JSON to Dataverse JSON. dryad2dataverse.transfer : metadata and file transfer utilities. dryad2dataverse.monitor : Monitoring and database tools for maintaining a pipeline to Dataverse without unnecessary downloading and file duplication. dryad2dataverse.exceptions : Custom exceptions. dryad2dataverse.serializer \u00b6 Serializes Dryad study JSON to Dataverse JSON, as well as producing associated file information. Serializer \u00b6 Serializes Dryad JSON to Dataverse JSON Source code in src/dryad2dataverse/serializer.py class Serializer(): ''' Serializes Dryad JSON to Dataverse JSON ''' CC0='''<p> <img src=\"https://licensebuttons.net/p/zero/1.0/88x31.png\" title=\"Creative Commons CC0 1.0 Universal Public Domain Dedication. \" style=\"display:none\" onload=\"this.style.display='inline'\" /> <a href=\"http://creativecommons.org/publicdomain/zero/1.0\" title=\"Creative Commons CC0 1.0 Universal Public Domain Dedication. \" target=\"_blank\">CC0 1.0</a> </p>''' def __init__(self, doi): ''' Creates Dryad study metadata instance. Parameters ---------- doi : str DOI of Dryad study. Required for downloading. eg: 'doi:10.5061/dryad.2rbnzs7jp' ''' self.doi = doi self._dryadJson = None self._fileJson = None self._dvJson = None #Serializer objects will be assigned a Dataverse study PID #if dryad2Dataverse.transfer.Transfer() is instantiated self.dvpid = None self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=constants.RETRY_STRATEGY)) LOGGER.debug('Creating Serializer instance object') def fetch_record(self, url=None, timeout=45): ''' Fetches Dryad study record JSON from Dryad V2 API at https://datadryad.org/api/v2/datasets/. Saves to self._dryadJson. Querying Serializer.dryadJson will call this function automatically. Parameters ---------- url : str Dryad instance base URL (eg: 'https://datadryad.org'). timeout : int Timeout in seconds. Default 45. ''' if not url: url = constants.DRYURL try: headers = {'accept':'application/json', 'Content-Type':'application/json'} headers.update(USER_AGENT) doiClean = urllib.parse.quote(self.doi, safe='') resp = self.session.get(f'{url}/api/v2/datasets/{doiClean}', headers=headers, timeout=timeout) resp.raise_for_status() self._dryadJson = resp.json() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('URL error for: %s', url) LOGGER.exception(err) raise @property def id(self): ''' Returns Dryad unique *database* ID, not the DOI. Where the original Dryad JSON is dryadJson, it's the integer trailing portion of: `self.dryadJson['_links']['stash:version']['href']` ''' href = self.dryadJson['_links']['stash:version']['href'] index = href.rfind('/') + 1 return int(href[index:]) @property def dryadJson(self): ''' Returns Dryad study JSON. Will call Serializer.fetch_record() if no JSON is present. ''' if not self._dryadJson: self.fetch_record() return self._dryadJson @dryadJson.setter def dryadJson(self, value=None): ''' Fetches Dryad JSON from Dryad website if not supplied. If supplying it, make sure it's correct or you will run into trouble with processing later. Parameters ---------- value : dict Dryad JSON. ''' if value: self._dryadJson = value else: self.fetch_record() @property def embargo(self)->bool: ''' Check embargo status. Returns boolean True if embargoed. ''' if self.dryadJson.get('curationStatus') == 'Embargoed': return True return False @property def dvJson(self): ''' Returns Dataverse study JSON as dict. ''' self._assemble_json() return self._dvJson @property def fileJson(self, timeout=45): ''' Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated, so the return consists of a list of dicts, one per page. Parameters ---------- timeout : int Request timeout in seconds. ''' if not self._fileJson: try: self._fileJson = [] headers = {'accept':'application/json', 'Content-Type':'application/json'} headers.update(USER_AGENT) fileList = self.session.get(f'{constants.DRYURL}/api/v2/versions/{self.id}/files', headers=headers, timeout=timeout) fileList.raise_for_status() #total = fileList.json()['total'] #Not needed lastPage = fileList.json()['_links']['last']['href'] pages = int(lastPage[lastPage.rfind('=')+1:]) self._fileJson.append(fileList.json()) for i in range(2, pages+1): fileCont = self.session.get(f'{constants.DRYURL}/api/v2' f'/versions/{self.id}/files?page={i}', headers=headers, timeout=timeout) fileCont.raise_for_status() self._fileJson.append(fileCont.json()) except Exception as e: LOGGER.exception(e) raise return self._fileJson @property def files(self)->list: ''' Returns a list of tuples with: (Download_location, filename, mimetype, size, description, digest, digestType ) Digest types include, but are not necessarily limited to: 'adler-32','crc-32','md2','md5','sha-1','sha-256', 'sha-384','sha-512' ''' out = [] for page in self.fileJson: files = page['_embedded'].get('stash:files') if files: for f in files: #This broke with this commit: # https://github.com/datadryad/dryad-app/commit/b8a333ba34b14e55cbc1d7ed5aa4451e0f41db66 #downLink = f['_links']['stash:file-download']['href'] downLink = f['_links']['stash:download']['href'] downLink = f'{constants.DRYURL}{downLink}' name = f['path'] mimeType = f['mimeType'] size = f['size'] #HOW ABOUT PUTTING THIS IN THE DRYAD API PAGE? descr = f.get('description', '') digestType = f.get('digestType', '') #not all files have a digest digest = f.get('digest', '') #Does it matter? If the primary use case is to #compare why not take all the digest types. #md5 = '' #if digestType == 'md5' and digest: # md5 = digest # #nothing in the docs as to algorithms so just picking md5 # #Email from Ryan Scherle 30 Nov 20: supported digest type # #('adler-32','crc-32','md2','md5','sha-1','sha-256', # #'sha-384','sha-512') out.append((downLink, name, mimeType, size, descr, digestType, digest)) return out @property def oversize(self, maxsize=None): ''' Returns a list of Dryad files whose size value exceeds maxsize. Maximum size defaults to dryad2dataverse.constants.MAX_UPLOAD Parameters ---------- maxsize : int Size in bytes in which to flag as oversize. Defaults to constants.MAX_UPLOAD. ''' if not maxsize: maxsize = constants.MAX_UPLOAD toobig = [] for f in self.files: if f[3] >= maxsize: toobig.append(f) return toobig #def_typeclass(self, typeName, multiple, typeClass): @staticmethod def _typeclass(typeName, multiple, typeClass): ''' Creates wrapper around single or multiple Dataverse JSON objects. Returns a dict *without* the Dataverse 'value' key'. Parameters ---------- typeName : str Dataverse typeName (eg: 'author'). multiple : boolean \"Multiple\" value in Dataverse JSON. typeClass : str Dataverse typeClass. Usually one of 'compound', 'primitive, 'controlledVocabulary'). ''' return {'typeName':typeName, 'multiple':multiple, 'typeClass':typeClass} @staticmethod def _convert_generic(**kwargs): ''' Generic dataverse json segment creator of form: ``` {dvField: {'typeName': dvField, 'value': dryField} ``` Suitable for generalized conversions. Only provides fields with multiple: False and typeclass:Primitive Parameters ---------- kwargs : dict Dict from Dataverse JSON segment Other parameters ---------------- dvField : str Dataverse output field dryField : str Dryad JSON field to convert inJson : dict Dryad JSON **segment** to convert addJSON : dict (optional) any other JSON required to complete (cf ISNI) rType : str 'dict' (default) or 'list'. Returns 'value' field as dict value or list. pNotes : str Notes to be prepended to list type values. No trailing space required. ''' dvField = kwargs.get('dvField') dryField = kwargs.get('dryField') inJson = kwargs.get('inJson') addJson = kwargs.get('addJson') pNotes = kwargs.get('pNotes', '') rType = kwargs.get('rType', 'dict') if not dvField or not dryField or not inJson: try: raise ValueError('Incorrect or insufficient fields provided') except ValueError as e: LOGGER.exception(e) raise outfield = inJson.get(dryField) if outfield: outfield = outfield.strip() #if not outfield: # raise ValueError(f'Dryad field {dryField} not found') # If value missing can still concat empty dict if not outfield: return {} if rType == 'list': if pNotes: outfield = [f'{pNotes} {outfield}'] outJson = {dvField:{'typeName':dvField, 'multiple': False, 'typeClass':'primitive', 'value': outfield}} #Simple conversion if not addJson: return outJson #Add JSONs together addJson.update(outJson) return addJson @staticmethod def _convert_author_names(author): ''' Produces required author json fields. This is a special case, requiring concatenation of several fields. Parameters ---------- author : dict dryad['author'] JSON segment. ''' first = author.get('firstName') last = author.get('lastName') if first + last is None: return None authname = f\"{author.get('lastName','')}, {author.get('firstName', '')}\" return {'authorName': {'typeName':'authorName', 'value': authname, 'multiple':False, 'typeClass':'primitive'}} @staticmethod def _convert_keywords(*args): ''' Produces the insane keyword structure Dataverse JSON segment from a list of words. Parameters ---------- args : list List with elements as strings. Generally input is Dryad JSON 'keywords', ie *Dryad['keywords']. Don't forget to expand the list using *. ''' outlist = [] for arg in args: outlist.append({'keywordValue': { 'typeName':'keywordValue', 'value': arg}}) return outlist @staticmethod def _convert_notes(dryJson): ''' Returns formatted notes field with Dryad JSON values that don't really fit anywhere into the Dataverse JSON. Parameters ---------- dryJson : dict Dryad JSON as dict. ''' notes = '' #these fields should be concatenated into notes notable = ['versionNumber', 'versionStatus', 'manuscriptNumber', 'curationStatus', 'preserveCurationStatus', 'invoiceId', 'sharingLink', 'loosenValidation', 'skipDataciteUpdate', 'storageSize', 'visibility', 'skipEmails'] for note in notable: text = dryJson.get(note) if text: text = str(text).strip() if note == 'versionNumber': text = f'<b>Dryad version number:</b> {text}' if note == 'versionStatus': text = f'<b>Version status:</b> {text}' if note == 'manuscriptNumber': text = f'<b>Manuscript number:</b> {text}' if note == 'curationStatus': text = f'<b>Dryad curation status:</b> {text}' if note == 'preserveCurationStatus': text = f'<b>Dryad preserve curation status:</b> {text}' if note == 'invoiceId': text = f'<b>Invoice ID:</b> {text}' if note == 'sharingLink': text = f'<b>Sharing link:</b> {text}' if note == 'loosenValidation': text = f'<b>Loosen validation:</b> {text}' if note == 'skipDataciteUpdate': text = f'<b>Skip Datacite update:</b> {text}' if note == 'storageSize': text = f'<b>Storage size:</b> {text}' if note == 'visibility': text = f'<b>Visibility:</b> {text}' if note == 'skipEmails': text = f'<b>Skip emails:</b> {text}' notes += f'<p>{text}</p>\\n' concat = {'typeName':'notesText', 'multiple':False, 'typeClass': 'primitive', 'value': notes} return concat @staticmethod def _boundingbox(north, south, east, west): ''' Makes a Dataverse bounding box from appropriate coordinates. Returns Dataverse JSON segment as dict. Parameters ---------- north : float south : float east : float west : float Notes ----- Coordinates in decimal degrees. ''' names = ['north', 'south', 'east', 'west'] points = [str(x) for x in [north, south, east, west]] #Because coordinates in DV are strings BFY coords = [(x[0]+'Longitude', {x[0]:x[1]}) for x in zip(names, points)] #Yes, everything is longitude in Dataverse out = [] for coord in coords: out.append(Serializer._convert_generic(inJson=coord[1], dvField=coord[0], #dryField='north')) dryField=[k for k in coord[1].keys()][0])) return out @staticmethod def _convert_geospatial(dryJson): ''' Outputs Dataverse geospatial metadata block. Parameters ---------- dryJson : dict Dryad json as dict. ''' if dryJson.get('locations'): #out = {} coverage = [] box = [] otherCov = None gbbox = None for loc in dryJson.get('locations'): if loc.get('place'): #These are impossible to clean. Going to \"other\" field other = Serializer._convert_generic(inJson=loc, dvField='otherGeographicCoverage', dryField='place') coverage.append(other) if loc.get('point'): #makes size zero bounding box north = loc['point']['latitude'] south = north east = loc['point']['longitude'] west = east point = Serializer._boundingbox(north, south, east, west) box.append(point) if loc.get('box'): north = loc['box']['neLatitude'] south = loc['box']['swLatitude'] east = loc['box']['neLongitude'] west = loc['box']['swLongitude'] area = Serializer._boundingbox(north, south, east, west) box.append(area) if coverage: otherCov = Serializer._typeclass(typeName='geographicCoverage', multiple=True, typeClass='compound') otherCov['value'] = coverage if box: gbbox = Serializer._typeclass(typeName='geographicCoverage', multiple=True, typeClass='compound') gbbox['value'] = box if otherCov or gbbox: gblock = {'geospatial': {'displayName' : 'Geospatial Metadata', 'fields': []}} if otherCov: gblock['geospatial']['fields'].append(otherCov) if gbbox: gblock['geospatial']['fields'].append(gbbox) return gblock return {} def _assemble_json(self, dryJson=None, dvContact=None, dvEmail=None, defContact=True): ''' Assembles Dataverse json from Dryad JSON components. Dataverse JSON is a nightmare, so this function is too. Parameters ---------- dryJson : dict Dryad json as dict. dvContact : str Default Dataverse contact name. dvEmail : str Default Dataverse 4 contact email address. defContact : boolean Flag to include default contact information with record. ''' if not dvContact: dvContact = constants.DV_CONTACT_NAME if not dvEmail: dvEmail = constants.DV_CONTACT_EMAIL if not dryJson: dryJson = self.dryadJson LOGGER.debug(dryJson) #Licence block changes ensure that it will only work with #Dataverse v5.10+ #Go back to previous commits to see the earlier \"standard\" self._dvJson = {'datasetVersion': {'license':{'name': 'CC0 1.0', 'uri': 'http://creativecommons.org/publicdomain/zero/1.0' }, 'termsOfUse': Serializer.CC0, 'metadataBlocks':{'citation': {'displayName': 'Citation Metadata', 'fields': []}, } } } #REQUIRED Dataverse fields #Dryad is a general purpose database; it is hard/impossible to get #Dataverse required subject tags out of their keywords, so: defaultSubj = {'typeName' : 'subject', 'typeClass':'controlledVocabulary', 'multiple': True, 'value' : ['Other']} self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(defaultSubj) reqdTitle = Serializer._convert_generic(inJson=dryJson, dryField='title', dvField='title')['title'] self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(reqdTitle) #authors out = [] for a in dryJson['authors']: reqdAuthor = Serializer._convert_author_names(a) if reqdAuthor: affiliation = Serializer._convert_generic(inJson=a, dvField='authorAffiliation', dryField='affiliation') addOrc = {'authorIdentifierScheme': {'typeName':'authorIdentifierScheme', 'value': 'ORCID', 'typeClass': 'controlledVocabulary', 'multiple':False}} #only ORCID at UBC orcid = Serializer._convert_generic(inJson=a, dvField='authorIdentifier', dryField='orcid', addJson=addOrc) if affiliation: reqdAuthor.update(affiliation) if orcid: reqdAuthor.update(orcid) out.append(reqdAuthor) authors = Serializer._typeclass(typeName='author', multiple=True, typeClass='compound') authors['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(authors) ##rewrite as function:contact out = [] for e in dryJson['authors']: reqdContact = Serializer._convert_generic(inJson=e, dvField='datasetContactEmail', dryField='email') if reqdContact: author = Serializer._convert_author_names(e) author = {'author':author['authorName']['value']} #for passing to function author = Serializer._convert_generic(inJson=author, dvField='datasetContactName', dryField='author') if author: reqdContact.update(author) affiliation = Serializer._convert_generic(inJson=e, dvField='datasetContactAffiliation', dryField='affiliation') if affiliation: reqdContact.update(affiliation) out.append(reqdContact) if defContact: #Adds default contact information the tail of the list defEmail = Serializer._convert_generic(inJson={'em':dvEmail}, dvField='datasetContactEmail', dryField='em') defName = Serializer._convert_generic(inJson={'name':dvContact}, dvField='datasetContactName', dryField='name') defEmail.update(defName) out.append(defEmail) contacts = Serializer._typeclass(typeName='datasetContact', multiple=True, typeClass='compound') contacts['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(contacts) #Description description = Serializer._typeclass(typeName='dsDescription', multiple=True, typeClass='compound') desCat = [('abstract', '<b>Abstract</b><br/>'), ('methods', '<b>Methods</b><br />'), ('usageNotes', '<b>Usage notes</b><br />')] out = [] for desc in desCat: if dryJson.get(desc[0]): descrField = Serializer._convert_generic(inJson=dryJson, dvField='dsDescriptionValue', dryField=desc[0]) descrField['dsDescriptionValue']['value'] = (desc[1] + descrField['dsDescriptionValue']['value']) descDate = Serializer._convert_generic(inJson=dryJson, dvField='dsDescriptionDate', dryField='lastModificationDate') descrField.update(descDate) out.append(descrField) description['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(description) #Granting agencies if dryJson.get('funders'): out = [] for fund in dryJson['funders']: org = Serializer._convert_generic(inJson=fund, dvField='grantNumberAgency', dryField='organization') if fund.get('awardNumber'): fund = Serializer._convert_generic(inJson=fund, dvField='grantNumberValue', dryField='awardNumber') org.update(fund) out.append(org) grants = Serializer._typeclass(typeName='grantNumber', multiple=True, typeClass='compound') grants['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(grants) #Keywords keywords = Serializer._typeclass(typeName='keyword', multiple=True, typeClass='compound') out = [] for key in dryJson.get('keywords', []): #Apparently keywords are not required keydict = {'keyword':key} #because takes a dict kv = Serializer._convert_generic(inJson=keydict, dvField='keywordValue', dryField='keyword') vocab = {'dryad':'Dryad'} voc = Serializer._convert_generic(inJson=vocab, dvField='keywordVocabulary', dryField='dryad') kv.update(voc) out.append(kv) keywords['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(keywords) #modification date moddate = Serializer._convert_generic(inJson=dryJson, dvField='dateOfDeposit', dryField='lastModificationDate') self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(moddate['dateOfDeposit']) #This one isn't nested BFY #distribution date distdate = Serializer._convert_generic(inJson=dryJson, dvField='distributionDate', dryField='publicationDate') self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(distdate['distributionDate']) #Also not nested #publications publications = Serializer._typeclass(typeName='publication', multiple=True, typeClass='compound') #quick and dirty lookup table #TODONE see https://github.com/CDL-Dryad/dryad-app/blob/ #31d17d8dab7ea3bab1256063a1e4d0cb706dd5ec/stash/stash_datacite/ #app/models/stash_datacite/related_identifier.rb #no longer required #lookup = {'IsDerivedFrom':'Is derived from', # 'Cites':'Cites', # 'IsSupplementTo': 'Is supplement to', # 'IsSupplementedBy': 'Is supplemented by'} out = [] if dryJson.get('relatedWorks'): for r in dryJson.get('relatedWorks'): #id = r.get('identifier') #TODONE Verify that changing id to _id has not broken anything: 11Feb21 _id = r.get('identifier') #Note:10 Feb 2021 : some records have identifier = ''. BAD DRYAD. if not _id: continue relationship = r.get('relationship') #idType = r.get('identifierType') #not required in _convert_generic #citation = {'citation': f\"{lookup[relationship]}: {id}\"} citation = {'citation': relationship.capitalize()} pubcite = Serializer._convert_generic(inJson=citation, dvField='publicationCitation', dryField='citation') pubIdType = Serializer._convert_generic(inJson=r, dvField='publicationIDType', dryField='identifierType') #ID type must be lower case pubIdType['publicationIDType']['value'] = pubIdType['publicationIDType']['value'].lower() pubIdType['publicationIDType']['typeClass'] = 'controlledVocabulary' pubUrl = Serializer._convert_generic(inJson=r, dvField='publicationURL', dryField='identifier') #Dryad doesn't just put URLs in their URL field. if pubUrl['publicationURL']['value'].lower().startswith('doi:'): fixurl = 'https://doi.org/' + pubUrl['publicationURL']['value'][4:] pubUrl['publicationURL']['value'] = fixurl LOGGER.debug('Rewrote URLs to be %s', fixurl) #Dryad doesn't validate URL fields to start with http or https. Assume https if not pubUrl['publicationURL']['value'].lower().startswith('htt'): pubUrl['publicationURL']['value'] = ('https://' + pubUrl['publicationURL']['value']) pubcite.update(pubIdType) pubcite.update(pubUrl) out.append(pubcite) publications['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(publications) #notes #go into primary notes field, not DDI self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(Serializer._convert_notes(dryJson)) #Geospatial metadata self._dvJson['datasetVersion']['metadataBlocks'].update(Serializer._convert_geospatial(dryJson)) #DOI --> agency/identifier doi = Serializer._convert_generic(inJson=dryJson, dryField='identifier', dvField='otherIdValue') doi.update(Serializer._convert_generic(inJson={'agency':'Dryad'}, dryField='agency', dvField='otherIdAgency')) agency = Serializer._typeclass(typeName='otherId', multiple=True, typeClass='compound') agency['value'] = [doi] self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(agency) dryadJson property writable \u00b6 Returns Dryad study JSON. Will call Serializer.fetch_record() if no JSON is present. dvJson property \u00b6 Returns Dataverse study JSON as dict. embargo property \u00b6 Check embargo status. Returns boolean True if embargoed. fileJson property \u00b6 Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated, so the return consists of a list of dicts, one per page. Parameters: timeout ( int ) \u2013 Request timeout in seconds. files property \u00b6 Returns a list of tuples with: (Download_location, filename, mimetype, size, description, digest, digestType ) Digest types include, but are not necessarily limited to: \u2018adler-32\u2019,\u2019crc-32\u2019,\u2019md2\u2019,\u2019md5\u2019,\u2019sha-1\u2019,\u2019sha-256\u2019, \u2018sha-384\u2019,\u2019sha-512\u2019 id property \u00b6 Returns Dryad unique database ID, not the DOI. Where the original Dryad JSON is dryadJson, it\u2019s the integer trailing portion of: self.dryadJson['_links']['stash:version']['href'] oversize property \u00b6 Returns a list of Dryad files whose size value exceeds maxsize. Maximum size defaults to dryad2dataverse.constants.MAX_UPLOAD Parameters: maxsize ( int ) \u2013 Size in bytes in which to flag as oversize. Defaults to constants.MAX_UPLOAD. __init__(doi) \u00b6 Creates Dryad study metadata instance. Parameters: doi ( str ) \u2013 DOI of Dryad study. Required for downloading. eg: \u2018doi:10.5061/dryad.2rbnzs7jp\u2019 Source code in src/dryad2dataverse/serializer.py def __init__(self, doi): ''' Creates Dryad study metadata instance. Parameters ---------- doi : str DOI of Dryad study. Required for downloading. eg: 'doi:10.5061/dryad.2rbnzs7jp' ''' self.doi = doi self._dryadJson = None self._fileJson = None self._dvJson = None #Serializer objects will be assigned a Dataverse study PID #if dryad2Dataverse.transfer.Transfer() is instantiated self.dvpid = None self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=constants.RETRY_STRATEGY)) LOGGER.debug('Creating Serializer instance object') fetch_record(url=None, timeout=45) \u00b6 Fetches Dryad study record JSON from Dryad V2 API at https://datadryad.org/api/v2/datasets/. Saves to self._dryadJson. Querying Serializer.dryadJson will call this function automatically. Parameters: url ( str , default: None ) \u2013 Dryad instance base URL (eg: \u2018https://datadryad.org\u2019). timeout ( int , default: 45 ) \u2013 Timeout in seconds. Default 45. Source code in src/dryad2dataverse/serializer.py def fetch_record(self, url=None, timeout=45): ''' Fetches Dryad study record JSON from Dryad V2 API at https://datadryad.org/api/v2/datasets/. Saves to self._dryadJson. Querying Serializer.dryadJson will call this function automatically. Parameters ---------- url : str Dryad instance base URL (eg: 'https://datadryad.org'). timeout : int Timeout in seconds. Default 45. ''' if not url: url = constants.DRYURL try: headers = {'accept':'application/json', 'Content-Type':'application/json'} headers.update(USER_AGENT) doiClean = urllib.parse.quote(self.doi, safe='') resp = self.session.get(f'{url}/api/v2/datasets/{doiClean}', headers=headers, timeout=timeout) resp.raise_for_status() self._dryadJson = resp.json() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('URL error for: %s', url) LOGGER.exception(err) raise dryad2dataverse.transfer \u00b6 This module handles data downloads and uploads from a Dryad instance to a Dataverse instance Transfer \u00b6 Transfers metadata and data files from a Dryad installation to Dataverse installation. Source code in src/dryad2dataverse/transfer.py class Transfer(): ''' Transfers metadata and data files from a Dryad installation to Dataverse installation. ''' def __init__(self, dryad): ''' Creates a dryad2dataverse.transfer.Transfer instance. Parameters ---------- dryad : dryad2dataverse.serializer.Serializer ''' self.dryad = dryad self._fileJson = None self._files = [list(f) for f in self.dryad.files] #self._files = copy.deepcopy(self.dryad.files) self.fileUpRecord = [] self.fileDelRecord = [] self.dvStudy = None self.jsonFlag = None #Whether or not new json uploaded self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=constants.RETRY_STRATEGY)) def _del__(self): #TODONE: Change name to __del__ to make a destructor '''Expunges files from constants.TMP on deletion''' for f in self.files: if os.path.exists(f'{constants.TMP}{os.sep}{f[1]}'): os.remove(f'{constants.TMP}{os.sep}{f[1]}') def test_api_key(self, url=None, apikey=None): ''' Tests for an expired API key and raises dryad2dataverse.exceptions.Dryad2dataverseBadApiKeyError the API key is bad. Ignores other HTTP errors. Parameters ---------- url : str Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL apikey : str Default dryad2dataverse.constants.APIKEY. ''' #API validity check appears to come before a PID validity check params = {'persistentId': 'doi:000/000/000'} # PID is irrelevant if not url: url = constants.DVURL headers = {'X-Dataverse-key': apikey if apikey else constants.APIKEY} headers.update(USER_AGENT) bad_test = self.session.get(f'{url}/api/datasets/:persistentId', headers=headers, params=params) #There's an extra space in the message which Harvard #will probably find out about, so . . . if bad_test.json().get('message').startswith('Bad api key'): try: raise exceptions.DataverseBadApiKeyError('Bad API key') except exceptions.DataverseBadApiKeyError as e: LOGGER.critical('API key has expired or is otherwise invalid') LOGGER.exception(e) #LOGGER.exception(traceback.format_exc()) #not really necessary raise try: #other errors bad_test.raise_for_status() except requests.exceptions.HTTPError: pass except Exception as e: LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise @property def dvpid(self): ''' Returns Dataverse study persistent ID as str. ''' return self.dryad.dvpid @property def auth(self): ''' Returns datavese authentication header dict. ie: `{X-Dataverse-key' : 'APIKEYSTRING'}` ''' return {'X-Dataverse-key' : constants.APIKEY} @property def fileJson(self): ''' Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated. ''' return self.dryad.fileJson.copy() @property def files(self): ''' Returns a list of lists with: [Download_location, filename, mimetype, size, description, md5digest] This is mutable; downloading a file will add md5 info if not available. ''' return self._files @property def oversize(self): ''' Returns list of files exceeding Dataverse ingest limit dryad2dataverse.constants.MAX_UPLOAD. ''' return self.dryad.oversize @property def doi(self): ''' Returns Dryad DOI. ''' return self.dryad.doi @staticmethod def _dryad_file_id(url:str): ''' Returns Dryad fileID from dryad file download URL as integer. Parameters ---------- url : str Dryad file URL in format 'https://datadryad.org/api/v2/files/385820/download'. ''' fid = url.strip('/download') fid = int(fid[fid.rfind('/')+1:]) return fid @staticmethod def _make_dv_head(apikey): ''' Returns Dataverse authentication header as dict. Parameters ---------- apikey : str Dataverse API key. ''' return {'X-Dataverse-key' : apikey} #@staticmethod def set_correct_date(self, url=None, hdl=None, d_type='distributionDate', apikey=None): ''' Sets \"correct\" publication date for Dataverse. Parameters ---------- url : str Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL hdl : str Persistent indentifier for Dataverse study. Defaults to Transfer.dvpid (which can be None if the study has not yet been uploaded). d_type : str Date type. One of 'distributionDate', 'productionDate', `dateOfDeposit'. Default 'distributionDate'. apikey : str Default dryad2dataverse.constants.APIKEY. Notes ----- dryad2dataverse.serializer maps Dryad 'publicationDate' to Dataverse 'distributionDate' (see serializer.py ~line 675). Dataverse citation date default is \":publicationDate\". See Dataverse API reference: <https://guides.dataverse.org/en/4.20/api/native-api.html#id54>. ''' try: if not url: url = constants.DVURL if not hdl: hdl = self.dvpid headers = {'X-Dataverse-key' : apikey} if apikey: headers = {'X-Dataverse-key' : apikey} else: headers = {'X-Dataverse-key' : constants.APIKEY} headers.update(USER_AGENT) params = {'persistentId': hdl} set_date = self.session.put(f'{url}/api/datasets/:persistentId/citationdate', headers=headers, data=d_type, params=params, timeout=45) set_date.raise_for_status() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.warning('Unable to set citation date for %s', hdl) LOGGER.warning(err) LOGGER.warning(set_date.text) def upload_study(self, url=None, apikey=None, timeout=45, **kwargs): ''' Uploads Dryad study metadata to target Dataverse or updates existing. Supplying a `targetDv` kwarg creates a new study and supplying a `dvpid` kwarg updates a currently existing Dataverse study. Parameters ---------- url : str URL of Dataverse instance. Defaults to constants.DVURL. apikey : str API key of user. Defaults to contants.APIKEY. timeout : int timeout on POST request. kwargs : dict Other parameters ---------------- targetDv : str Short name of target dataverse. Required if new dataset. Specify as targetDV=value. dvpid : str Dataverse persistent ID (for updating metadata). This is not required for new uploads, specify as dvpid=value Notes ----- One of targetDv or dvpid is required. ''' if not url: url = constants.DVURL if not apikey: apikey = constants.APIKEY headers = {'X-Dataverse-key' : apikey} headers.update(USER_AGENT) targetDv = kwargs.get('targetDv') dvpid = kwargs.get('dvpid') #dryFid = kwargs.get('dryFid') #Why did I put this here? if not targetDv and not dvpid: try: raise exceptions.NoTargetError('You must supply one of targetDv \\ (target dataverse) \\ or dvpid (Dataverse persistent ID)') except exceptions.NoTargetError as e: LOGGER.error('No target dataverse or dvpid supplied') LOGGER.exception(e) raise if targetDv and dvpid: try: raise ValueError('Supply only one of targetDv or dvpid') except ValueError as e: LOGGER.exception(e) raise if not dvpid: endpoint = f'{url}/api/dataverses/{targetDv}/datasets' upload = self.session.post(endpoint, headers=headers, json=self.dryad.dvJson, timeout=timeout) LOGGER.debug(upload.text) else: endpoint = f'{url}/api/datasets/:persistentId/versions/:draft' params = {'persistentId':dvpid} #Yes, dataverse uses *different* json for edits upload = self.session.put(endpoint, params=params, headers=headers, json=self.dryad.dvJson['datasetVersion'], timeout=timeout) #self._dvrecord = upload.json() LOGGER.debug(upload.text) try: updata = upload.json() self.dvStudy = updata if updata.get('status') != 'OK': try: raise exceptions.DataverseUploadError(('Status return is not OK.' f'{upload.status_code}: ' f'{upload.reason}. ' f'{upload.request.url} ' f'{upload.text}')) except exceptions.DataverseUploadError as e: LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise exceptions.DataverseUploadError(('Status return is not OK.' f'{upload.status_code}: ' f'{upload.reason}. ' f'{upload.request.url} ' f'{upload.text}')) upload.raise_for_status() except Exception as e: # Only accessible via non-requests exception LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise if targetDv: self.dryad.dvpid = updata['data'].get('persistentId') if dvpid: self.dryad.dvpid = updata['data'].get('datasetPersistentId') return self.dvpid @staticmethod def _check_md5(infile, dig_type): ''' Returns the hex digest of a file (formerly just md5sum). Parameters ---------- infile : str Complete path to target file. dig_type : Union[str, None] Digest type ''' #From Ryan Scherle #When Dryad calculates a digest, it only uses MD5. #But if you have precomputed some other type of digest, we should accept it. #The list of allowed values is: #('adler-32','crc-32','md2','md5','sha-1','sha-256','sha-384','sha-512') #hashlib doesn't support adler-32, crc-32, md2 blocksize = 2**16 #Well, this is inelegant with open(infile, 'rb') as m: #fmd5 = hashlib.md5() ## var name kept for posterity. Maybe refactor if dig_type in ['sha-1', 'sha-256', 'sha-384', 'sha-512', 'md5', 'md2']: if dig_type == 'md2': fmd5 = Crypto.Hash.MD2.new() else: fmd5 = HASHTABLE[dig_type]() fblock = m.read(blocksize) while fblock: fmd5.update(fblock) fblock = m.read(blocksize) return fmd5.hexdigest() if dig_type in ['adler-32', 'crc-32']: fblock = m.read(blocksize) curvalue = HASHTABLE[dig_type](fblock) while fblock: fblock = m.read(blocksize) curvalue = HASHTABLE[dig_type](fblock, curvalue) return curvalue raise exceptions.HashError(f'Unable to determine hash type for{infile}: {dig_type}') def download_file(self, url=None, filename=None, tmp=None, size=None, chk=None, timeout=45, **kwargs): ''' Downloads a file via requests streaming and saves to constants.TMP. returns checksum on success and an exception on failure. Parameters ---------- url : str URL of download. filename : str Output file name. timeout : int Requests timeout. tmp : str Temporary directory for downloads. Defaults to dryad2dataverse.constants.TMP. size : int Reported file size in bytes. Defaults to dryad2dataverse.constants.MAX_UPLOAD. chk : str checksum of file (if available and known). timeout : int timeout in seconds kwargs : dict Other parameters ---------------- digest_type : str checksum type (ie, md5, sha-256, etc) ''' LOGGER.debug('Start download sequence') LOGGER.debug('MAX SIZE = %s', constants.MAX_UPLOAD) LOGGER.debug('Filename: %s, size=%s', filename, size) if not tmp: tmp = constants.TMP if tmp.endswith(os.sep): tmp = tmp[:-1] if size: if size > constants.MAX_UPLOAD: #TOO BIG LOGGER.warning('%s: File %s exceeds ' 'Dataverse MAX_UPLOAD size. Skipping download.', self.doi, filename) md5 = 'this_file_is_too_big_to_upload__' #HA HA for i in self._files: if url == i[0]: i[-1] = md5 LOGGER.debug('Stop download sequence with large file skip') return md5 try: down = self.session.get(url, timeout=timeout, stream=True) down.raise_for_status() with open(f'{tmp}{os.sep}{filename}', 'wb') as fi: for chunk in down.iter_content(chunk_size=8192): fi.write(chunk) #verify size #https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python' if size: checkSize = os.stat(f'{tmp}{os.sep}{filename}').st_size if checkSize != size: try: raise exceptions.DownloadSizeError('Download size does not match ' 'reported size') except exceptions.DownloadSizeError as e: LOGGER.exception(e) raise #now check the md5 md5 = None if chk and kwargs.get('digest_type') in HASHTABLE: md5 = Transfer._check_md5(f'{tmp}{os.sep}{filename}', kwargs['digest_type']) if md5 != chk: try: raise exceptions.HashError(f'Hex digest mismatch: {md5} : {chk}') #is this really what I want to do on a bad checksum? except exceptions.HashError as e: LOGGER.exception(e) raise for i in self._files: if url == i[0]: i[-1] = md5 LOGGER.debug('Complete download sequence') #This doesn't actually return an md5, just the hash value return md5 except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.critical('Unable to download %s', url) LOGGER.exception(err) raise exceptions.DataverseDownloadError def download_files(self, files=None): ''' Bulk downloader for files. Parameters ---------- files : list Items in list can be tuples or list with a minimum of: `(dryaddownloadurl, filenamewithoutpath, [md5sum])` The md5 sum should be the last member of the tuple. Defaults to self.files. Notes ----- Normally used without arguments to download all the associated files with a Dryad study. ''' if not files: files = self.files try: for f in files: self.download_file(url=f[0], filename=f[1], mimetype=f[2], size=f[3], descr=f[4], digest_type=f[5], chk=f[-1]) except exceptions.DataverseDownloadError as e: LOGGER.exception('Unable to download file with info %s\\n%s', f, e) raise def file_lock_check(self, study, dv_url, apikey=None, count=0): ''' Checks for a study lock Returns True if locked. Normally used to check if processing is completed. As tabular processing halts file ingest, there should be no locks on a Dataverse study before performing a data file upload. Parameters ---------- study : str Persistent indentifer of study. dv_url : str URL to base Dataverse installation. apikey : str API key for user. If not present authorization defaults to self.auth. count : int Number of times the function has been called. Logs lock messages only on 0. ''' if dv_url.endswith('/'): dv_url = dv_url[:-1] if apikey: headers = {'X-Dataverse-key': apikey} else: headers = self.auth headers.update(USER_AGENT) params = {'persistentId': study} try: lock_status = self.session.get(f'{dv_url}/api/datasets/:persistentId/locks', headers=headers, params=params, timeout=300) lock_status.raise_for_status() if lock_status.json().get('data'): if count == 0: LOGGER.warning('Study %s has been locked', study) LOGGER.warning('Lock info:\\n%s', lock_status.json()) return True return False except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('Unable to detect lock status for %s', study) LOGGER.error('ERROR message: %s', lock_status.text) LOGGER.exception(err) #return True #Should I raise here? raise def force_notab_unlock(self, study, dv_url, apikey=None): ''' Checks for a study lock and forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. **Forcible unlocks require a superuser API key.** Parameters ---------- study : str Persistent indentifer of study. dv_url : str URL to base Dataverse installation. apikey : str API key for user. If not present authorization defaults to self.auth. ''' if dv_url.endswith('/'): dv_url = dv_url[:-1] if apikey: headers = {'X-Dataverse-key': apikey} else: headers = self.auth headers.update(USER_AGENT) params = {'persistentId': study} lock_status = self.session.get(f'{dv_url}/api/datasets/:persistentId/locks', headers=headers, params=params, timeout=300) lock_status.raise_for_status() if lock_status.json()['data']: LOGGER.warning('Study %s has been locked', study) LOGGER.warning('Lock info:\\n%s', lock_status.json()) force_unlock = self.session.delete(f'{dv_url}/api/datasets/:persistentId/locks', params=params, headers=headers, timeout=300) force_unlock.raise_for_status() LOGGER.warning('Lock removed for %s', study) LOGGER.warning('Lock status:\\n %s', force_unlock.json()) #This is what the file ID was for, in case it can #be implemented again. #According to Harvard, you can't remove the progress bar #for uploaded tab files that squeak through unless you #let them ingest first then reingest them. Oh well. #See: #https://groups.google.com/d/msgid/dataverse-community/ #74caa708-e39b-4259-874d-5b6b74ef9723n%40googlegroups.com #Also, you can't uningest it because it hasn't been #ingested once it's been unlocked. So the commented #code below is useless (for now) #uningest = requests.post(f'{dv_url}/api/files/{fid}/uningest', # headers=headers, # timeout=300) #LOGGER.warning('Ingest halted for file %s for study %s', fid, study) #uningest.raise_for_status() def upload_file(self, dryadUrl=None, filename=None, mimetype=None, size=None, descr=None, hashtype=None, #md5=None, studyId=None, dest=None, digest=None, studyId=None, dest=None, fprefix=None, force_unlock=False, timeout=300): ''' Uploads file to Dataverse study. Returns a tuple of the dryadFid (or None) and Dataverse JSON from the POST request. Failures produce JSON with different status messages rather than raising an exception. Parameters ---------- filename : str Filename (not including path). mimetype : str Mimetype of file. size : int Size in bytes. studyId : str Persistent Dataverse study identifier. Defaults to Transfer.dvpid. dest : str Destination dataverse installation url. Defaults to constants.DVURL. hashtype: str original Dryad hash type fprefix : str Path to file, not including a trailing slash. timeout : int Timeout in seconds for POST request. Default 300. dryadUrl : str Dryad download URL if you want to include a Dryad file id. force_unlock : bool Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse `/locks` endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). **Forcible unlock requires a superuser API key.** ''' #return locals() #TODONE remove above if not studyId: studyId = self.dvpid if not dest: dest = constants.DVURL if not fprefix: fprefix = constants.TMP if dryadUrl: fid = dryadUrl.strip('/download') fid = int(fid[fid.rfind('/')+1:]) else: fid = 0 #dummy fid for non-Dryad use params = {'persistentId' : studyId} upfile = fprefix + os.sep + filename[:] badExt = filename[filename.rfind('.'):].lower() #Descriptions are technically possible, although how to add #them is buried in Dryad's API documentation dv4meta = {'label' : filename[:], 'description' : descr} #if mimetype == 'application/zip' or filename.lower().endswith('.zip'): if mimetype == 'application/zip' or badExt in constants.NOTAB: mimetype = 'application/octet-stream' # stop unzipping automatically filename += '.NOPROCESS' # Also screw with their naming convention #debug log about file names to see what is up with XSLX #see doi:10.5061/dryad.z8w9ghxb6 LOGGER.debug('File renamed to %s for upload', filename) if size >= constants.MAX_UPLOAD: fail = (fid, {'status' : 'Failure: MAX_UPLOAD size exceeded'}) self.fileUpRecord.append(fail) LOGGER.warning('%s: File %s of ' 'size %s exceeds ' 'Dataverse MAX_UPLOAD size. Skipping.', self.doi, filename, size) return fail fields = {'file': (filename, open(upfile, 'rb'), mimetype)} fields.update({'jsonData': f'{dv4meta}'}) multi = MultipartEncoder(fields=fields) ctype = {'Content-type' : multi.content_type} tmphead = self.auth.copy() tmphead.update(ctype) tmphead.update(USER_AGENT) url = dest + '/api/datasets/:persistentId/add' try: upload = self.session.post(url, params=params, headers=tmphead, data=multi, timeout=timeout) #print(upload.text) upload.raise_for_status() self.fileUpRecord.append((fid, upload.json())) upmd5 = upload.json()['data']['files'][0]['dataFile']['checksum']['value'] #Dataverse hash type _type = upload.json()['data']['files'][0]['dataFile']['checksum']['type'] if _type.lower() != hashtype.lower(): comparator = self._check_md5(upfile, _type.lower()) else: comparator = digest #if hashtype.lower () != 'md5': # #get an md5 because dataverse uses md5s. Or most of them do anyway. # #One day this will be rewritten properly. # md5 = self._check_md5(filename, 'md5') #else: # md5 = digest #if md5 and (upmd5 != md5): if upmd5 != comparator: try: raise exceptions.HashError(f'{_type} mismatch:\\nlocal: {comparator}\\nuploaded: {upmd5}') except exceptions.HashError as e: LOGGER.exception(e) raise #Make damn sure that the study isn't locked because of #tab file processing ##SPSS files still process despite spoofing MIME and extension ##so there's also a forcible unlock check #fid = upload.json()['data']['files'][0]['dataFile']['id'] #fid not required for unlock #self.force_notab_unlock(studyId, dest, fid) if force_unlock: self.force_notab_unlock(studyId, dest) else: count = 0 wait = True while wait: wait = self.file_lock_check(studyId, dest, count=count) if wait: time.sleep(15) # Don't hit it too often count += 1 return (fid, upload.json()) except Exception as e: LOGGER.exception(e) try: reason = upload.json()['message'] LOGGER.warning(upload.json()) return (fid, {'status' : f'Failure: {reason}'}) except Exception as e: LOGGER.warning('Further exceptions!') LOGGER.exception(e) LOGGER.warning(upload.text) return (fid, {'status' : f'Failure: Reason {upload.reason}'}) def upload_files(self, files=None, pid=None, fprefix=None, force_unlock=False): ''' Uploads multiple files to study with persistentId pid. Returns a list of the original tuples plus JSON responses. Parameters ---------- files : list List contains tuples with (dryadDownloadURL, filename, mimetype, size). pid : str Defaults to self.dvpid, which is generated by calling dryad2dataverse.transfer.Transfer.upload_study(). fprefix : str File location prefix. Defaults to dryad2dataverse.constants.TMP force_unlock : bool Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse `/locks` endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). **Forcible unlock requires a superuser API key.** ''' if not files: files = self.files if not fprefix: fprefix = constants.TMP out = [] for f in files: #out.append(self.upload_file(f[0], f[1], f[2], f[3], # f[4], f[5], pid, fprefix=fprefix)) #out.append(self.upload_file(*[x for x in f], #last item in files is not necessary out.append(self.upload_file(*list(f)[:-1], studyId=pid, fprefix=fprefix, force_unlock=force_unlock)) return out def upload_json(self, studyId=None, dest=None): ''' Uploads Dryad json as a separate file for archival purposes. Parameters ---------- studyId : str Dataverse persistent identifier. Default dryad2dataverse.transfer.Transfer.dvpid, which is only generated on dryad2dataverse.transfer.Transfer.upload_study() dest : str Base URL for transfer. Default dryad2datavese.constants.DVURL ''' if not studyId: studyId = self.dvpid if not dest: dest = constants.DVURL if not self.jsonFlag: url = dest + '/api/datasets/:persistentId/add' pack = io.StringIO(json.dumps(self.dryad.dryadJson)) desc = {'description':'Original JSON from Dryad', 'categories':['Documentation', 'Code']} fname = self.doi[self.doi.rfind('/')+1:].replace('.', '_') payload = {'file': (f'{fname}.json', pack, 'text/plain;charset=UTF-8'), 'jsonData':f'{desc}'} params = {'persistentId':studyId} try: meta = self.session.post(f'{url}', params=params, headers=self.auth, files=payload) #0 because no dryad fid will be zero meta.raise_for_status() self.fileUpRecord.append((0, meta.json())) self.jsonFlag = (0, meta.json()) LOGGER.debug('Successfully uploaded Dryad JSON to %s', studyId) #JSON uploads randomly fail with a Dataverse server.log error of #\"A system exception occurred during an invocation on EJB . . .\" #Not reproducible, so errors will only be written to the log. #Jesus. except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('Unable to upload Dryad JSON to %s', studyId) LOGGER.error('ERROR message: %s', meta.text) LOGGER.exception(err) #And further checking as to what is happening self.fileUpRecord.append((0, {'status':'Failure: Unable to upload Dryad JSON'})) if not isinstance(self.dryad.dryadJson, dict): LOGGER.error('Dryad JSON is not a dictionary') except Exception as err: LOGGER.error('Unable to upload Dryad JSON') LOGGER.exception(err) def delete_dv_file(self, dvfid, dvurl=None, key=None)->bool: #WTAF curl -u $API_TOKEN: -X DELETE #https://$HOSTNAME/dvn/api/data-deposit/v1.1/swordv2/edit-media/file/123 ''' Deletes files from Dataverse target given a dataverse file ID. This information is unknowable unless discovered by dryad2dataverse.monitor.Monitor or by other methods. Returns 1 on success (204 response), or 0 on other response. Parameters ---------- dvurl : str Base URL of dataverse instance. Defaults to dryad2dataverse.constants.DVURL. dvfid : str Dataverse file ID number. key : str API key ''' if not dvurl: dvurl = constants.DVURL if not key: key = constants.APIKEY delme = self.session.delete(f'{dvurl}/dvn/api/data-deposit/v1.1/swordv2/edit-media' f'/file/{dvfid}', auth=(key, '')) if delme.status_code == 204: self.fileDelRecord.append(dvfid) return 1 return 0 def delete_dv_files(self, dvfids=None, dvurl=None, key=None): ''' Deletes all files in list of Dataverse file ids from a Dataverse installation. Parameters ---------- dvfids : list List of Dataverse file ids. Defaults to dryad2dataverse.transfer.Transfer.fileDelRecord. dvurl : str Base URL of Dataverse. Defaults to dryad2dataverse.constants.DVURL. key : str API key for Dataverse. Defaults to dryad2dataverse.constants.APIKEY. ''' #if not dvfids: # dvfids = self.fileDelRecord if not dvurl: dvurl = constants.DVURL if not key: key = constants.APIKEY for fid in dvfids: self.delete_dv_file(fid, dvurl, key) auth property \u00b6 Returns datavese authentication header dict. ie: {X-Dataverse-key' : 'APIKEYSTRING'} doi property \u00b6 Returns Dryad DOI. dvpid property \u00b6 Returns Dataverse study persistent ID as str. fileJson property \u00b6 Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated. files property \u00b6 Returns a list of lists with: [Download_location, filename, mimetype, size, description, md5digest] This is mutable; downloading a file will add md5 info if not available. oversize property \u00b6 Returns list of files exceeding Dataverse ingest limit dryad2dataverse.constants.MAX_UPLOAD. __init__(dryad) \u00b6 Creates a dryad2dataverse.transfer.Transfer instance. Parameters: dryad ( Serializer ) \u2013 Source code in src/dryad2dataverse/transfer.py def __init__(self, dryad): ''' Creates a dryad2dataverse.transfer.Transfer instance. Parameters ---------- dryad : dryad2dataverse.serializer.Serializer ''' self.dryad = dryad self._fileJson = None self._files = [list(f) for f in self.dryad.files] #self._files = copy.deepcopy(self.dryad.files) self.fileUpRecord = [] self.fileDelRecord = [] self.dvStudy = None self.jsonFlag = None #Whether or not new json uploaded self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=constants.RETRY_STRATEGY)) delete_dv_file(dvfid, dvurl=None, key=None) \u00b6 Deletes files from Dataverse target given a dataverse file ID. This information is unknowable unless discovered by dryad2dataverse.monitor.Monitor or by other methods. Returns 1 on success (204 response), or 0 on other response. Parameters: dvurl ( str , default: None ) \u2013 Base URL of dataverse instance. Defaults to dryad2dataverse.constants.DVURL. dvfid ( str ) \u2013 Dataverse file ID number. key ( str , default: None ) \u2013 API key Source code in src/dryad2dataverse/transfer.py def delete_dv_file(self, dvfid, dvurl=None, key=None)->bool: #WTAF curl -u $API_TOKEN: -X DELETE #https://$HOSTNAME/dvn/api/data-deposit/v1.1/swordv2/edit-media/file/123 ''' Deletes files from Dataverse target given a dataverse file ID. This information is unknowable unless discovered by dryad2dataverse.monitor.Monitor or by other methods. Returns 1 on success (204 response), or 0 on other response. Parameters ---------- dvurl : str Base URL of dataverse instance. Defaults to dryad2dataverse.constants.DVURL. dvfid : str Dataverse file ID number. key : str API key ''' if not dvurl: dvurl = constants.DVURL if not key: key = constants.APIKEY delme = self.session.delete(f'{dvurl}/dvn/api/data-deposit/v1.1/swordv2/edit-media' f'/file/{dvfid}', auth=(key, '')) if delme.status_code == 204: self.fileDelRecord.append(dvfid) return 1 return 0 delete_dv_files(dvfids=None, dvurl=None, key=None) \u00b6 Deletes all files in list of Dataverse file ids from a Dataverse installation. Parameters: dvfids ( list , default: None ) \u2013 List of Dataverse file ids. Defaults to dryad2dataverse.transfer.Transfer.fileDelRecord. dvurl ( str , default: None ) \u2013 Base URL of Dataverse. Defaults to dryad2dataverse.constants.DVURL. key ( str , default: None ) \u2013 API key for Dataverse. Defaults to dryad2dataverse.constants.APIKEY. Source code in src/dryad2dataverse/transfer.py def delete_dv_files(self, dvfids=None, dvurl=None, key=None): ''' Deletes all files in list of Dataverse file ids from a Dataverse installation. Parameters ---------- dvfids : list List of Dataverse file ids. Defaults to dryad2dataverse.transfer.Transfer.fileDelRecord. dvurl : str Base URL of Dataverse. Defaults to dryad2dataverse.constants.DVURL. key : str API key for Dataverse. Defaults to dryad2dataverse.constants.APIKEY. ''' #if not dvfids: # dvfids = self.fileDelRecord if not dvurl: dvurl = constants.DVURL if not key: key = constants.APIKEY for fid in dvfids: self.delete_dv_file(fid, dvurl, key) download_file(url=None, filename=None, tmp=None, size=None, chk=None, timeout=45, **kwargs) \u00b6 Downloads a file via requests streaming and saves to constants.TMP. returns checksum on success and an exception on failure. Parameters: url ( str , default: None ) \u2013 URL of download. filename ( str , default: None ) \u2013 Output file name. timeout ( int , default: 45 ) \u2013 Requests timeout. tmp ( str , default: None ) \u2013 Temporary directory for downloads. Defaults to dryad2dataverse.constants.TMP. size ( int , default: None ) \u2013 Reported file size in bytes. Defaults to dryad2dataverse.constants.MAX_UPLOAD. chk ( str , default: None ) \u2013 checksum of file (if available and known). timeout ( int , default: 45 ) \u2013 timeout in seconds kwargs ( dict , default: {} ) \u2013 digest_type ( str ) \u2013 checksum type (ie, md5, sha-256, etc) Source code in src/dryad2dataverse/transfer.py def download_file(self, url=None, filename=None, tmp=None, size=None, chk=None, timeout=45, **kwargs): ''' Downloads a file via requests streaming and saves to constants.TMP. returns checksum on success and an exception on failure. Parameters ---------- url : str URL of download. filename : str Output file name. timeout : int Requests timeout. tmp : str Temporary directory for downloads. Defaults to dryad2dataverse.constants.TMP. size : int Reported file size in bytes. Defaults to dryad2dataverse.constants.MAX_UPLOAD. chk : str checksum of file (if available and known). timeout : int timeout in seconds kwargs : dict Other parameters ---------------- digest_type : str checksum type (ie, md5, sha-256, etc) ''' LOGGER.debug('Start download sequence') LOGGER.debug('MAX SIZE = %s', constants.MAX_UPLOAD) LOGGER.debug('Filename: %s, size=%s', filename, size) if not tmp: tmp = constants.TMP if tmp.endswith(os.sep): tmp = tmp[:-1] if size: if size > constants.MAX_UPLOAD: #TOO BIG LOGGER.warning('%s: File %s exceeds ' 'Dataverse MAX_UPLOAD size. Skipping download.', self.doi, filename) md5 = 'this_file_is_too_big_to_upload__' #HA HA for i in self._files: if url == i[0]: i[-1] = md5 LOGGER.debug('Stop download sequence with large file skip') return md5 try: down = self.session.get(url, timeout=timeout, stream=True) down.raise_for_status() with open(f'{tmp}{os.sep}{filename}', 'wb') as fi: for chunk in down.iter_content(chunk_size=8192): fi.write(chunk) #verify size #https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python' if size: checkSize = os.stat(f'{tmp}{os.sep}{filename}').st_size if checkSize != size: try: raise exceptions.DownloadSizeError('Download size does not match ' 'reported size') except exceptions.DownloadSizeError as e: LOGGER.exception(e) raise #now check the md5 md5 = None if chk and kwargs.get('digest_type') in HASHTABLE: md5 = Transfer._check_md5(f'{tmp}{os.sep}{filename}', kwargs['digest_type']) if md5 != chk: try: raise exceptions.HashError(f'Hex digest mismatch: {md5} : {chk}') #is this really what I want to do on a bad checksum? except exceptions.HashError as e: LOGGER.exception(e) raise for i in self._files: if url == i[0]: i[-1] = md5 LOGGER.debug('Complete download sequence') #This doesn't actually return an md5, just the hash value return md5 except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.critical('Unable to download %s', url) LOGGER.exception(err) raise exceptions.DataverseDownloadError download_files(files=None) \u00b6 Bulk downloader for files. Parameters: files ( list , default: None ) \u2013 Items in list can be tuples or list with a minimum of: (dryaddownloadurl, filenamewithoutpath, [md5sum]) The md5 sum should be the last member of the tuple. Defaults to self.files. Notes Normally used without arguments to download all the associated files with a Dryad study. Source code in src/dryad2dataverse/transfer.py def download_files(self, files=None): ''' Bulk downloader for files. Parameters ---------- files : list Items in list can be tuples or list with a minimum of: `(dryaddownloadurl, filenamewithoutpath, [md5sum])` The md5 sum should be the last member of the tuple. Defaults to self.files. Notes ----- Normally used without arguments to download all the associated files with a Dryad study. ''' if not files: files = self.files try: for f in files: self.download_file(url=f[0], filename=f[1], mimetype=f[2], size=f[3], descr=f[4], digest_type=f[5], chk=f[-1]) except exceptions.DataverseDownloadError as e: LOGGER.exception('Unable to download file with info %s\\n%s', f, e) raise file_lock_check(study, dv_url, apikey=None, count=0) \u00b6 Checks for a study lock Returns True if locked. Normally used to check if processing is completed. As tabular processing halts file ingest, there should be no locks on a Dataverse study before performing a data file upload. Parameters: study ( str ) \u2013 Persistent indentifer of study. dv_url ( str ) \u2013 URL to base Dataverse installation. apikey ( str , default: None ) \u2013 API key for user. If not present authorization defaults to self.auth. count ( int , default: 0 ) \u2013 Number of times the function has been called. Logs lock messages only on 0. Source code in src/dryad2dataverse/transfer.py def file_lock_check(self, study, dv_url, apikey=None, count=0): ''' Checks for a study lock Returns True if locked. Normally used to check if processing is completed. As tabular processing halts file ingest, there should be no locks on a Dataverse study before performing a data file upload. Parameters ---------- study : str Persistent indentifer of study. dv_url : str URL to base Dataverse installation. apikey : str API key for user. If not present authorization defaults to self.auth. count : int Number of times the function has been called. Logs lock messages only on 0. ''' if dv_url.endswith('/'): dv_url = dv_url[:-1] if apikey: headers = {'X-Dataverse-key': apikey} else: headers = self.auth headers.update(USER_AGENT) params = {'persistentId': study} try: lock_status = self.session.get(f'{dv_url}/api/datasets/:persistentId/locks', headers=headers, params=params, timeout=300) lock_status.raise_for_status() if lock_status.json().get('data'): if count == 0: LOGGER.warning('Study %s has been locked', study) LOGGER.warning('Lock info:\\n%s', lock_status.json()) return True return False except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('Unable to detect lock status for %s', study) LOGGER.error('ERROR message: %s', lock_status.text) LOGGER.exception(err) #return True #Should I raise here? raise force_notab_unlock(study, dv_url, apikey=None) \u00b6 Checks for a study lock and forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Forcible unlocks require a superuser API key. Parameters: study ( str ) \u2013 Persistent indentifer of study. dv_url ( str ) \u2013 URL to base Dataverse installation. apikey ( str , default: None ) \u2013 API key for user. If not present authorization defaults to self.auth. Source code in src/dryad2dataverse/transfer.py def force_notab_unlock(self, study, dv_url, apikey=None): ''' Checks for a study lock and forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. **Forcible unlocks require a superuser API key.** Parameters ---------- study : str Persistent indentifer of study. dv_url : str URL to base Dataverse installation. apikey : str API key for user. If not present authorization defaults to self.auth. ''' if dv_url.endswith('/'): dv_url = dv_url[:-1] if apikey: headers = {'X-Dataverse-key': apikey} else: headers = self.auth headers.update(USER_AGENT) params = {'persistentId': study} lock_status = self.session.get(f'{dv_url}/api/datasets/:persistentId/locks', headers=headers, params=params, timeout=300) lock_status.raise_for_status() if lock_status.json()['data']: LOGGER.warning('Study %s has been locked', study) LOGGER.warning('Lock info:\\n%s', lock_status.json()) force_unlock = self.session.delete(f'{dv_url}/api/datasets/:persistentId/locks', params=params, headers=headers, timeout=300) force_unlock.raise_for_status() LOGGER.warning('Lock removed for %s', study) LOGGER.warning('Lock status:\\n %s', force_unlock.json()) set_correct_date(url=None, hdl=None, d_type='distributionDate', apikey=None) \u00b6 Sets \u201ccorrect\u201d publication date for Dataverse. Parameters: url ( str , default: None ) \u2013 Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL hdl ( str , default: None ) \u2013 Persistent indentifier for Dataverse study. Defaults to Transfer.dvpid (which can be None if the study has not yet been uploaded). d_type ( str , default: 'distributionDate' ) \u2013 Date type. One of \u2018distributionDate\u2019, \u2018productionDate\u2019, `dateOfDeposit\u2019. Default \u2018distributionDate\u2019. apikey ( str , default: None ) \u2013 Default dryad2dataverse.constants.APIKEY. Notes dryad2dataverse.serializer maps Dryad \u2018publicationDate\u2019 to Dataverse \u2018distributionDate\u2019 (see serializer.py ~line 675). Dataverse citation date default is \u201c:publicationDate\u201d. See Dataverse API reference: https://guides.dataverse.org/en/4.20/api/native-api.html#id54 . Source code in src/dryad2dataverse/transfer.py def set_correct_date(self, url=None, hdl=None, d_type='distributionDate', apikey=None): ''' Sets \"correct\" publication date for Dataverse. Parameters ---------- url : str Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL hdl : str Persistent indentifier for Dataverse study. Defaults to Transfer.dvpid (which can be None if the study has not yet been uploaded). d_type : str Date type. One of 'distributionDate', 'productionDate', `dateOfDeposit'. Default 'distributionDate'. apikey : str Default dryad2dataverse.constants.APIKEY. Notes ----- dryad2dataverse.serializer maps Dryad 'publicationDate' to Dataverse 'distributionDate' (see serializer.py ~line 675). Dataverse citation date default is \":publicationDate\". See Dataverse API reference: <https://guides.dataverse.org/en/4.20/api/native-api.html#id54>. ''' try: if not url: url = constants.DVURL if not hdl: hdl = self.dvpid headers = {'X-Dataverse-key' : apikey} if apikey: headers = {'X-Dataverse-key' : apikey} else: headers = {'X-Dataverse-key' : constants.APIKEY} headers.update(USER_AGENT) params = {'persistentId': hdl} set_date = self.session.put(f'{url}/api/datasets/:persistentId/citationdate', headers=headers, data=d_type, params=params, timeout=45) set_date.raise_for_status() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.warning('Unable to set citation date for %s', hdl) LOGGER.warning(err) LOGGER.warning(set_date.text) test_api_key(url=None, apikey=None) \u00b6 Tests for an expired API key and raises dryad2dataverse.exceptions.Dryad2dataverseBadApiKeyError the API key is bad. Ignores other HTTP errors. Parameters: url ( str , default: None ) \u2013 Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL apikey ( str , default: None ) \u2013 Default dryad2dataverse.constants.APIKEY. Source code in src/dryad2dataverse/transfer.py def test_api_key(self, url=None, apikey=None): ''' Tests for an expired API key and raises dryad2dataverse.exceptions.Dryad2dataverseBadApiKeyError the API key is bad. Ignores other HTTP errors. Parameters ---------- url : str Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL apikey : str Default dryad2dataverse.constants.APIKEY. ''' #API validity check appears to come before a PID validity check params = {'persistentId': 'doi:000/000/000'} # PID is irrelevant if not url: url = constants.DVURL headers = {'X-Dataverse-key': apikey if apikey else constants.APIKEY} headers.update(USER_AGENT) bad_test = self.session.get(f'{url}/api/datasets/:persistentId', headers=headers, params=params) #There's an extra space in the message which Harvard #will probably find out about, so . . . if bad_test.json().get('message').startswith('Bad api key'): try: raise exceptions.DataverseBadApiKeyError('Bad API key') except exceptions.DataverseBadApiKeyError as e: LOGGER.critical('API key has expired or is otherwise invalid') LOGGER.exception(e) #LOGGER.exception(traceback.format_exc()) #not really necessary raise try: #other errors bad_test.raise_for_status() except requests.exceptions.HTTPError: pass except Exception as e: LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise upload_file(dryadUrl=None, filename=None, mimetype=None, size=None, descr=None, hashtype=None, digest=None, studyId=None, dest=None, fprefix=None, force_unlock=False, timeout=300) \u00b6 Uploads file to Dataverse study. Returns a tuple of the dryadFid (or None) and Dataverse JSON from the POST request. Failures produce JSON with different status messages rather than raising an exception. Parameters: filename ( str , default: None ) \u2013 Filename (not including path). mimetype ( str , default: None ) \u2013 Mimetype of file. size ( int , default: None ) \u2013 Size in bytes. studyId ( str , default: None ) \u2013 Persistent Dataverse study identifier. Defaults to Transfer.dvpid. dest ( str , default: None ) \u2013 Destination dataverse installation url. Defaults to constants.DVURL. hashtype \u2013 original Dryad hash type fprefix ( str , default: None ) \u2013 Path to file, not including a trailing slash. timeout ( int , default: 300 ) \u2013 Timeout in seconds for POST request. Default 300. dryadUrl ( str , default: None ) \u2013 Dryad download URL if you want to include a Dryad file id. force_unlock ( bool , default: False ) \u2013 Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse /locks endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). Forcible unlock requires a superuser API key. Source code in src/dryad2dataverse/transfer.py def upload_file(self, dryadUrl=None, filename=None, mimetype=None, size=None, descr=None, hashtype=None, #md5=None, studyId=None, dest=None, digest=None, studyId=None, dest=None, fprefix=None, force_unlock=False, timeout=300): ''' Uploads file to Dataverse study. Returns a tuple of the dryadFid (or None) and Dataverse JSON from the POST request. Failures produce JSON with different status messages rather than raising an exception. Parameters ---------- filename : str Filename (not including path). mimetype : str Mimetype of file. size : int Size in bytes. studyId : str Persistent Dataverse study identifier. Defaults to Transfer.dvpid. dest : str Destination dataverse installation url. Defaults to constants.DVURL. hashtype: str original Dryad hash type fprefix : str Path to file, not including a trailing slash. timeout : int Timeout in seconds for POST request. Default 300. dryadUrl : str Dryad download URL if you want to include a Dryad file id. force_unlock : bool Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse `/locks` endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). **Forcible unlock requires a superuser API key.** ''' #return locals() #TODONE remove above if not studyId: studyId = self.dvpid if not dest: dest = constants.DVURL if not fprefix: fprefix = constants.TMP if dryadUrl: fid = dryadUrl.strip('/download') fid = int(fid[fid.rfind('/')+1:]) else: fid = 0 #dummy fid for non-Dryad use params = {'persistentId' : studyId} upfile = fprefix + os.sep + filename[:] badExt = filename[filename.rfind('.'):].lower() #Descriptions are technically possible, although how to add #them is buried in Dryad's API documentation dv4meta = {'label' : filename[:], 'description' : descr} #if mimetype == 'application/zip' or filename.lower().endswith('.zip'): if mimetype == 'application/zip' or badExt in constants.NOTAB: mimetype = 'application/octet-stream' # stop unzipping automatically filename += '.NOPROCESS' # Also screw with their naming convention #debug log about file names to see what is up with XSLX #see doi:10.5061/dryad.z8w9ghxb6 LOGGER.debug('File renamed to %s for upload', filename) if size >= constants.MAX_UPLOAD: fail = (fid, {'status' : 'Failure: MAX_UPLOAD size exceeded'}) self.fileUpRecord.append(fail) LOGGER.warning('%s: File %s of ' 'size %s exceeds ' 'Dataverse MAX_UPLOAD size. Skipping.', self.doi, filename, size) return fail fields = {'file': (filename, open(upfile, 'rb'), mimetype)} fields.update({'jsonData': f'{dv4meta}'}) multi = MultipartEncoder(fields=fields) ctype = {'Content-type' : multi.content_type} tmphead = self.auth.copy() tmphead.update(ctype) tmphead.update(USER_AGENT) url = dest + '/api/datasets/:persistentId/add' try: upload = self.session.post(url, params=params, headers=tmphead, data=multi, timeout=timeout) #print(upload.text) upload.raise_for_status() self.fileUpRecord.append((fid, upload.json())) upmd5 = upload.json()['data']['files'][0]['dataFile']['checksum']['value'] #Dataverse hash type _type = upload.json()['data']['files'][0]['dataFile']['checksum']['type'] if _type.lower() != hashtype.lower(): comparator = self._check_md5(upfile, _type.lower()) else: comparator = digest #if hashtype.lower () != 'md5': # #get an md5 because dataverse uses md5s. Or most of them do anyway. # #One day this will be rewritten properly. # md5 = self._check_md5(filename, 'md5') #else: # md5 = digest #if md5 and (upmd5 != md5): if upmd5 != comparator: try: raise exceptions.HashError(f'{_type} mismatch:\\nlocal: {comparator}\\nuploaded: {upmd5}') except exceptions.HashError as e: LOGGER.exception(e) raise #Make damn sure that the study isn't locked because of #tab file processing ##SPSS files still process despite spoofing MIME and extension ##so there's also a forcible unlock check #fid = upload.json()['data']['files'][0]['dataFile']['id'] #fid not required for unlock #self.force_notab_unlock(studyId, dest, fid) if force_unlock: self.force_notab_unlock(studyId, dest) else: count = 0 wait = True while wait: wait = self.file_lock_check(studyId, dest, count=count) if wait: time.sleep(15) # Don't hit it too often count += 1 return (fid, upload.json()) except Exception as e: LOGGER.exception(e) try: reason = upload.json()['message'] LOGGER.warning(upload.json()) return (fid, {'status' : f'Failure: {reason}'}) except Exception as e: LOGGER.warning('Further exceptions!') LOGGER.exception(e) LOGGER.warning(upload.text) return (fid, {'status' : f'Failure: Reason {upload.reason}'}) upload_files(files=None, pid=None, fprefix=None, force_unlock=False) \u00b6 Uploads multiple files to study with persistentId pid. Returns a list of the original tuples plus JSON responses. Parameters: files ( list , default: None ) \u2013 List contains tuples with (dryadDownloadURL, filename, mimetype, size). pid ( str , default: None ) \u2013 Defaults to self.dvpid, which is generated by calling dryad2dataverse.transfer.Transfer.upload_study(). fprefix ( str , default: None ) \u2013 File location prefix. Defaults to dryad2dataverse.constants.TMP force_unlock ( bool , default: False ) \u2013 Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse /locks endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). Forcible unlock requires a superuser API key. Source code in src/dryad2dataverse/transfer.py def upload_files(self, files=None, pid=None, fprefix=None, force_unlock=False): ''' Uploads multiple files to study with persistentId pid. Returns a list of the original tuples plus JSON responses. Parameters ---------- files : list List contains tuples with (dryadDownloadURL, filename, mimetype, size). pid : str Defaults to self.dvpid, which is generated by calling dryad2dataverse.transfer.Transfer.upload_study(). fprefix : str File location prefix. Defaults to dryad2dataverse.constants.TMP force_unlock : bool Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse `/locks` endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). **Forcible unlock requires a superuser API key.** ''' if not files: files = self.files if not fprefix: fprefix = constants.TMP out = [] for f in files: #out.append(self.upload_file(f[0], f[1], f[2], f[3], # f[4], f[5], pid, fprefix=fprefix)) #out.append(self.upload_file(*[x for x in f], #last item in files is not necessary out.append(self.upload_file(*list(f)[:-1], studyId=pid, fprefix=fprefix, force_unlock=force_unlock)) return out upload_json(studyId=None, dest=None) \u00b6 Uploads Dryad json as a separate file for archival purposes. Parameters: studyId ( str , default: None ) \u2013 Dataverse persistent identifier. Default dryad2dataverse.transfer.Transfer.dvpid, which is only generated on dryad2dataverse.transfer.Transfer.upload_study() dest ( str , default: None ) \u2013 Base URL for transfer. Default dryad2datavese.constants.DVURL Source code in src/dryad2dataverse/transfer.py def upload_json(self, studyId=None, dest=None): ''' Uploads Dryad json as a separate file for archival purposes. Parameters ---------- studyId : str Dataverse persistent identifier. Default dryad2dataverse.transfer.Transfer.dvpid, which is only generated on dryad2dataverse.transfer.Transfer.upload_study() dest : str Base URL for transfer. Default dryad2datavese.constants.DVURL ''' if not studyId: studyId = self.dvpid if not dest: dest = constants.DVURL if not self.jsonFlag: url = dest + '/api/datasets/:persistentId/add' pack = io.StringIO(json.dumps(self.dryad.dryadJson)) desc = {'description':'Original JSON from Dryad', 'categories':['Documentation', 'Code']} fname = self.doi[self.doi.rfind('/')+1:].replace('.', '_') payload = {'file': (f'{fname}.json', pack, 'text/plain;charset=UTF-8'), 'jsonData':f'{desc}'} params = {'persistentId':studyId} try: meta = self.session.post(f'{url}', params=params, headers=self.auth, files=payload) #0 because no dryad fid will be zero meta.raise_for_status() self.fileUpRecord.append((0, meta.json())) self.jsonFlag = (0, meta.json()) LOGGER.debug('Successfully uploaded Dryad JSON to %s', studyId) #JSON uploads randomly fail with a Dataverse server.log error of #\"A system exception occurred during an invocation on EJB . . .\" #Not reproducible, so errors will only be written to the log. #Jesus. except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('Unable to upload Dryad JSON to %s', studyId) LOGGER.error('ERROR message: %s', meta.text) LOGGER.exception(err) #And further checking as to what is happening self.fileUpRecord.append((0, {'status':'Failure: Unable to upload Dryad JSON'})) if not isinstance(self.dryad.dryadJson, dict): LOGGER.error('Dryad JSON is not a dictionary') except Exception as err: LOGGER.error('Unable to upload Dryad JSON') LOGGER.exception(err) upload_study(url=None, apikey=None, timeout=45, **kwargs) \u00b6 Uploads Dryad study metadata to target Dataverse or updates existing. Supplying a targetDv kwarg creates a new study and supplying a dvpid kwarg updates a currently existing Dataverse study. Parameters: url ( str , default: None ) \u2013 URL of Dataverse instance. Defaults to constants.DVURL. apikey ( str , default: None ) \u2013 API key of user. Defaults to contants.APIKEY. timeout ( int , default: 45 ) \u2013 timeout on POST request. kwargs ( dict , default: {} ) \u2013 targetDv ( str ) \u2013 Short name of target dataverse. Required if new dataset. Specify as targetDV=value. dvpid ( str ) \u2013 Dataverse persistent ID (for updating metadata). This is not required for new uploads, specify as dvpid=value Notes One of targetDv or dvpid is required. Source code in src/dryad2dataverse/transfer.py def upload_study(self, url=None, apikey=None, timeout=45, **kwargs): ''' Uploads Dryad study metadata to target Dataverse or updates existing. Supplying a `targetDv` kwarg creates a new study and supplying a `dvpid` kwarg updates a currently existing Dataverse study. Parameters ---------- url : str URL of Dataverse instance. Defaults to constants.DVURL. apikey : str API key of user. Defaults to contants.APIKEY. timeout : int timeout on POST request. kwargs : dict Other parameters ---------------- targetDv : str Short name of target dataverse. Required if new dataset. Specify as targetDV=value. dvpid : str Dataverse persistent ID (for updating metadata). This is not required for new uploads, specify as dvpid=value Notes ----- One of targetDv or dvpid is required. ''' if not url: url = constants.DVURL if not apikey: apikey = constants.APIKEY headers = {'X-Dataverse-key' : apikey} headers.update(USER_AGENT) targetDv = kwargs.get('targetDv') dvpid = kwargs.get('dvpid') #dryFid = kwargs.get('dryFid') #Why did I put this here? if not targetDv and not dvpid: try: raise exceptions.NoTargetError('You must supply one of targetDv \\ (target dataverse) \\ or dvpid (Dataverse persistent ID)') except exceptions.NoTargetError as e: LOGGER.error('No target dataverse or dvpid supplied') LOGGER.exception(e) raise if targetDv and dvpid: try: raise ValueError('Supply only one of targetDv or dvpid') except ValueError as e: LOGGER.exception(e) raise if not dvpid: endpoint = f'{url}/api/dataverses/{targetDv}/datasets' upload = self.session.post(endpoint, headers=headers, json=self.dryad.dvJson, timeout=timeout) LOGGER.debug(upload.text) else: endpoint = f'{url}/api/datasets/:persistentId/versions/:draft' params = {'persistentId':dvpid} #Yes, dataverse uses *different* json for edits upload = self.session.put(endpoint, params=params, headers=headers, json=self.dryad.dvJson['datasetVersion'], timeout=timeout) #self._dvrecord = upload.json() LOGGER.debug(upload.text) try: updata = upload.json() self.dvStudy = updata if updata.get('status') != 'OK': try: raise exceptions.DataverseUploadError(('Status return is not OK.' f'{upload.status_code}: ' f'{upload.reason}. ' f'{upload.request.url} ' f'{upload.text}')) except exceptions.DataverseUploadError as e: LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise exceptions.DataverseUploadError(('Status return is not OK.' f'{upload.status_code}: ' f'{upload.reason}. ' f'{upload.request.url} ' f'{upload.text}')) upload.raise_for_status() except Exception as e: # Only accessible via non-requests exception LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise if targetDv: self.dryad.dvpid = updata['data'].get('persistentId') if dvpid: self.dryad.dvpid = updata['data'].get('datasetPersistentId') return self.dvpid dryad2dataverse.monitor \u00b6 Dryad/Dataverse status tracker. Monitor creates a singleton object which writes to a SQLite database. Methods will (generally) take either a dryad2dataverse.serializer.Serializer instance or dryad2dataverse.transfer.Transfer instance The monitor\u2019s primary function is to allow for state checking for Dryad studies so that files and studies aren\u2019t downloaded unneccessarily. Monitor \u00b6 The Monitor object is a tracker and database updater, so that Dryad files can be monitored and updated over time. Monitor is a singleton, but is not thread-safe. Source code in src/dryad2dataverse/monitor.py class Monitor(): ''' The Monitor object is a tracker and database updater, so that Dryad files can be monitored and updated over time. Monitor is a singleton, but is not thread-safe. ''' __instance = None def __new__(cls, dbase=None, *args, **kwargs): ''' Creates a new singleton instance of Monitor. Also creates a database if existing database is not present. Parameters ---------- dbase : str Path to sqlite3 database. That is: /path/to/file.sqlite3 *args : list **kwargs : dict ''' if cls.__instance is None: cls.__instance = super(Monitor, cls).__new__(cls) cls.__instance.__initialized = False cls.dbase = dbase if not cls.dbase: cls.dbase = constants.DBASE cls.conn = sqlite3.Connection(cls.dbase) cls.cursor = cls.conn.cursor() create = ['CREATE TABLE IF NOT EXISTS dryadStudy \\ (uid INTEGER PRIMARY KEY AUTOINCREMENT, \\ doi TEXT, lastmoddate TEXT, dryadjson TEXT, \\ dvjson TEXT);', 'CREATE TABLE IF NOT EXISTS dryadFiles \\ (dryaduid INTEGER REFERENCES dryadStudy (uid), \\ dryfilesjson TEXT);', 'CREATE TABLE IF NOT EXISTS dvStudy \\ (dryaduid INTEGER references dryadStudy (uid), \\ dvpid TEXT);', 'CREATE TABLE IF NOT EXISTS dvFiles \\ (dryaduid INTEGER references dryadStudy (uid), \\ dryfid INT, \\ drymd5 TEXT, dvfid TEXT, dvmd5 TEXT, \\ dvfilejson TEXT);', 'CREATE TABLE IF NOT EXISTS lastcheck \\ (checkdate TEXT);', 'CREATE TABLE IF NOT EXISTS failed_uploads \\ (dryaduid INTEGER references dryadstudy (uid), \\ dryfid INT, status TEXT);' ] for line in create: cls.cursor.execute(line) cls.conn.commit() LOGGER.info('Using database %s', cls.dbase) return cls.__instance def __init__(self, dbase=None, *args, **kwargs): # remove args and kwargs when you find out how init interacts with new. ''' Initialize the Monitor instance if not instantiated already (ie, Monitor is a singleton). Parameters ---------- dbase : str, default=dryad2datverse.constants.DBASE Complete path to desired location of tracking database (eg: /tmp/test.db). *args : list **kwargs : dict ''' if self.__initialized: return self.__initialized = True if not dbase: self.dbase = constants.DBASE else: self.dbase = dbase def __del__(self): ''' Commits all database transactions on object deletion and closes database. ''' self.conn.commit() self.conn.close() @property def lastmod(self): ''' Returns last modification date from monitor.dbase. ''' self.cursor.execute('SELECT checkdate FROM lastcheck ORDER BY rowid DESC;') last_mod = self.cursor.fetchall() if last_mod: return last_mod[0][0] return None def status(self, serial)->dict: ''' Returns a dictionary with keys 'status' and 'dvpid' and 'notes'. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- `{status :'updated', 'dvpid':'doi://some/ident'}`. Notes ------ `status` is one of 'new', 'identical', 'lastmodsame', 'updated' 'new' is a completely new file. 'identical' The metadata from Dryad is *identical* to the last time the check was run. 'lastmodsame' Dryad lastModificationDate == last modification date in database AND output JSON is different. This can indicate a Dryad API output change, reindexing or something else. But the lastModificationDate is supposed to be an indicator of meaningful change, so this option exists so you can decide what to do given this option 'updated' Indicates changes to lastModificationDate Note that Dryad constantly changes their API output, so the changes may not actually be meaningful. `dvpid` is a Dataverse persistent identifier. `None` in the case of status='new' `notes`: value of Dryad versionChanges field. One of `files_changed` or `metatdata_changed`. Non-null value present only when status is not `new` or `identical`. Note that Dryad has no way to indicate *both* a file and metadata change, so this value reflects only the *last* change in the Dryad state. ''' # Last mod date is indicator of change. # From email w/Ryan Scherle 10 Nov 2020 #The versionNumber updates for either a metadata change or a #file change. Although we save all of these changes internally, our web #interface only displays the versions that have file changes, along #with the most recent metadata. So a dataset that has only two versions #of files listed on the web may actually have several more versions in #the API. # #If your only need is to track when there are changes to a #dataset, you may want to use the `lastModificationDate`, which we have #recently added to our metadata. # #Note that the Dryad API output ISN'T STABLE; they add fields etc. #This means that a comparison of JSON may yield differences even though #metadata is technically \"the same\". Just comparing two dicts doesn't cut #it. ############################# ## Note: by inspection, Dryad outputs JSON that is different ## EVEN IF lastModificationDate is unchanged. (14 January 2022) ## So now what? ############################# doi = serial.dryadJson['identifier'] self.cursor.execute('SELECT * FROM dryadStudy WHERE doi = ?', (doi,)) result = self.cursor.fetchall() if not result: return {'status': 'new', 'dvpid': None, 'notes': ''} # dvjson = json.loads(result[-1][4]) # Check the fresh vs. updated jsons for the keys try: dryaduid = result[-1][0] self.cursor.execute('SELECT dvpid from dvStudy WHERE \\ dryaduid = ?', (dryaduid,)) dvpid = self.cursor.fetchall()[-1][0] serial.dvpid = dvpid except TypeError: try: raise exceptions.DatabaseError except exceptions.DatabaseError as e: LOGGER.error('Dryad DOI : %s. Error finding Dataverse PID', doi) LOGGER.exception(e) raise newfile = copy.deepcopy(serial.dryadJson) testfile = copy.deepcopy(json.loads(result[-1][3])) if newfile == testfile: return {'status': 'identical', 'dvpid': dvpid, 'notes': ''} if newfile['lastModificationDate'] != testfile['lastModificationDate']: return {'status': 'updated', 'dvpid': dvpid, 'notes': newfile['versionChanges']} return {'status': 'lastmodsame', 'dvpid': dvpid, 'notes': newfile.get('versionChanges')} def diff_metadata(self, serial): ''' Analyzes differences in metadata between current serializer instance and last updated serializer instance. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- Returns a list of field changes consisting of: [{key: (old_value, new_value}] or None if no changes. Notes ----- For example: ``` [{'title': ('Cascading effects of algal warming in a freshwater community', 'Cascading effects of algal warming in a freshwater community theatre')} ] ``` ''' if self.status(serial)['status'] == 'updated': self.cursor.execute('SELECT dryadjson from dryadStudy \\ WHERE doi = ?', (serial.dryadJson['identifier'],)) oldJson = json.loads(self.cursor.fetchall()[-1][0]) out = [] for k in serial.dryadJson: if serial.dryadJson[k] != oldJson.get(k): out.append({k: (oldJson.get(k), serial.dryadJson[k])}) return out return None @staticmethod def __added_hashes(oldFiles, newFiles): ''' Checks that two objects in dryad2dataverse.serializer.files format stripped of digestType and digest values are identical. Returns array of files with changed hash. Assumes name, mimeType, size, descr all unchanged, which is not necessarily a valid assumption Parameters ---------- oldFiles : Union[list, tuple] (name, mimeType, size, descr, digestType, digest) newFiles : Union[list, tuple] (name, mimeType, size, descr, digestType, digest) ''' hash_change = [] old = [x[1:-2] for x in oldFiles] #URLs are not permanent old_no_url = [x[1:] for x in oldFiles] for fi in newFiles: if fi[1:-2] in old and fi[1:] not in old_no_url: hash_change.append(fi) return hash_change def diff_files(self, serial): ''' Returns a dict with additions and deletions from previous Dryad to dataverse upload. Because checksums are not necessarily included in Dryad file metadata, this method uses dryad file IDs, size, or whatever is available. If dryad2dataverse.monitor.Monitor.status() indicates a change it will produce dictionary output with a list of additions, deletions or hash changes (ie, identical except for hash changes), as below: `{'add':[dyadfiletuples], 'delete:[dryadfiletuples], 'hash_change': [dryadfiletuples]}` Parameters ---------- serial : dryad2dataverse.serializer.Serializer ''' diffReport = {} if self.status(serial)['status'] == 'new': #do we want to show what needs to be added? return {'add': serial.files} #return {} self.cursor.execute('SELECT uid from dryadStudy WHERE doi = ?', (serial.doi,)) mostRecent = self.cursor.fetchall()[-1][0] self.cursor.execute('SELECT dryfilesjson from dryadFiles WHERE \\ dryaduid = ?', (mostRecent,)) oldFileList = self.cursor.fetchall()[-1][0] if not oldFileList: oldFileList = [] else: out = [] #With Dryad API change, files are paginated #now stored as list for old in json.loads(oldFileList): #for old in oldFileList: oldFiles = old['_embedded'].get('stash:files') # comparing file tuples from dryad2dataverse.serializer. # Maybe JSON is better? # because of code duplication below. for f in oldFiles: #Download links are not persistent. Be warned try: downLink = f['_links']['stash:file-download']['href'] except KeyError: downLink = f['_links']['stash:download']['href'] downLink = f'{constants.DRYURL}{downLink}' name = f['path'] mimeType = f['mimeType'] size = f['size'] descr = f.get('description', '') digestType = f.get('digestType', '') digest = f.get('digest', '') out.append((downLink, name, mimeType, size, descr, digestType, digest)) oldFiles = out newFiles = serial.files[:] # Tests go here #Check for identity first #if returned here there are definitely no changes if (set(oldFiles).issuperset(set(newFiles)) and set(newFiles).issuperset(oldFiles)): return diffReport #filenames for checking hash changes. #Can't use URL or hashes for comparisons because they can change #without warning, despite the fact that the API says that #file IDs are unique. They aren't. Verified by Ryan Scherle at #Dryad December 2021 old_map = {x:{'orig':y, 'no_hash':y[1:4]} for x,y in enumerate(oldFiles)} new_map = {x:{'orig':y, 'no_hash':y[1:4]} for x,y in enumerate(newFiles)} old_no_hash = [old_map[x]['no_hash'] for x in old_map] new_no_hash = [new_map[x]['no_hash'] for x in new_map] #check for added hash only hash_change = Monitor.__added_hashes(oldFiles, newFiles) must = set(old_no_hash).issuperset(set(new_no_hash)) if not must: needsadd = set(new_no_hash) - (set(old_no_hash) & set(new_no_hash)) #Use the map created above to return the full file info diffReport.update({'add': [new_map[new_no_hash.index(x)]['orig'] for x in needsadd]}) must = set(new_no_hash).issuperset(old_no_hash) if not must: needsdel = set(old_no_hash) - (set(new_no_hash) & set(old_no_hash)) diffReport.update({'delete' : [old_map[old_no_hash.index(x)]['orig'] for x in needsdel]}) if hash_change: diffReport.update({'hash_change': hash_change}) return diffReport def get_dv_fid(self, url): ''' Returns str \u2014 the Dataverse file ID from parsing a Dryad file download link. Normally used for determining dataverse file ids for *deletion* in case of dryad file changes. Parameters ---------- url : str *Dryad* file URL in form of 'https://datadryad.org/api/v2/files/385819/download'. ''' fid = url[url.rfind('/', 0, -10)+1:].strip('/download') try: fid = int(fid) except ValueError as e: LOGGER.error('File ID %s is not an integer', fid) LOGGER.exception(e) raise #File IDs are *CHANGEABLE* according to Dryad, Dec 2021 #SQLite default returns are by ROWID ASC, so the last record #returned should still be the correct, ie. most recent, one. #However, just in case, this is now done explicitly. self.cursor.execute('SELECT dvfid, ROWID FROM dvFiles WHERE \\ dryfid = ? ORDER BY ROWID ASC;', (fid,)) dvfid = self.cursor.fetchall() if dvfid: return dvfid[-1][0] return None def get_dv_fids(self, filelist): ''' Returns Dataverse file IDs from a list of Dryad file tuples. Generally, you would use the output from dryad2dataverse.monitor.Monitor.diff_files['delete'] to discover Dataverse file ids for deletion. Parameters ---------- filelist : list List of Dryad file tuples: eg: ``` [('https://datadryad.org/api/v2/files/385819/download', 'GCB_ACG_Mortality_2020.zip', 'application/x-zip-compressed', 23787587), ('https://datadryad.org/api/v2/files/385820/download', 'Readme_ACG_Mortality.txt', 'text/plain', 1350)] ``` ''' fids = [] for f in filelist: fids.append(self.get_dv_fid(f[0])) return fids # return [self.get_dv_fid(f[0]) for f in filelist] def get_json_dvfids(self, serial)->list: ''' Return a list of Dataverse file ids for Dryad JSONs which were uploaded to Dataverse. Normally used to discover the file IDs to remove Dryad JSONs which have changed. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- list ''' self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE doi=?', (serial.doi,)) try: uid = self.cursor.fetchone()[0] self.cursor.execute('SELECT dvfid FROM dvFiles WHERE \\ dryaduid = ? AND dryfid=?', (uid, 0)) jsonfid = [f[0] for f in self.cursor.fetchall()] return jsonfid except TypeError: return [] def update(self, transfer): ''' Updates the Monitor database with information from a dryad2dataverse.transfer.Transfer instance. If a Dryad primary metadata record has changes, it will be deleted from the database. This method should be called after all transfers are completed, including Dryad JSON updates, as the last action for transfer. Parameters ---------- transfer : dryad2dataverse.transfer.Transfer ''' # get the pre-update dryad uid in case we need it. self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE doi = ?', (transfer.dryad.dryadJson['identifier'],)) olduid = self.cursor.fetchone()[0] if olduid: olduid = int(olduid) if self.status(transfer.dryad)['status'] != 'unchanged': doi = transfer.doi lastmod = transfer.dryad.dryadJson.get('lastModificationDate') dryadJson = json.dumps(transfer.dryad.dryadJson) dvJson = json.dumps(transfer.dvStudy) # Update study metadata self.cursor.execute('INSERT INTO dryadStudy \\ (doi, lastmoddate, dryadjson, dvjson) \\ VALUES (?, ?, ?, ?)', (doi, lastmod, dryadJson, dvJson)) self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE \\ doi = ?', (doi,)) dryaduid = self.cursor.fetchone()[0] #if type(dryaduid) != int: if not isinstance(dryaduid, int): try: raise TypeError('Dryad UID is not an integer') except TypeError as e: LOGGER.error(e) raise # Update dryad file json self.cursor.execute('INSERT INTO dryadFiles VALUES (?, ?)', (dryaduid, json.dumps(transfer.dryad.fileJson))) # Update dataverse study map self.cursor.execute('SELECT dvpid FROM dvStudy WHERE \\ dvpid = ?', (transfer.dryad.dvpid,)) if not self.cursor.fetchone(): self.cursor.execute('INSERT INTO dvStudy VALUES (?, ?)', (dryaduid, transfer.dryad.dvpid)) else: self.cursor.execute('UPDATE dvStudy SET dryaduid=?, \\ dvpid=? WHERE dvpid =?', (dryaduid, transfer.dryad.dvpid, transfer.dryad.dvpid)) # Update the files table # Because we want to have a *complete* file list for each # dryaduid, we have to copy any existing old files, # then add and delete. if olduid: self.cursor.execute('SELECT * FROM dvFiles WHERE \\ dryaduid=?', (olduid,)) inserter = self.cursor.fetchall() for rec in inserter: # TODONE FIX THIS #I think it's fixed 11 Feb 21 self.cursor.execute('INSERT INTO dvFiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, rec[1], rec[2], rec[3], rec[4], rec[5])) # insert newly uploaded files for rec in transfer.fileUpRecord: try: dvfid = rec[1]['data']['files'][0]['dataFile']['id'] # Screw you for burying the file ID this deep recMd5 = rec[1]['data']['files'][0]['dataFile']['checksum']['value'] except (KeyError, IndexError) as err: #write to failed uploads table instead status = rec[1].get('status') if not status: LOGGER.error('JSON read error for Dryad file ID %s', rec[0]) LOGGER.error('File %s for DOI %s may not be uploaded', rec[0], transfer.doi) LOGGER.exception(err) msg = {'status': 'Failure: Other non-specific ' 'failure. Check logs'} self.cursor.execute('INSERT INTO failed_uploads VALUES \\ (?, ?, ?);', (dryaduid, rec[0], json.dumps(msg))) continue self.cursor.execute('INSERT INTO failed_uploads VALUES \\ (?, ?, ?);', (dryaduid, rec[0], json.dumps(rec[1]))) LOGGER.warning(type(err)) LOGGER.warning('%s. DOI %s, File ID %s', rec[1].get('status'), transfer.doi, rec[0]) continue # md5s verified during upload step, so they should # match already self.cursor.execute('INSERT INTO dvFiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, rec[0], recMd5, dvfid, recMd5, json.dumps(rec[1]))) # Now the deleted files for rec in transfer.fileDelRecord: # fileDelRecord consists only of [fid,fid2, ...] # Dryad record ID is int not str self.cursor.execute('DELETE FROM dvFiles WHERE dvfid=? \\ AND dryaduid=?', (int(rec), dryaduid)) LOGGER.debug('deleted dryfid = %s, dryaduid = %s', rec, dryaduid) # And lastly, any JSON metadata updates: # NOW WHAT? # JSON has dryfid==0 self.cursor.execute('SELECT * FROM dvfiles WHERE \\ dryfid=? and dryaduid=?', (0, dryaduid)) try: exists = self.cursor.fetchone()[0] # Old metadata must be deleted on a change. if exists: shouldDel = self.status(transfer.dryad)['status'] if shouldDel == 'updated': self.cursor.execute('DELETE FROM dvfiles WHERE \\ dryfid=? and dryaduid=?', (0, dryaduid)) except TypeError: pass if transfer.jsonFlag: # update dryad JSON djson5 = transfer.jsonFlag[1]['data']['files'][0]['dataFile']['checksum']['value'] dfid = transfer.jsonFlag[1]['data']['files'][0]['dataFile']['id'] self.cursor.execute('INSERT INTO dvfiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, 0, djson5, dfid, djson5, json.dumps(transfer.jsonFlag[1]))) self.conn.commit() def set_timestamp(self, curdate=None): ''' Adds current time to the database table. Can be queried and be used for subsequent checking for updates. To query last modification time, use the dataverse2dryad.monitor.Monitor.lastmod attribute. Parameters ---------- curdate : str UTC datetime string in the format suitable for the Dryad API. eg. 2021-01-21T21:42:40Z or .strftime('%Y-%m-%dT%H:%M:%SZ'). ''' #Dryad API uses Zulu time if not curdate: curdate = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ') self.cursor.execute('INSERT INTO lastcheck VALUES (?)', (curdate,)) self.conn.commit() lastmod property \u00b6 Returns last modification date from monitor.dbase. __del__() \u00b6 Commits all database transactions on object deletion and closes database. Source code in src/dryad2dataverse/monitor.py def __del__(self): ''' Commits all database transactions on object deletion and closes database. ''' self.conn.commit() self.conn.close() __init__(dbase=None, *args, **kwargs) \u00b6 Initialize the Monitor instance if not instantiated already (ie, Monitor is a singleton). Parameters: dbase ( str , default: dryad2datverse.constants.DBASE ) \u2013 Complete path to desired location of tracking database (eg: /tmp/test.db). *args ( list , default: () ) \u2013 **kwargs ( dict , default: {} ) \u2013 Source code in src/dryad2dataverse/monitor.py def __init__(self, dbase=None, *args, **kwargs): # remove args and kwargs when you find out how init interacts with new. ''' Initialize the Monitor instance if not instantiated already (ie, Monitor is a singleton). Parameters ---------- dbase : str, default=dryad2datverse.constants.DBASE Complete path to desired location of tracking database (eg: /tmp/test.db). *args : list **kwargs : dict ''' if self.__initialized: return self.__initialized = True if not dbase: self.dbase = constants.DBASE else: self.dbase = dbase __new__(dbase=None, *args, **kwargs) \u00b6 Creates a new singleton instance of Monitor. Also creates a database if existing database is not present. Parameters: dbase ( str , default: None ) \u2013 Path to sqlite3 database. That is: /path/to/file.sqlite3 *args ( list , default: () ) \u2013 **kwargs ( dict , default: {} ) \u2013 Source code in src/dryad2dataverse/monitor.py def __new__(cls, dbase=None, *args, **kwargs): ''' Creates a new singleton instance of Monitor. Also creates a database if existing database is not present. Parameters ---------- dbase : str Path to sqlite3 database. That is: /path/to/file.sqlite3 *args : list **kwargs : dict ''' if cls.__instance is None: cls.__instance = super(Monitor, cls).__new__(cls) cls.__instance.__initialized = False cls.dbase = dbase if not cls.dbase: cls.dbase = constants.DBASE cls.conn = sqlite3.Connection(cls.dbase) cls.cursor = cls.conn.cursor() create = ['CREATE TABLE IF NOT EXISTS dryadStudy \\ (uid INTEGER PRIMARY KEY AUTOINCREMENT, \\ doi TEXT, lastmoddate TEXT, dryadjson TEXT, \\ dvjson TEXT);', 'CREATE TABLE IF NOT EXISTS dryadFiles \\ (dryaduid INTEGER REFERENCES dryadStudy (uid), \\ dryfilesjson TEXT);', 'CREATE TABLE IF NOT EXISTS dvStudy \\ (dryaduid INTEGER references dryadStudy (uid), \\ dvpid TEXT);', 'CREATE TABLE IF NOT EXISTS dvFiles \\ (dryaduid INTEGER references dryadStudy (uid), \\ dryfid INT, \\ drymd5 TEXT, dvfid TEXT, dvmd5 TEXT, \\ dvfilejson TEXT);', 'CREATE TABLE IF NOT EXISTS lastcheck \\ (checkdate TEXT);', 'CREATE TABLE IF NOT EXISTS failed_uploads \\ (dryaduid INTEGER references dryadstudy (uid), \\ dryfid INT, status TEXT);' ] for line in create: cls.cursor.execute(line) cls.conn.commit() LOGGER.info('Using database %s', cls.dbase) return cls.__instance diff_files(serial) \u00b6 Returns a dict with additions and deletions from previous Dryad to dataverse upload. Because checksums are not necessarily included in Dryad file metadata, this method uses dryad file IDs, size, or whatever is available. If dryad2dataverse.monitor.Monitor.status() indicates a change it will produce dictionary output with a list of additions, deletions or hash changes (ie, identical except for hash changes), as below: {'add':[dyadfiletuples], 'delete:[dryadfiletuples], 'hash_change': [dryadfiletuples]} Parameters: serial ( Serializer ) \u2013 Source code in src/dryad2dataverse/monitor.py def diff_files(self, serial): ''' Returns a dict with additions and deletions from previous Dryad to dataverse upload. Because checksums are not necessarily included in Dryad file metadata, this method uses dryad file IDs, size, or whatever is available. If dryad2dataverse.monitor.Monitor.status() indicates a change it will produce dictionary output with a list of additions, deletions or hash changes (ie, identical except for hash changes), as below: `{'add':[dyadfiletuples], 'delete:[dryadfiletuples], 'hash_change': [dryadfiletuples]}` Parameters ---------- serial : dryad2dataverse.serializer.Serializer ''' diffReport = {} if self.status(serial)['status'] == 'new': #do we want to show what needs to be added? return {'add': serial.files} #return {} self.cursor.execute('SELECT uid from dryadStudy WHERE doi = ?', (serial.doi,)) mostRecent = self.cursor.fetchall()[-1][0] self.cursor.execute('SELECT dryfilesjson from dryadFiles WHERE \\ dryaduid = ?', (mostRecent,)) oldFileList = self.cursor.fetchall()[-1][0] if not oldFileList: oldFileList = [] else: out = [] #With Dryad API change, files are paginated #now stored as list for old in json.loads(oldFileList): #for old in oldFileList: oldFiles = old['_embedded'].get('stash:files') # comparing file tuples from dryad2dataverse.serializer. # Maybe JSON is better? # because of code duplication below. for f in oldFiles: #Download links are not persistent. Be warned try: downLink = f['_links']['stash:file-download']['href'] except KeyError: downLink = f['_links']['stash:download']['href'] downLink = f'{constants.DRYURL}{downLink}' name = f['path'] mimeType = f['mimeType'] size = f['size'] descr = f.get('description', '') digestType = f.get('digestType', '') digest = f.get('digest', '') out.append((downLink, name, mimeType, size, descr, digestType, digest)) oldFiles = out newFiles = serial.files[:] # Tests go here #Check for identity first #if returned here there are definitely no changes if (set(oldFiles).issuperset(set(newFiles)) and set(newFiles).issuperset(oldFiles)): return diffReport #filenames for checking hash changes. #Can't use URL or hashes for comparisons because they can change #without warning, despite the fact that the API says that #file IDs are unique. They aren't. Verified by Ryan Scherle at #Dryad December 2021 old_map = {x:{'orig':y, 'no_hash':y[1:4]} for x,y in enumerate(oldFiles)} new_map = {x:{'orig':y, 'no_hash':y[1:4]} for x,y in enumerate(newFiles)} old_no_hash = [old_map[x]['no_hash'] for x in old_map] new_no_hash = [new_map[x]['no_hash'] for x in new_map] #check for added hash only hash_change = Monitor.__added_hashes(oldFiles, newFiles) must = set(old_no_hash).issuperset(set(new_no_hash)) if not must: needsadd = set(new_no_hash) - (set(old_no_hash) & set(new_no_hash)) #Use the map created above to return the full file info diffReport.update({'add': [new_map[new_no_hash.index(x)]['orig'] for x in needsadd]}) must = set(new_no_hash).issuperset(old_no_hash) if not must: needsdel = set(old_no_hash) - (set(new_no_hash) & set(old_no_hash)) diffReport.update({'delete' : [old_map[old_no_hash.index(x)]['orig'] for x in needsdel]}) if hash_change: diffReport.update({'hash_change': hash_change}) return diffReport diff_metadata(serial) \u00b6 Analyzes differences in metadata between current serializer instance and last updated serializer instance. Parameters: serial ( Serializer ) \u2013 Returns: Returns a list of field changes consisting of: \u2013 [{key: (old_value, new_value}] or None if no changes. \u2013 Notes For example: [{'title': ('Cascading effects of algal warming in a freshwater community', 'Cascading effects of algal warming in a freshwater community theatre')} ] Source code in src/dryad2dataverse/monitor.py def diff_metadata(self, serial): ''' Analyzes differences in metadata between current serializer instance and last updated serializer instance. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- Returns a list of field changes consisting of: [{key: (old_value, new_value}] or None if no changes. Notes ----- For example: ``` [{'title': ('Cascading effects of algal warming in a freshwater community', 'Cascading effects of algal warming in a freshwater community theatre')} ] ``` ''' if self.status(serial)['status'] == 'updated': self.cursor.execute('SELECT dryadjson from dryadStudy \\ WHERE doi = ?', (serial.dryadJson['identifier'],)) oldJson = json.loads(self.cursor.fetchall()[-1][0]) out = [] for k in serial.dryadJson: if serial.dryadJson[k] != oldJson.get(k): out.append({k: (oldJson.get(k), serial.dryadJson[k])}) return out return None get_dv_fid(url) \u00b6 Returns str \u2014 the Dataverse file ID from parsing a Dryad file download link. Normally used for determining dataverse file ids for deletion in case of dryad file changes. Parameters: url ( str ) \u2013 Dryad file URL in form of \u2018https://datadryad.org/api/v2/files/385819/download\u2019. Source code in src/dryad2dataverse/monitor.py def get_dv_fid(self, url): ''' Returns str \u2014 the Dataverse file ID from parsing a Dryad file download link. Normally used for determining dataverse file ids for *deletion* in case of dryad file changes. Parameters ---------- url : str *Dryad* file URL in form of 'https://datadryad.org/api/v2/files/385819/download'. ''' fid = url[url.rfind('/', 0, -10)+1:].strip('/download') try: fid = int(fid) except ValueError as e: LOGGER.error('File ID %s is not an integer', fid) LOGGER.exception(e) raise #File IDs are *CHANGEABLE* according to Dryad, Dec 2021 #SQLite default returns are by ROWID ASC, so the last record #returned should still be the correct, ie. most recent, one. #However, just in case, this is now done explicitly. self.cursor.execute('SELECT dvfid, ROWID FROM dvFiles WHERE \\ dryfid = ? ORDER BY ROWID ASC;', (fid,)) dvfid = self.cursor.fetchall() if dvfid: return dvfid[-1][0] return None get_dv_fids(filelist) \u00b6 Returns Dataverse file IDs from a list of Dryad file tuples. Generally, you would use the output from dryad2dataverse.monitor.Monitor.diff_files[\u2018delete\u2019] to discover Dataverse file ids for deletion. Parameters: filelist ( list ) \u2013 List of Dryad file tuples: eg: [('https://datadryad.org/api/v2/files/385819/download', 'GCB_ACG_Mortality_2020.zip', 'application/x-zip-compressed', 23787587), ('https://datadryad.org/api/v2/files/385820/download', 'Readme_ACG_Mortality.txt', 'text/plain', 1350)] Source code in src/dryad2dataverse/monitor.py def get_dv_fids(self, filelist): ''' Returns Dataverse file IDs from a list of Dryad file tuples. Generally, you would use the output from dryad2dataverse.monitor.Monitor.diff_files['delete'] to discover Dataverse file ids for deletion. Parameters ---------- filelist : list List of Dryad file tuples: eg: ``` [('https://datadryad.org/api/v2/files/385819/download', 'GCB_ACG_Mortality_2020.zip', 'application/x-zip-compressed', 23787587), ('https://datadryad.org/api/v2/files/385820/download', 'Readme_ACG_Mortality.txt', 'text/plain', 1350)] ``` ''' fids = [] for f in filelist: fids.append(self.get_dv_fid(f[0])) return fids get_json_dvfids(serial) \u00b6 Return a list of Dataverse file ids for Dryad JSONs which were uploaded to Dataverse. Normally used to discover the file IDs to remove Dryad JSONs which have changed. Parameters: serial ( Serializer ) \u2013 Returns: list \u2013 Source code in src/dryad2dataverse/monitor.py def get_json_dvfids(self, serial)->list: ''' Return a list of Dataverse file ids for Dryad JSONs which were uploaded to Dataverse. Normally used to discover the file IDs to remove Dryad JSONs which have changed. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- list ''' self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE doi=?', (serial.doi,)) try: uid = self.cursor.fetchone()[0] self.cursor.execute('SELECT dvfid FROM dvFiles WHERE \\ dryaduid = ? AND dryfid=?', (uid, 0)) jsonfid = [f[0] for f in self.cursor.fetchall()] return jsonfid except TypeError: return [] set_timestamp(curdate=None) \u00b6 Adds current time to the database table. Can be queried and be used for subsequent checking for updates. To query last modification time, use the dataverse2dryad.monitor.Monitor.lastmod attribute. Parameters: curdate ( str , default: None ) \u2013 UTC datetime string in the format suitable for the Dryad API. eg. 2021-01-21T21:42:40Z or .strftime(\u2018%Y-%m-%dT%H:%M:%SZ\u2019). Source code in src/dryad2dataverse/monitor.py def set_timestamp(self, curdate=None): ''' Adds current time to the database table. Can be queried and be used for subsequent checking for updates. To query last modification time, use the dataverse2dryad.monitor.Monitor.lastmod attribute. Parameters ---------- curdate : str UTC datetime string in the format suitable for the Dryad API. eg. 2021-01-21T21:42:40Z or .strftime('%Y-%m-%dT%H:%M:%SZ'). ''' #Dryad API uses Zulu time if not curdate: curdate = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ') self.cursor.execute('INSERT INTO lastcheck VALUES (?)', (curdate,)) self.conn.commit() status(serial) \u00b6 Returns a dictionary with keys \u2018status\u2019 and \u2018dvpid\u2019 and \u2018notes\u2019. Parameters: serial ( dryad2dataverse.serializer.Serializer ) \u2013 Returns: `{status :'updated', 'dvpid':'doi://some/ident'}`. \u2013 Notes status is one of \u2018new\u2019, \u2018identical\u2019, \u2018lastmodsame\u2019, \u2018updated\u2019 \u2018new\u2019 is a completely new file. \u2018identical\u2019 The metadata from Dryad is identical to the last time the check was run. \u2018lastmodsame\u2019 Dryad lastModificationDate == last modification date in database AND output JSON is different. This can indicate a Dryad API output change, reindexing or something else. But the lastModificationDate is supposed to be an indicator of meaningful change, so this option exists so you can decide what to do given this option \u2018updated\u2019 Indicates changes to lastModificationDate Note that Dryad constantly changes their API output, so the changes may not actually be meaningful. dvpid is a Dataverse persistent identifier. None in the case of status=\u2019new\u2019 notes : value of Dryad versionChanges field. One of files_changed or metatdata_changed . Non-null value present only when status is not new or identical . Note that Dryad has no way to indicate both a file and metadata change, so this value reflects only the last change in the Dryad state. Source code in src/dryad2dataverse/monitor.py def status(self, serial)->dict: ''' Returns a dictionary with keys 'status' and 'dvpid' and 'notes'. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- `{status :'updated', 'dvpid':'doi://some/ident'}`. Notes ------ `status` is one of 'new', 'identical', 'lastmodsame', 'updated' 'new' is a completely new file. 'identical' The metadata from Dryad is *identical* to the last time the check was run. 'lastmodsame' Dryad lastModificationDate == last modification date in database AND output JSON is different. This can indicate a Dryad API output change, reindexing or something else. But the lastModificationDate is supposed to be an indicator of meaningful change, so this option exists so you can decide what to do given this option 'updated' Indicates changes to lastModificationDate Note that Dryad constantly changes their API output, so the changes may not actually be meaningful. `dvpid` is a Dataverse persistent identifier. `None` in the case of status='new' `notes`: value of Dryad versionChanges field. One of `files_changed` or `metatdata_changed`. Non-null value present only when status is not `new` or `identical`. Note that Dryad has no way to indicate *both* a file and metadata change, so this value reflects only the *last* change in the Dryad state. ''' # Last mod date is indicator of change. # From email w/Ryan Scherle 10 Nov 2020 #The versionNumber updates for either a metadata change or a #file change. Although we save all of these changes internally, our web #interface only displays the versions that have file changes, along #with the most recent metadata. So a dataset that has only two versions #of files listed on the web may actually have several more versions in #the API. # #If your only need is to track when there are changes to a #dataset, you may want to use the `lastModificationDate`, which we have #recently added to our metadata. # #Note that the Dryad API output ISN'T STABLE; they add fields etc. #This means that a comparison of JSON may yield differences even though #metadata is technically \"the same\". Just comparing two dicts doesn't cut #it. ############################# ## Note: by inspection, Dryad outputs JSON that is different ## EVEN IF lastModificationDate is unchanged. (14 January 2022) ## So now what? ############################# doi = serial.dryadJson['identifier'] self.cursor.execute('SELECT * FROM dryadStudy WHERE doi = ?', (doi,)) result = self.cursor.fetchall() if not result: return {'status': 'new', 'dvpid': None, 'notes': ''} # dvjson = json.loads(result[-1][4]) # Check the fresh vs. updated jsons for the keys try: dryaduid = result[-1][0] self.cursor.execute('SELECT dvpid from dvStudy WHERE \\ dryaduid = ?', (dryaduid,)) dvpid = self.cursor.fetchall()[-1][0] serial.dvpid = dvpid except TypeError: try: raise exceptions.DatabaseError except exceptions.DatabaseError as e: LOGGER.error('Dryad DOI : %s. Error finding Dataverse PID', doi) LOGGER.exception(e) raise newfile = copy.deepcopy(serial.dryadJson) testfile = copy.deepcopy(json.loads(result[-1][3])) if newfile == testfile: return {'status': 'identical', 'dvpid': dvpid, 'notes': ''} if newfile['lastModificationDate'] != testfile['lastModificationDate']: return {'status': 'updated', 'dvpid': dvpid, 'notes': newfile['versionChanges']} return {'status': 'lastmodsame', 'dvpid': dvpid, 'notes': newfile.get('versionChanges')} update(transfer) \u00b6 Updates the Monitor database with information from a dryad2dataverse.transfer.Transfer instance. If a Dryad primary metadata record has changes, it will be deleted from the database. This method should be called after all transfers are completed, including Dryad JSON updates, as the last action for transfer. Parameters: transfer ( Transfer ) \u2013 Source code in src/dryad2dataverse/monitor.py def update(self, transfer): ''' Updates the Monitor database with information from a dryad2dataverse.transfer.Transfer instance. If a Dryad primary metadata record has changes, it will be deleted from the database. This method should be called after all transfers are completed, including Dryad JSON updates, as the last action for transfer. Parameters ---------- transfer : dryad2dataverse.transfer.Transfer ''' # get the pre-update dryad uid in case we need it. self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE doi = ?', (transfer.dryad.dryadJson['identifier'],)) olduid = self.cursor.fetchone()[0] if olduid: olduid = int(olduid) if self.status(transfer.dryad)['status'] != 'unchanged': doi = transfer.doi lastmod = transfer.dryad.dryadJson.get('lastModificationDate') dryadJson = json.dumps(transfer.dryad.dryadJson) dvJson = json.dumps(transfer.dvStudy) # Update study metadata self.cursor.execute('INSERT INTO dryadStudy \\ (doi, lastmoddate, dryadjson, dvjson) \\ VALUES (?, ?, ?, ?)', (doi, lastmod, dryadJson, dvJson)) self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE \\ doi = ?', (doi,)) dryaduid = self.cursor.fetchone()[0] #if type(dryaduid) != int: if not isinstance(dryaduid, int): try: raise TypeError('Dryad UID is not an integer') except TypeError as e: LOGGER.error(e) raise # Update dryad file json self.cursor.execute('INSERT INTO dryadFiles VALUES (?, ?)', (dryaduid, json.dumps(transfer.dryad.fileJson))) # Update dataverse study map self.cursor.execute('SELECT dvpid FROM dvStudy WHERE \\ dvpid = ?', (transfer.dryad.dvpid,)) if not self.cursor.fetchone(): self.cursor.execute('INSERT INTO dvStudy VALUES (?, ?)', (dryaduid, transfer.dryad.dvpid)) else: self.cursor.execute('UPDATE dvStudy SET dryaduid=?, \\ dvpid=? WHERE dvpid =?', (dryaduid, transfer.dryad.dvpid, transfer.dryad.dvpid)) # Update the files table # Because we want to have a *complete* file list for each # dryaduid, we have to copy any existing old files, # then add and delete. if olduid: self.cursor.execute('SELECT * FROM dvFiles WHERE \\ dryaduid=?', (olduid,)) inserter = self.cursor.fetchall() for rec in inserter: # TODONE FIX THIS #I think it's fixed 11 Feb 21 self.cursor.execute('INSERT INTO dvFiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, rec[1], rec[2], rec[3], rec[4], rec[5])) # insert newly uploaded files for rec in transfer.fileUpRecord: try: dvfid = rec[1]['data']['files'][0]['dataFile']['id'] # Screw you for burying the file ID this deep recMd5 = rec[1]['data']['files'][0]['dataFile']['checksum']['value'] except (KeyError, IndexError) as err: #write to failed uploads table instead status = rec[1].get('status') if not status: LOGGER.error('JSON read error for Dryad file ID %s', rec[0]) LOGGER.error('File %s for DOI %s may not be uploaded', rec[0], transfer.doi) LOGGER.exception(err) msg = {'status': 'Failure: Other non-specific ' 'failure. Check logs'} self.cursor.execute('INSERT INTO failed_uploads VALUES \\ (?, ?, ?);', (dryaduid, rec[0], json.dumps(msg))) continue self.cursor.execute('INSERT INTO failed_uploads VALUES \\ (?, ?, ?);', (dryaduid, rec[0], json.dumps(rec[1]))) LOGGER.warning(type(err)) LOGGER.warning('%s. DOI %s, File ID %s', rec[1].get('status'), transfer.doi, rec[0]) continue # md5s verified during upload step, so they should # match already self.cursor.execute('INSERT INTO dvFiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, rec[0], recMd5, dvfid, recMd5, json.dumps(rec[1]))) # Now the deleted files for rec in transfer.fileDelRecord: # fileDelRecord consists only of [fid,fid2, ...] # Dryad record ID is int not str self.cursor.execute('DELETE FROM dvFiles WHERE dvfid=? \\ AND dryaduid=?', (int(rec), dryaduid)) LOGGER.debug('deleted dryfid = %s, dryaduid = %s', rec, dryaduid) # And lastly, any JSON metadata updates: # NOW WHAT? # JSON has dryfid==0 self.cursor.execute('SELECT * FROM dvfiles WHERE \\ dryfid=? and dryaduid=?', (0, dryaduid)) try: exists = self.cursor.fetchone()[0] # Old metadata must be deleted on a change. if exists: shouldDel = self.status(transfer.dryad)['status'] if shouldDel == 'updated': self.cursor.execute('DELETE FROM dvfiles WHERE \\ dryfid=? and dryaduid=?', (0, dryaduid)) except TypeError: pass if transfer.jsonFlag: # update dryad JSON djson5 = transfer.jsonFlag[1]['data']['files'][0]['dataFile']['checksum']['value'] dfid = transfer.jsonFlag[1]['data']['files'][0]['dataFile']['id'] self.cursor.execute('INSERT INTO dvfiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, 0, djson5, dfid, djson5, json.dumps(transfer.jsonFlag[1]))) self.conn.commit() dryad2dataverse.handlers \u00b6 Custom log handlers for sending log information to recipients. SSLSMTPHandler \u00b6 Bases: SMTPHandler An SSL handler for logging.handlers Source code in src/dryad2dataverse/handlers.py class SSLSMTPHandler(SMTPHandler): ''' An SSL handler for logging.handlers ''' def emit(self, record:logging.LogRecord): ''' Emit a record while using an SSL mail server. Parameters ---------- record : logging.LogRecord ''' #Praise be to #https://stackoverflow.com/questions/36937461/ #how-can-i-send-an-email-using-python-loggings- #smtphandler-and-ssl try: port = self.mailport if not port: port = smtplib.SMTP_PORT smtp = smtplib.SMTP_SSL(self.mailhost, port) msg = self.format(record) out = EmailMessage() out['Subject'] = self.getSubject(record) out['From'] = self.fromaddr out['To'] = self.toaddrs out.set_content(msg) #global rec2 #rec2 = record if self.username: smtp.login(self.username, self.password) #smtp.sendmail(self.fromaddr, self.toaddrs, msg) #Attempting to send using smtp.sendmail as above #results in messages with no text, so use smtp.send_message(out) smtp.quit() except (KeyboardInterrupt, SystemExit): raise except: # pylint: disable=bare-except self.handleError(record) emit(record) \u00b6 Emit a record while using an SSL mail server. Parameters: record ( LogRecord ) \u2013 Source code in src/dryad2dataverse/handlers.py def emit(self, record:logging.LogRecord): ''' Emit a record while using an SSL mail server. Parameters ---------- record : logging.LogRecord ''' #Praise be to #https://stackoverflow.com/questions/36937461/ #how-can-i-send-an-email-using-python-loggings- #smtphandler-and-ssl try: port = self.mailport if not port: port = smtplib.SMTP_PORT smtp = smtplib.SMTP_SSL(self.mailhost, port) msg = self.format(record) out = EmailMessage() out['Subject'] = self.getSubject(record) out['From'] = self.fromaddr out['To'] = self.toaddrs out.set_content(msg) #global rec2 #rec2 = record if self.username: smtp.login(self.username, self.password) #smtp.sendmail(self.fromaddr, self.toaddrs, msg) #Attempting to send using smtp.sendmail as above #results in messages with no text, so use smtp.send_message(out) smtp.quit() except (KeyboardInterrupt, SystemExit): raise except: # pylint: disable=bare-except self.handleError(record) dryad2dataverse.exceptions \u00b6 Custom exceptions for error handling. DatabaseError \u00b6 Bases: Dryad2DataverseError Tracking database error. Source code in src/dryad2dataverse/exceptions.py class DatabaseError(Dryad2DataverseError): ''' Tracking database error. ''' DataverseBadApiKeyError \u00b6 Bases: Dryad2DataverseError Returned on not OK respose (ie, request.request.json()[\u2018message\u2019] == \u2018Bad api key \u2018). Source code in src/dryad2dataverse/exceptions.py class DataverseBadApiKeyError(Dryad2DataverseError): ''' Returned on not OK respose (ie, request.request.json()['message'] == 'Bad api key '). ''' DataverseDownloadError \u00b6 Bases: Dryad2DataverseError Returned on not OK respose (ie, not requests.status_code == 200). Source code in src/dryad2dataverse/exceptions.py class DataverseDownloadError(Dryad2DataverseError): ''' Returned on not OK respose (ie, not requests.status_code == 200). ''' DataverseUploadError \u00b6 Bases: Dryad2DataverseError Returned on not OK respose (ie, not requests.status_code == 200). Source code in src/dryad2dataverse/exceptions.py class DataverseUploadError(Dryad2DataverseError): ''' Returned on not OK respose (ie, not requests.status_code == 200). ''' DownloadSizeError \u00b6 Bases: Dryad2DataverseError Raised when download sizes don\u2019t match reported Dryad file size. Source code in src/dryad2dataverse/exceptions.py class DownloadSizeError(Dryad2DataverseError): ''' Raised when download sizes don't match reported Dryad file size. ''' Dryad2DataverseError \u00b6 Bases: Exception Base exception class for Dryad2Dataverse errors. Source code in src/dryad2dataverse/exceptions.py class Dryad2DataverseError(Exception): ''' Base exception class for Dryad2Dataverse errors. ''' HashError \u00b6 Bases: Dryad2DataverseError Raised on hex digest mismatch. Source code in src/dryad2dataverse/exceptions.py class HashError(Dryad2DataverseError): ''' Raised on hex digest mismatch. ''' NoTargetError \u00b6 Bases: Dryad2DataverseError No dataverse target supplied error. Source code in src/dryad2dataverse/exceptions.py class NoTargetError(Dryad2DataverseError): ''' No dataverse target supplied error. ''' dryad2dataverse.constants \u00b6 This module contains the information that configures all the parameters required to transfer data from Dryad to Dataverse. \u201cConstants\u201d may be a bit strong, but the only constant is the presence of change.","title":"API Reference"},{"location":"api_reference/#api-reference","text":"","title":"API Reference"},{"location":"api_reference/#dryad2dataverse","text":"Dryad to Dataverse utilities. No modules are loaded by default, so >>> import dryad2dataverse will work, but will have no effect. Modules included: dryad2dataverse.constants : \u201cConstants\u201d for all modules. URLs, API keys, etc are all here. dryad2dataverse.serializer : Download and serialize Dryad JSON to Dataverse JSON. dryad2dataverse.transfer : metadata and file transfer utilities. dryad2dataverse.monitor : Monitoring and database tools for maintaining a pipeline to Dataverse without unnecessary downloading and file duplication. dryad2dataverse.exceptions : Custom exceptions.","title":"dryad2dataverse"},{"location":"api_reference/#dryad2dataverse.serializer","text":"Serializes Dryad study JSON to Dataverse JSON, as well as producing associated file information.","title":"serializer"},{"location":"api_reference/#dryad2dataverse.serializer.Serializer","text":"Serializes Dryad JSON to Dataverse JSON Source code in src/dryad2dataverse/serializer.py class Serializer(): ''' Serializes Dryad JSON to Dataverse JSON ''' CC0='''<p> <img src=\"https://licensebuttons.net/p/zero/1.0/88x31.png\" title=\"Creative Commons CC0 1.0 Universal Public Domain Dedication. \" style=\"display:none\" onload=\"this.style.display='inline'\" /> <a href=\"http://creativecommons.org/publicdomain/zero/1.0\" title=\"Creative Commons CC0 1.0 Universal Public Domain Dedication. \" target=\"_blank\">CC0 1.0</a> </p>''' def __init__(self, doi): ''' Creates Dryad study metadata instance. Parameters ---------- doi : str DOI of Dryad study. Required for downloading. eg: 'doi:10.5061/dryad.2rbnzs7jp' ''' self.doi = doi self._dryadJson = None self._fileJson = None self._dvJson = None #Serializer objects will be assigned a Dataverse study PID #if dryad2Dataverse.transfer.Transfer() is instantiated self.dvpid = None self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=constants.RETRY_STRATEGY)) LOGGER.debug('Creating Serializer instance object') def fetch_record(self, url=None, timeout=45): ''' Fetches Dryad study record JSON from Dryad V2 API at https://datadryad.org/api/v2/datasets/. Saves to self._dryadJson. Querying Serializer.dryadJson will call this function automatically. Parameters ---------- url : str Dryad instance base URL (eg: 'https://datadryad.org'). timeout : int Timeout in seconds. Default 45. ''' if not url: url = constants.DRYURL try: headers = {'accept':'application/json', 'Content-Type':'application/json'} headers.update(USER_AGENT) doiClean = urllib.parse.quote(self.doi, safe='') resp = self.session.get(f'{url}/api/v2/datasets/{doiClean}', headers=headers, timeout=timeout) resp.raise_for_status() self._dryadJson = resp.json() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('URL error for: %s', url) LOGGER.exception(err) raise @property def id(self): ''' Returns Dryad unique *database* ID, not the DOI. Where the original Dryad JSON is dryadJson, it's the integer trailing portion of: `self.dryadJson['_links']['stash:version']['href']` ''' href = self.dryadJson['_links']['stash:version']['href'] index = href.rfind('/') + 1 return int(href[index:]) @property def dryadJson(self): ''' Returns Dryad study JSON. Will call Serializer.fetch_record() if no JSON is present. ''' if not self._dryadJson: self.fetch_record() return self._dryadJson @dryadJson.setter def dryadJson(self, value=None): ''' Fetches Dryad JSON from Dryad website if not supplied. If supplying it, make sure it's correct or you will run into trouble with processing later. Parameters ---------- value : dict Dryad JSON. ''' if value: self._dryadJson = value else: self.fetch_record() @property def embargo(self)->bool: ''' Check embargo status. Returns boolean True if embargoed. ''' if self.dryadJson.get('curationStatus') == 'Embargoed': return True return False @property def dvJson(self): ''' Returns Dataverse study JSON as dict. ''' self._assemble_json() return self._dvJson @property def fileJson(self, timeout=45): ''' Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated, so the return consists of a list of dicts, one per page. Parameters ---------- timeout : int Request timeout in seconds. ''' if not self._fileJson: try: self._fileJson = [] headers = {'accept':'application/json', 'Content-Type':'application/json'} headers.update(USER_AGENT) fileList = self.session.get(f'{constants.DRYURL}/api/v2/versions/{self.id}/files', headers=headers, timeout=timeout) fileList.raise_for_status() #total = fileList.json()['total'] #Not needed lastPage = fileList.json()['_links']['last']['href'] pages = int(lastPage[lastPage.rfind('=')+1:]) self._fileJson.append(fileList.json()) for i in range(2, pages+1): fileCont = self.session.get(f'{constants.DRYURL}/api/v2' f'/versions/{self.id}/files?page={i}', headers=headers, timeout=timeout) fileCont.raise_for_status() self._fileJson.append(fileCont.json()) except Exception as e: LOGGER.exception(e) raise return self._fileJson @property def files(self)->list: ''' Returns a list of tuples with: (Download_location, filename, mimetype, size, description, digest, digestType ) Digest types include, but are not necessarily limited to: 'adler-32','crc-32','md2','md5','sha-1','sha-256', 'sha-384','sha-512' ''' out = [] for page in self.fileJson: files = page['_embedded'].get('stash:files') if files: for f in files: #This broke with this commit: # https://github.com/datadryad/dryad-app/commit/b8a333ba34b14e55cbc1d7ed5aa4451e0f41db66 #downLink = f['_links']['stash:file-download']['href'] downLink = f['_links']['stash:download']['href'] downLink = f'{constants.DRYURL}{downLink}' name = f['path'] mimeType = f['mimeType'] size = f['size'] #HOW ABOUT PUTTING THIS IN THE DRYAD API PAGE? descr = f.get('description', '') digestType = f.get('digestType', '') #not all files have a digest digest = f.get('digest', '') #Does it matter? If the primary use case is to #compare why not take all the digest types. #md5 = '' #if digestType == 'md5' and digest: # md5 = digest # #nothing in the docs as to algorithms so just picking md5 # #Email from Ryan Scherle 30 Nov 20: supported digest type # #('adler-32','crc-32','md2','md5','sha-1','sha-256', # #'sha-384','sha-512') out.append((downLink, name, mimeType, size, descr, digestType, digest)) return out @property def oversize(self, maxsize=None): ''' Returns a list of Dryad files whose size value exceeds maxsize. Maximum size defaults to dryad2dataverse.constants.MAX_UPLOAD Parameters ---------- maxsize : int Size in bytes in which to flag as oversize. Defaults to constants.MAX_UPLOAD. ''' if not maxsize: maxsize = constants.MAX_UPLOAD toobig = [] for f in self.files: if f[3] >= maxsize: toobig.append(f) return toobig #def_typeclass(self, typeName, multiple, typeClass): @staticmethod def _typeclass(typeName, multiple, typeClass): ''' Creates wrapper around single or multiple Dataverse JSON objects. Returns a dict *without* the Dataverse 'value' key'. Parameters ---------- typeName : str Dataverse typeName (eg: 'author'). multiple : boolean \"Multiple\" value in Dataverse JSON. typeClass : str Dataverse typeClass. Usually one of 'compound', 'primitive, 'controlledVocabulary'). ''' return {'typeName':typeName, 'multiple':multiple, 'typeClass':typeClass} @staticmethod def _convert_generic(**kwargs): ''' Generic dataverse json segment creator of form: ``` {dvField: {'typeName': dvField, 'value': dryField} ``` Suitable for generalized conversions. Only provides fields with multiple: False and typeclass:Primitive Parameters ---------- kwargs : dict Dict from Dataverse JSON segment Other parameters ---------------- dvField : str Dataverse output field dryField : str Dryad JSON field to convert inJson : dict Dryad JSON **segment** to convert addJSON : dict (optional) any other JSON required to complete (cf ISNI) rType : str 'dict' (default) or 'list'. Returns 'value' field as dict value or list. pNotes : str Notes to be prepended to list type values. No trailing space required. ''' dvField = kwargs.get('dvField') dryField = kwargs.get('dryField') inJson = kwargs.get('inJson') addJson = kwargs.get('addJson') pNotes = kwargs.get('pNotes', '') rType = kwargs.get('rType', 'dict') if not dvField or not dryField or not inJson: try: raise ValueError('Incorrect or insufficient fields provided') except ValueError as e: LOGGER.exception(e) raise outfield = inJson.get(dryField) if outfield: outfield = outfield.strip() #if not outfield: # raise ValueError(f'Dryad field {dryField} not found') # If value missing can still concat empty dict if not outfield: return {} if rType == 'list': if pNotes: outfield = [f'{pNotes} {outfield}'] outJson = {dvField:{'typeName':dvField, 'multiple': False, 'typeClass':'primitive', 'value': outfield}} #Simple conversion if not addJson: return outJson #Add JSONs together addJson.update(outJson) return addJson @staticmethod def _convert_author_names(author): ''' Produces required author json fields. This is a special case, requiring concatenation of several fields. Parameters ---------- author : dict dryad['author'] JSON segment. ''' first = author.get('firstName') last = author.get('lastName') if first + last is None: return None authname = f\"{author.get('lastName','')}, {author.get('firstName', '')}\" return {'authorName': {'typeName':'authorName', 'value': authname, 'multiple':False, 'typeClass':'primitive'}} @staticmethod def _convert_keywords(*args): ''' Produces the insane keyword structure Dataverse JSON segment from a list of words. Parameters ---------- args : list List with elements as strings. Generally input is Dryad JSON 'keywords', ie *Dryad['keywords']. Don't forget to expand the list using *. ''' outlist = [] for arg in args: outlist.append({'keywordValue': { 'typeName':'keywordValue', 'value': arg}}) return outlist @staticmethod def _convert_notes(dryJson): ''' Returns formatted notes field with Dryad JSON values that don't really fit anywhere into the Dataverse JSON. Parameters ---------- dryJson : dict Dryad JSON as dict. ''' notes = '' #these fields should be concatenated into notes notable = ['versionNumber', 'versionStatus', 'manuscriptNumber', 'curationStatus', 'preserveCurationStatus', 'invoiceId', 'sharingLink', 'loosenValidation', 'skipDataciteUpdate', 'storageSize', 'visibility', 'skipEmails'] for note in notable: text = dryJson.get(note) if text: text = str(text).strip() if note == 'versionNumber': text = f'<b>Dryad version number:</b> {text}' if note == 'versionStatus': text = f'<b>Version status:</b> {text}' if note == 'manuscriptNumber': text = f'<b>Manuscript number:</b> {text}' if note == 'curationStatus': text = f'<b>Dryad curation status:</b> {text}' if note == 'preserveCurationStatus': text = f'<b>Dryad preserve curation status:</b> {text}' if note == 'invoiceId': text = f'<b>Invoice ID:</b> {text}' if note == 'sharingLink': text = f'<b>Sharing link:</b> {text}' if note == 'loosenValidation': text = f'<b>Loosen validation:</b> {text}' if note == 'skipDataciteUpdate': text = f'<b>Skip Datacite update:</b> {text}' if note == 'storageSize': text = f'<b>Storage size:</b> {text}' if note == 'visibility': text = f'<b>Visibility:</b> {text}' if note == 'skipEmails': text = f'<b>Skip emails:</b> {text}' notes += f'<p>{text}</p>\\n' concat = {'typeName':'notesText', 'multiple':False, 'typeClass': 'primitive', 'value': notes} return concat @staticmethod def _boundingbox(north, south, east, west): ''' Makes a Dataverse bounding box from appropriate coordinates. Returns Dataverse JSON segment as dict. Parameters ---------- north : float south : float east : float west : float Notes ----- Coordinates in decimal degrees. ''' names = ['north', 'south', 'east', 'west'] points = [str(x) for x in [north, south, east, west]] #Because coordinates in DV are strings BFY coords = [(x[0]+'Longitude', {x[0]:x[1]}) for x in zip(names, points)] #Yes, everything is longitude in Dataverse out = [] for coord in coords: out.append(Serializer._convert_generic(inJson=coord[1], dvField=coord[0], #dryField='north')) dryField=[k for k in coord[1].keys()][0])) return out @staticmethod def _convert_geospatial(dryJson): ''' Outputs Dataverse geospatial metadata block. Parameters ---------- dryJson : dict Dryad json as dict. ''' if dryJson.get('locations'): #out = {} coverage = [] box = [] otherCov = None gbbox = None for loc in dryJson.get('locations'): if loc.get('place'): #These are impossible to clean. Going to \"other\" field other = Serializer._convert_generic(inJson=loc, dvField='otherGeographicCoverage', dryField='place') coverage.append(other) if loc.get('point'): #makes size zero bounding box north = loc['point']['latitude'] south = north east = loc['point']['longitude'] west = east point = Serializer._boundingbox(north, south, east, west) box.append(point) if loc.get('box'): north = loc['box']['neLatitude'] south = loc['box']['swLatitude'] east = loc['box']['neLongitude'] west = loc['box']['swLongitude'] area = Serializer._boundingbox(north, south, east, west) box.append(area) if coverage: otherCov = Serializer._typeclass(typeName='geographicCoverage', multiple=True, typeClass='compound') otherCov['value'] = coverage if box: gbbox = Serializer._typeclass(typeName='geographicCoverage', multiple=True, typeClass='compound') gbbox['value'] = box if otherCov or gbbox: gblock = {'geospatial': {'displayName' : 'Geospatial Metadata', 'fields': []}} if otherCov: gblock['geospatial']['fields'].append(otherCov) if gbbox: gblock['geospatial']['fields'].append(gbbox) return gblock return {} def _assemble_json(self, dryJson=None, dvContact=None, dvEmail=None, defContact=True): ''' Assembles Dataverse json from Dryad JSON components. Dataverse JSON is a nightmare, so this function is too. Parameters ---------- dryJson : dict Dryad json as dict. dvContact : str Default Dataverse contact name. dvEmail : str Default Dataverse 4 contact email address. defContact : boolean Flag to include default contact information with record. ''' if not dvContact: dvContact = constants.DV_CONTACT_NAME if not dvEmail: dvEmail = constants.DV_CONTACT_EMAIL if not dryJson: dryJson = self.dryadJson LOGGER.debug(dryJson) #Licence block changes ensure that it will only work with #Dataverse v5.10+ #Go back to previous commits to see the earlier \"standard\" self._dvJson = {'datasetVersion': {'license':{'name': 'CC0 1.0', 'uri': 'http://creativecommons.org/publicdomain/zero/1.0' }, 'termsOfUse': Serializer.CC0, 'metadataBlocks':{'citation': {'displayName': 'Citation Metadata', 'fields': []}, } } } #REQUIRED Dataverse fields #Dryad is a general purpose database; it is hard/impossible to get #Dataverse required subject tags out of their keywords, so: defaultSubj = {'typeName' : 'subject', 'typeClass':'controlledVocabulary', 'multiple': True, 'value' : ['Other']} self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(defaultSubj) reqdTitle = Serializer._convert_generic(inJson=dryJson, dryField='title', dvField='title')['title'] self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(reqdTitle) #authors out = [] for a in dryJson['authors']: reqdAuthor = Serializer._convert_author_names(a) if reqdAuthor: affiliation = Serializer._convert_generic(inJson=a, dvField='authorAffiliation', dryField='affiliation') addOrc = {'authorIdentifierScheme': {'typeName':'authorIdentifierScheme', 'value': 'ORCID', 'typeClass': 'controlledVocabulary', 'multiple':False}} #only ORCID at UBC orcid = Serializer._convert_generic(inJson=a, dvField='authorIdentifier', dryField='orcid', addJson=addOrc) if affiliation: reqdAuthor.update(affiliation) if orcid: reqdAuthor.update(orcid) out.append(reqdAuthor) authors = Serializer._typeclass(typeName='author', multiple=True, typeClass='compound') authors['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(authors) ##rewrite as function:contact out = [] for e in dryJson['authors']: reqdContact = Serializer._convert_generic(inJson=e, dvField='datasetContactEmail', dryField='email') if reqdContact: author = Serializer._convert_author_names(e) author = {'author':author['authorName']['value']} #for passing to function author = Serializer._convert_generic(inJson=author, dvField='datasetContactName', dryField='author') if author: reqdContact.update(author) affiliation = Serializer._convert_generic(inJson=e, dvField='datasetContactAffiliation', dryField='affiliation') if affiliation: reqdContact.update(affiliation) out.append(reqdContact) if defContact: #Adds default contact information the tail of the list defEmail = Serializer._convert_generic(inJson={'em':dvEmail}, dvField='datasetContactEmail', dryField='em') defName = Serializer._convert_generic(inJson={'name':dvContact}, dvField='datasetContactName', dryField='name') defEmail.update(defName) out.append(defEmail) contacts = Serializer._typeclass(typeName='datasetContact', multiple=True, typeClass='compound') contacts['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(contacts) #Description description = Serializer._typeclass(typeName='dsDescription', multiple=True, typeClass='compound') desCat = [('abstract', '<b>Abstract</b><br/>'), ('methods', '<b>Methods</b><br />'), ('usageNotes', '<b>Usage notes</b><br />')] out = [] for desc in desCat: if dryJson.get(desc[0]): descrField = Serializer._convert_generic(inJson=dryJson, dvField='dsDescriptionValue', dryField=desc[0]) descrField['dsDescriptionValue']['value'] = (desc[1] + descrField['dsDescriptionValue']['value']) descDate = Serializer._convert_generic(inJson=dryJson, dvField='dsDescriptionDate', dryField='lastModificationDate') descrField.update(descDate) out.append(descrField) description['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(description) #Granting agencies if dryJson.get('funders'): out = [] for fund in dryJson['funders']: org = Serializer._convert_generic(inJson=fund, dvField='grantNumberAgency', dryField='organization') if fund.get('awardNumber'): fund = Serializer._convert_generic(inJson=fund, dvField='grantNumberValue', dryField='awardNumber') org.update(fund) out.append(org) grants = Serializer._typeclass(typeName='grantNumber', multiple=True, typeClass='compound') grants['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(grants) #Keywords keywords = Serializer._typeclass(typeName='keyword', multiple=True, typeClass='compound') out = [] for key in dryJson.get('keywords', []): #Apparently keywords are not required keydict = {'keyword':key} #because takes a dict kv = Serializer._convert_generic(inJson=keydict, dvField='keywordValue', dryField='keyword') vocab = {'dryad':'Dryad'} voc = Serializer._convert_generic(inJson=vocab, dvField='keywordVocabulary', dryField='dryad') kv.update(voc) out.append(kv) keywords['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(keywords) #modification date moddate = Serializer._convert_generic(inJson=dryJson, dvField='dateOfDeposit', dryField='lastModificationDate') self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(moddate['dateOfDeposit']) #This one isn't nested BFY #distribution date distdate = Serializer._convert_generic(inJson=dryJson, dvField='distributionDate', dryField='publicationDate') self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(distdate['distributionDate']) #Also not nested #publications publications = Serializer._typeclass(typeName='publication', multiple=True, typeClass='compound') #quick and dirty lookup table #TODONE see https://github.com/CDL-Dryad/dryad-app/blob/ #31d17d8dab7ea3bab1256063a1e4d0cb706dd5ec/stash/stash_datacite/ #app/models/stash_datacite/related_identifier.rb #no longer required #lookup = {'IsDerivedFrom':'Is derived from', # 'Cites':'Cites', # 'IsSupplementTo': 'Is supplement to', # 'IsSupplementedBy': 'Is supplemented by'} out = [] if dryJson.get('relatedWorks'): for r in dryJson.get('relatedWorks'): #id = r.get('identifier') #TODONE Verify that changing id to _id has not broken anything: 11Feb21 _id = r.get('identifier') #Note:10 Feb 2021 : some records have identifier = ''. BAD DRYAD. if not _id: continue relationship = r.get('relationship') #idType = r.get('identifierType') #not required in _convert_generic #citation = {'citation': f\"{lookup[relationship]}: {id}\"} citation = {'citation': relationship.capitalize()} pubcite = Serializer._convert_generic(inJson=citation, dvField='publicationCitation', dryField='citation') pubIdType = Serializer._convert_generic(inJson=r, dvField='publicationIDType', dryField='identifierType') #ID type must be lower case pubIdType['publicationIDType']['value'] = pubIdType['publicationIDType']['value'].lower() pubIdType['publicationIDType']['typeClass'] = 'controlledVocabulary' pubUrl = Serializer._convert_generic(inJson=r, dvField='publicationURL', dryField='identifier') #Dryad doesn't just put URLs in their URL field. if pubUrl['publicationURL']['value'].lower().startswith('doi:'): fixurl = 'https://doi.org/' + pubUrl['publicationURL']['value'][4:] pubUrl['publicationURL']['value'] = fixurl LOGGER.debug('Rewrote URLs to be %s', fixurl) #Dryad doesn't validate URL fields to start with http or https. Assume https if not pubUrl['publicationURL']['value'].lower().startswith('htt'): pubUrl['publicationURL']['value'] = ('https://' + pubUrl['publicationURL']['value']) pubcite.update(pubIdType) pubcite.update(pubUrl) out.append(pubcite) publications['value'] = out self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(publications) #notes #go into primary notes field, not DDI self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(Serializer._convert_notes(dryJson)) #Geospatial metadata self._dvJson['datasetVersion']['metadataBlocks'].update(Serializer._convert_geospatial(dryJson)) #DOI --> agency/identifier doi = Serializer._convert_generic(inJson=dryJson, dryField='identifier', dvField='otherIdValue') doi.update(Serializer._convert_generic(inJson={'agency':'Dryad'}, dryField='agency', dvField='otherIdAgency')) agency = Serializer._typeclass(typeName='otherId', multiple=True, typeClass='compound') agency['value'] = [doi] self._dvJson['datasetVersion']['metadataBlocks']['citation']['fields'].append(agency)","title":"Serializer"},{"location":"api_reference/#dryad2dataverse.serializer.Serializer.dryadJson","text":"Returns Dryad study JSON. Will call Serializer.fetch_record() if no JSON is present.","title":"dryadJson"},{"location":"api_reference/#dryad2dataverse.serializer.Serializer.dvJson","text":"Returns Dataverse study JSON as dict.","title":"dvJson"},{"location":"api_reference/#dryad2dataverse.serializer.Serializer.embargo","text":"Check embargo status. Returns boolean True if embargoed.","title":"embargo"},{"location":"api_reference/#dryad2dataverse.serializer.Serializer.fileJson","text":"Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated, so the return consists of a list of dicts, one per page. Parameters: timeout ( int ) \u2013 Request timeout in seconds.","title":"fileJson"},{"location":"api_reference/#dryad2dataverse.serializer.Serializer.files","text":"Returns a list of tuples with: (Download_location, filename, mimetype, size, description, digest, digestType ) Digest types include, but are not necessarily limited to: \u2018adler-32\u2019,\u2019crc-32\u2019,\u2019md2\u2019,\u2019md5\u2019,\u2019sha-1\u2019,\u2019sha-256\u2019, \u2018sha-384\u2019,\u2019sha-512\u2019","title":"files"},{"location":"api_reference/#dryad2dataverse.serializer.Serializer.id","text":"Returns Dryad unique database ID, not the DOI. Where the original Dryad JSON is dryadJson, it\u2019s the integer trailing portion of: self.dryadJson['_links']['stash:version']['href']","title":"id"},{"location":"api_reference/#dryad2dataverse.serializer.Serializer.oversize","text":"Returns a list of Dryad files whose size value exceeds maxsize. Maximum size defaults to dryad2dataverse.constants.MAX_UPLOAD Parameters: maxsize ( int ) \u2013 Size in bytes in which to flag as oversize. Defaults to constants.MAX_UPLOAD.","title":"oversize"},{"location":"api_reference/#dryad2dataverse.serializer.Serializer.__init__","text":"Creates Dryad study metadata instance. Parameters: doi ( str ) \u2013 DOI of Dryad study. Required for downloading. eg: \u2018doi:10.5061/dryad.2rbnzs7jp\u2019 Source code in src/dryad2dataverse/serializer.py def __init__(self, doi): ''' Creates Dryad study metadata instance. Parameters ---------- doi : str DOI of Dryad study. Required for downloading. eg: 'doi:10.5061/dryad.2rbnzs7jp' ''' self.doi = doi self._dryadJson = None self._fileJson = None self._dvJson = None #Serializer objects will be assigned a Dataverse study PID #if dryad2Dataverse.transfer.Transfer() is instantiated self.dvpid = None self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=constants.RETRY_STRATEGY)) LOGGER.debug('Creating Serializer instance object')","title":"__init__"},{"location":"api_reference/#dryad2dataverse.serializer.Serializer.fetch_record","text":"Fetches Dryad study record JSON from Dryad V2 API at https://datadryad.org/api/v2/datasets/. Saves to self._dryadJson. Querying Serializer.dryadJson will call this function automatically. Parameters: url ( str , default: None ) \u2013 Dryad instance base URL (eg: \u2018https://datadryad.org\u2019). timeout ( int , default: 45 ) \u2013 Timeout in seconds. Default 45. Source code in src/dryad2dataverse/serializer.py def fetch_record(self, url=None, timeout=45): ''' Fetches Dryad study record JSON from Dryad V2 API at https://datadryad.org/api/v2/datasets/. Saves to self._dryadJson. Querying Serializer.dryadJson will call this function automatically. Parameters ---------- url : str Dryad instance base URL (eg: 'https://datadryad.org'). timeout : int Timeout in seconds. Default 45. ''' if not url: url = constants.DRYURL try: headers = {'accept':'application/json', 'Content-Type':'application/json'} headers.update(USER_AGENT) doiClean = urllib.parse.quote(self.doi, safe='') resp = self.session.get(f'{url}/api/v2/datasets/{doiClean}', headers=headers, timeout=timeout) resp.raise_for_status() self._dryadJson = resp.json() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('URL error for: %s', url) LOGGER.exception(err) raise","title":"fetch_record"},{"location":"api_reference/#dryad2dataverse.transfer","text":"This module handles data downloads and uploads from a Dryad instance to a Dataverse instance","title":"transfer"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer","text":"Transfers metadata and data files from a Dryad installation to Dataverse installation. Source code in src/dryad2dataverse/transfer.py class Transfer(): ''' Transfers metadata and data files from a Dryad installation to Dataverse installation. ''' def __init__(self, dryad): ''' Creates a dryad2dataverse.transfer.Transfer instance. Parameters ---------- dryad : dryad2dataverse.serializer.Serializer ''' self.dryad = dryad self._fileJson = None self._files = [list(f) for f in self.dryad.files] #self._files = copy.deepcopy(self.dryad.files) self.fileUpRecord = [] self.fileDelRecord = [] self.dvStudy = None self.jsonFlag = None #Whether or not new json uploaded self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=constants.RETRY_STRATEGY)) def _del__(self): #TODONE: Change name to __del__ to make a destructor '''Expunges files from constants.TMP on deletion''' for f in self.files: if os.path.exists(f'{constants.TMP}{os.sep}{f[1]}'): os.remove(f'{constants.TMP}{os.sep}{f[1]}') def test_api_key(self, url=None, apikey=None): ''' Tests for an expired API key and raises dryad2dataverse.exceptions.Dryad2dataverseBadApiKeyError the API key is bad. Ignores other HTTP errors. Parameters ---------- url : str Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL apikey : str Default dryad2dataverse.constants.APIKEY. ''' #API validity check appears to come before a PID validity check params = {'persistentId': 'doi:000/000/000'} # PID is irrelevant if not url: url = constants.DVURL headers = {'X-Dataverse-key': apikey if apikey else constants.APIKEY} headers.update(USER_AGENT) bad_test = self.session.get(f'{url}/api/datasets/:persistentId', headers=headers, params=params) #There's an extra space in the message which Harvard #will probably find out about, so . . . if bad_test.json().get('message').startswith('Bad api key'): try: raise exceptions.DataverseBadApiKeyError('Bad API key') except exceptions.DataverseBadApiKeyError as e: LOGGER.critical('API key has expired or is otherwise invalid') LOGGER.exception(e) #LOGGER.exception(traceback.format_exc()) #not really necessary raise try: #other errors bad_test.raise_for_status() except requests.exceptions.HTTPError: pass except Exception as e: LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise @property def dvpid(self): ''' Returns Dataverse study persistent ID as str. ''' return self.dryad.dvpid @property def auth(self): ''' Returns datavese authentication header dict. ie: `{X-Dataverse-key' : 'APIKEYSTRING'}` ''' return {'X-Dataverse-key' : constants.APIKEY} @property def fileJson(self): ''' Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated. ''' return self.dryad.fileJson.copy() @property def files(self): ''' Returns a list of lists with: [Download_location, filename, mimetype, size, description, md5digest] This is mutable; downloading a file will add md5 info if not available. ''' return self._files @property def oversize(self): ''' Returns list of files exceeding Dataverse ingest limit dryad2dataverse.constants.MAX_UPLOAD. ''' return self.dryad.oversize @property def doi(self): ''' Returns Dryad DOI. ''' return self.dryad.doi @staticmethod def _dryad_file_id(url:str): ''' Returns Dryad fileID from dryad file download URL as integer. Parameters ---------- url : str Dryad file URL in format 'https://datadryad.org/api/v2/files/385820/download'. ''' fid = url.strip('/download') fid = int(fid[fid.rfind('/')+1:]) return fid @staticmethod def _make_dv_head(apikey): ''' Returns Dataverse authentication header as dict. Parameters ---------- apikey : str Dataverse API key. ''' return {'X-Dataverse-key' : apikey} #@staticmethod def set_correct_date(self, url=None, hdl=None, d_type='distributionDate', apikey=None): ''' Sets \"correct\" publication date for Dataverse. Parameters ---------- url : str Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL hdl : str Persistent indentifier for Dataverse study. Defaults to Transfer.dvpid (which can be None if the study has not yet been uploaded). d_type : str Date type. One of 'distributionDate', 'productionDate', `dateOfDeposit'. Default 'distributionDate'. apikey : str Default dryad2dataverse.constants.APIKEY. Notes ----- dryad2dataverse.serializer maps Dryad 'publicationDate' to Dataverse 'distributionDate' (see serializer.py ~line 675). Dataverse citation date default is \":publicationDate\". See Dataverse API reference: <https://guides.dataverse.org/en/4.20/api/native-api.html#id54>. ''' try: if not url: url = constants.DVURL if not hdl: hdl = self.dvpid headers = {'X-Dataverse-key' : apikey} if apikey: headers = {'X-Dataverse-key' : apikey} else: headers = {'X-Dataverse-key' : constants.APIKEY} headers.update(USER_AGENT) params = {'persistentId': hdl} set_date = self.session.put(f'{url}/api/datasets/:persistentId/citationdate', headers=headers, data=d_type, params=params, timeout=45) set_date.raise_for_status() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.warning('Unable to set citation date for %s', hdl) LOGGER.warning(err) LOGGER.warning(set_date.text) def upload_study(self, url=None, apikey=None, timeout=45, **kwargs): ''' Uploads Dryad study metadata to target Dataverse or updates existing. Supplying a `targetDv` kwarg creates a new study and supplying a `dvpid` kwarg updates a currently existing Dataverse study. Parameters ---------- url : str URL of Dataverse instance. Defaults to constants.DVURL. apikey : str API key of user. Defaults to contants.APIKEY. timeout : int timeout on POST request. kwargs : dict Other parameters ---------------- targetDv : str Short name of target dataverse. Required if new dataset. Specify as targetDV=value. dvpid : str Dataverse persistent ID (for updating metadata). This is not required for new uploads, specify as dvpid=value Notes ----- One of targetDv or dvpid is required. ''' if not url: url = constants.DVURL if not apikey: apikey = constants.APIKEY headers = {'X-Dataverse-key' : apikey} headers.update(USER_AGENT) targetDv = kwargs.get('targetDv') dvpid = kwargs.get('dvpid') #dryFid = kwargs.get('dryFid') #Why did I put this here? if not targetDv and not dvpid: try: raise exceptions.NoTargetError('You must supply one of targetDv \\ (target dataverse) \\ or dvpid (Dataverse persistent ID)') except exceptions.NoTargetError as e: LOGGER.error('No target dataverse or dvpid supplied') LOGGER.exception(e) raise if targetDv and dvpid: try: raise ValueError('Supply only one of targetDv or dvpid') except ValueError as e: LOGGER.exception(e) raise if not dvpid: endpoint = f'{url}/api/dataverses/{targetDv}/datasets' upload = self.session.post(endpoint, headers=headers, json=self.dryad.dvJson, timeout=timeout) LOGGER.debug(upload.text) else: endpoint = f'{url}/api/datasets/:persistentId/versions/:draft' params = {'persistentId':dvpid} #Yes, dataverse uses *different* json for edits upload = self.session.put(endpoint, params=params, headers=headers, json=self.dryad.dvJson['datasetVersion'], timeout=timeout) #self._dvrecord = upload.json() LOGGER.debug(upload.text) try: updata = upload.json() self.dvStudy = updata if updata.get('status') != 'OK': try: raise exceptions.DataverseUploadError(('Status return is not OK.' f'{upload.status_code}: ' f'{upload.reason}. ' f'{upload.request.url} ' f'{upload.text}')) except exceptions.DataverseUploadError as e: LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise exceptions.DataverseUploadError(('Status return is not OK.' f'{upload.status_code}: ' f'{upload.reason}. ' f'{upload.request.url} ' f'{upload.text}')) upload.raise_for_status() except Exception as e: # Only accessible via non-requests exception LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise if targetDv: self.dryad.dvpid = updata['data'].get('persistentId') if dvpid: self.dryad.dvpid = updata['data'].get('datasetPersistentId') return self.dvpid @staticmethod def _check_md5(infile, dig_type): ''' Returns the hex digest of a file (formerly just md5sum). Parameters ---------- infile : str Complete path to target file. dig_type : Union[str, None] Digest type ''' #From Ryan Scherle #When Dryad calculates a digest, it only uses MD5. #But if you have precomputed some other type of digest, we should accept it. #The list of allowed values is: #('adler-32','crc-32','md2','md5','sha-1','sha-256','sha-384','sha-512') #hashlib doesn't support adler-32, crc-32, md2 blocksize = 2**16 #Well, this is inelegant with open(infile, 'rb') as m: #fmd5 = hashlib.md5() ## var name kept for posterity. Maybe refactor if dig_type in ['sha-1', 'sha-256', 'sha-384', 'sha-512', 'md5', 'md2']: if dig_type == 'md2': fmd5 = Crypto.Hash.MD2.new() else: fmd5 = HASHTABLE[dig_type]() fblock = m.read(blocksize) while fblock: fmd5.update(fblock) fblock = m.read(blocksize) return fmd5.hexdigest() if dig_type in ['adler-32', 'crc-32']: fblock = m.read(blocksize) curvalue = HASHTABLE[dig_type](fblock) while fblock: fblock = m.read(blocksize) curvalue = HASHTABLE[dig_type](fblock, curvalue) return curvalue raise exceptions.HashError(f'Unable to determine hash type for{infile}: {dig_type}') def download_file(self, url=None, filename=None, tmp=None, size=None, chk=None, timeout=45, **kwargs): ''' Downloads a file via requests streaming and saves to constants.TMP. returns checksum on success and an exception on failure. Parameters ---------- url : str URL of download. filename : str Output file name. timeout : int Requests timeout. tmp : str Temporary directory for downloads. Defaults to dryad2dataverse.constants.TMP. size : int Reported file size in bytes. Defaults to dryad2dataverse.constants.MAX_UPLOAD. chk : str checksum of file (if available and known). timeout : int timeout in seconds kwargs : dict Other parameters ---------------- digest_type : str checksum type (ie, md5, sha-256, etc) ''' LOGGER.debug('Start download sequence') LOGGER.debug('MAX SIZE = %s', constants.MAX_UPLOAD) LOGGER.debug('Filename: %s, size=%s', filename, size) if not tmp: tmp = constants.TMP if tmp.endswith(os.sep): tmp = tmp[:-1] if size: if size > constants.MAX_UPLOAD: #TOO BIG LOGGER.warning('%s: File %s exceeds ' 'Dataverse MAX_UPLOAD size. Skipping download.', self.doi, filename) md5 = 'this_file_is_too_big_to_upload__' #HA HA for i in self._files: if url == i[0]: i[-1] = md5 LOGGER.debug('Stop download sequence with large file skip') return md5 try: down = self.session.get(url, timeout=timeout, stream=True) down.raise_for_status() with open(f'{tmp}{os.sep}{filename}', 'wb') as fi: for chunk in down.iter_content(chunk_size=8192): fi.write(chunk) #verify size #https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python' if size: checkSize = os.stat(f'{tmp}{os.sep}{filename}').st_size if checkSize != size: try: raise exceptions.DownloadSizeError('Download size does not match ' 'reported size') except exceptions.DownloadSizeError as e: LOGGER.exception(e) raise #now check the md5 md5 = None if chk and kwargs.get('digest_type') in HASHTABLE: md5 = Transfer._check_md5(f'{tmp}{os.sep}{filename}', kwargs['digest_type']) if md5 != chk: try: raise exceptions.HashError(f'Hex digest mismatch: {md5} : {chk}') #is this really what I want to do on a bad checksum? except exceptions.HashError as e: LOGGER.exception(e) raise for i in self._files: if url == i[0]: i[-1] = md5 LOGGER.debug('Complete download sequence') #This doesn't actually return an md5, just the hash value return md5 except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.critical('Unable to download %s', url) LOGGER.exception(err) raise exceptions.DataverseDownloadError def download_files(self, files=None): ''' Bulk downloader for files. Parameters ---------- files : list Items in list can be tuples or list with a minimum of: `(dryaddownloadurl, filenamewithoutpath, [md5sum])` The md5 sum should be the last member of the tuple. Defaults to self.files. Notes ----- Normally used without arguments to download all the associated files with a Dryad study. ''' if not files: files = self.files try: for f in files: self.download_file(url=f[0], filename=f[1], mimetype=f[2], size=f[3], descr=f[4], digest_type=f[5], chk=f[-1]) except exceptions.DataverseDownloadError as e: LOGGER.exception('Unable to download file with info %s\\n%s', f, e) raise def file_lock_check(self, study, dv_url, apikey=None, count=0): ''' Checks for a study lock Returns True if locked. Normally used to check if processing is completed. As tabular processing halts file ingest, there should be no locks on a Dataverse study before performing a data file upload. Parameters ---------- study : str Persistent indentifer of study. dv_url : str URL to base Dataverse installation. apikey : str API key for user. If not present authorization defaults to self.auth. count : int Number of times the function has been called. Logs lock messages only on 0. ''' if dv_url.endswith('/'): dv_url = dv_url[:-1] if apikey: headers = {'X-Dataverse-key': apikey} else: headers = self.auth headers.update(USER_AGENT) params = {'persistentId': study} try: lock_status = self.session.get(f'{dv_url}/api/datasets/:persistentId/locks', headers=headers, params=params, timeout=300) lock_status.raise_for_status() if lock_status.json().get('data'): if count == 0: LOGGER.warning('Study %s has been locked', study) LOGGER.warning('Lock info:\\n%s', lock_status.json()) return True return False except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('Unable to detect lock status for %s', study) LOGGER.error('ERROR message: %s', lock_status.text) LOGGER.exception(err) #return True #Should I raise here? raise def force_notab_unlock(self, study, dv_url, apikey=None): ''' Checks for a study lock and forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. **Forcible unlocks require a superuser API key.** Parameters ---------- study : str Persistent indentifer of study. dv_url : str URL to base Dataverse installation. apikey : str API key for user. If not present authorization defaults to self.auth. ''' if dv_url.endswith('/'): dv_url = dv_url[:-1] if apikey: headers = {'X-Dataverse-key': apikey} else: headers = self.auth headers.update(USER_AGENT) params = {'persistentId': study} lock_status = self.session.get(f'{dv_url}/api/datasets/:persistentId/locks', headers=headers, params=params, timeout=300) lock_status.raise_for_status() if lock_status.json()['data']: LOGGER.warning('Study %s has been locked', study) LOGGER.warning('Lock info:\\n%s', lock_status.json()) force_unlock = self.session.delete(f'{dv_url}/api/datasets/:persistentId/locks', params=params, headers=headers, timeout=300) force_unlock.raise_for_status() LOGGER.warning('Lock removed for %s', study) LOGGER.warning('Lock status:\\n %s', force_unlock.json()) #This is what the file ID was for, in case it can #be implemented again. #According to Harvard, you can't remove the progress bar #for uploaded tab files that squeak through unless you #let them ingest first then reingest them. Oh well. #See: #https://groups.google.com/d/msgid/dataverse-community/ #74caa708-e39b-4259-874d-5b6b74ef9723n%40googlegroups.com #Also, you can't uningest it because it hasn't been #ingested once it's been unlocked. So the commented #code below is useless (for now) #uningest = requests.post(f'{dv_url}/api/files/{fid}/uningest', # headers=headers, # timeout=300) #LOGGER.warning('Ingest halted for file %s for study %s', fid, study) #uningest.raise_for_status() def upload_file(self, dryadUrl=None, filename=None, mimetype=None, size=None, descr=None, hashtype=None, #md5=None, studyId=None, dest=None, digest=None, studyId=None, dest=None, fprefix=None, force_unlock=False, timeout=300): ''' Uploads file to Dataverse study. Returns a tuple of the dryadFid (or None) and Dataverse JSON from the POST request. Failures produce JSON with different status messages rather than raising an exception. Parameters ---------- filename : str Filename (not including path). mimetype : str Mimetype of file. size : int Size in bytes. studyId : str Persistent Dataverse study identifier. Defaults to Transfer.dvpid. dest : str Destination dataverse installation url. Defaults to constants.DVURL. hashtype: str original Dryad hash type fprefix : str Path to file, not including a trailing slash. timeout : int Timeout in seconds for POST request. Default 300. dryadUrl : str Dryad download URL if you want to include a Dryad file id. force_unlock : bool Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse `/locks` endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). **Forcible unlock requires a superuser API key.** ''' #return locals() #TODONE remove above if not studyId: studyId = self.dvpid if not dest: dest = constants.DVURL if not fprefix: fprefix = constants.TMP if dryadUrl: fid = dryadUrl.strip('/download') fid = int(fid[fid.rfind('/')+1:]) else: fid = 0 #dummy fid for non-Dryad use params = {'persistentId' : studyId} upfile = fprefix + os.sep + filename[:] badExt = filename[filename.rfind('.'):].lower() #Descriptions are technically possible, although how to add #them is buried in Dryad's API documentation dv4meta = {'label' : filename[:], 'description' : descr} #if mimetype == 'application/zip' or filename.lower().endswith('.zip'): if mimetype == 'application/zip' or badExt in constants.NOTAB: mimetype = 'application/octet-stream' # stop unzipping automatically filename += '.NOPROCESS' # Also screw with their naming convention #debug log about file names to see what is up with XSLX #see doi:10.5061/dryad.z8w9ghxb6 LOGGER.debug('File renamed to %s for upload', filename) if size >= constants.MAX_UPLOAD: fail = (fid, {'status' : 'Failure: MAX_UPLOAD size exceeded'}) self.fileUpRecord.append(fail) LOGGER.warning('%s: File %s of ' 'size %s exceeds ' 'Dataverse MAX_UPLOAD size. Skipping.', self.doi, filename, size) return fail fields = {'file': (filename, open(upfile, 'rb'), mimetype)} fields.update({'jsonData': f'{dv4meta}'}) multi = MultipartEncoder(fields=fields) ctype = {'Content-type' : multi.content_type} tmphead = self.auth.copy() tmphead.update(ctype) tmphead.update(USER_AGENT) url = dest + '/api/datasets/:persistentId/add' try: upload = self.session.post(url, params=params, headers=tmphead, data=multi, timeout=timeout) #print(upload.text) upload.raise_for_status() self.fileUpRecord.append((fid, upload.json())) upmd5 = upload.json()['data']['files'][0]['dataFile']['checksum']['value'] #Dataverse hash type _type = upload.json()['data']['files'][0]['dataFile']['checksum']['type'] if _type.lower() != hashtype.lower(): comparator = self._check_md5(upfile, _type.lower()) else: comparator = digest #if hashtype.lower () != 'md5': # #get an md5 because dataverse uses md5s. Or most of them do anyway. # #One day this will be rewritten properly. # md5 = self._check_md5(filename, 'md5') #else: # md5 = digest #if md5 and (upmd5 != md5): if upmd5 != comparator: try: raise exceptions.HashError(f'{_type} mismatch:\\nlocal: {comparator}\\nuploaded: {upmd5}') except exceptions.HashError as e: LOGGER.exception(e) raise #Make damn sure that the study isn't locked because of #tab file processing ##SPSS files still process despite spoofing MIME and extension ##so there's also a forcible unlock check #fid = upload.json()['data']['files'][0]['dataFile']['id'] #fid not required for unlock #self.force_notab_unlock(studyId, dest, fid) if force_unlock: self.force_notab_unlock(studyId, dest) else: count = 0 wait = True while wait: wait = self.file_lock_check(studyId, dest, count=count) if wait: time.sleep(15) # Don't hit it too often count += 1 return (fid, upload.json()) except Exception as e: LOGGER.exception(e) try: reason = upload.json()['message'] LOGGER.warning(upload.json()) return (fid, {'status' : f'Failure: {reason}'}) except Exception as e: LOGGER.warning('Further exceptions!') LOGGER.exception(e) LOGGER.warning(upload.text) return (fid, {'status' : f'Failure: Reason {upload.reason}'}) def upload_files(self, files=None, pid=None, fprefix=None, force_unlock=False): ''' Uploads multiple files to study with persistentId pid. Returns a list of the original tuples plus JSON responses. Parameters ---------- files : list List contains tuples with (dryadDownloadURL, filename, mimetype, size). pid : str Defaults to self.dvpid, which is generated by calling dryad2dataverse.transfer.Transfer.upload_study(). fprefix : str File location prefix. Defaults to dryad2dataverse.constants.TMP force_unlock : bool Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse `/locks` endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). **Forcible unlock requires a superuser API key.** ''' if not files: files = self.files if not fprefix: fprefix = constants.TMP out = [] for f in files: #out.append(self.upload_file(f[0], f[1], f[2], f[3], # f[4], f[5], pid, fprefix=fprefix)) #out.append(self.upload_file(*[x for x in f], #last item in files is not necessary out.append(self.upload_file(*list(f)[:-1], studyId=pid, fprefix=fprefix, force_unlock=force_unlock)) return out def upload_json(self, studyId=None, dest=None): ''' Uploads Dryad json as a separate file for archival purposes. Parameters ---------- studyId : str Dataverse persistent identifier. Default dryad2dataverse.transfer.Transfer.dvpid, which is only generated on dryad2dataverse.transfer.Transfer.upload_study() dest : str Base URL for transfer. Default dryad2datavese.constants.DVURL ''' if not studyId: studyId = self.dvpid if not dest: dest = constants.DVURL if not self.jsonFlag: url = dest + '/api/datasets/:persistentId/add' pack = io.StringIO(json.dumps(self.dryad.dryadJson)) desc = {'description':'Original JSON from Dryad', 'categories':['Documentation', 'Code']} fname = self.doi[self.doi.rfind('/')+1:].replace('.', '_') payload = {'file': (f'{fname}.json', pack, 'text/plain;charset=UTF-8'), 'jsonData':f'{desc}'} params = {'persistentId':studyId} try: meta = self.session.post(f'{url}', params=params, headers=self.auth, files=payload) #0 because no dryad fid will be zero meta.raise_for_status() self.fileUpRecord.append((0, meta.json())) self.jsonFlag = (0, meta.json()) LOGGER.debug('Successfully uploaded Dryad JSON to %s', studyId) #JSON uploads randomly fail with a Dataverse server.log error of #\"A system exception occurred during an invocation on EJB . . .\" #Not reproducible, so errors will only be written to the log. #Jesus. except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('Unable to upload Dryad JSON to %s', studyId) LOGGER.error('ERROR message: %s', meta.text) LOGGER.exception(err) #And further checking as to what is happening self.fileUpRecord.append((0, {'status':'Failure: Unable to upload Dryad JSON'})) if not isinstance(self.dryad.dryadJson, dict): LOGGER.error('Dryad JSON is not a dictionary') except Exception as err: LOGGER.error('Unable to upload Dryad JSON') LOGGER.exception(err) def delete_dv_file(self, dvfid, dvurl=None, key=None)->bool: #WTAF curl -u $API_TOKEN: -X DELETE #https://$HOSTNAME/dvn/api/data-deposit/v1.1/swordv2/edit-media/file/123 ''' Deletes files from Dataverse target given a dataverse file ID. This information is unknowable unless discovered by dryad2dataverse.monitor.Monitor or by other methods. Returns 1 on success (204 response), or 0 on other response. Parameters ---------- dvurl : str Base URL of dataverse instance. Defaults to dryad2dataverse.constants.DVURL. dvfid : str Dataverse file ID number. key : str API key ''' if not dvurl: dvurl = constants.DVURL if not key: key = constants.APIKEY delme = self.session.delete(f'{dvurl}/dvn/api/data-deposit/v1.1/swordv2/edit-media' f'/file/{dvfid}', auth=(key, '')) if delme.status_code == 204: self.fileDelRecord.append(dvfid) return 1 return 0 def delete_dv_files(self, dvfids=None, dvurl=None, key=None): ''' Deletes all files in list of Dataverse file ids from a Dataverse installation. Parameters ---------- dvfids : list List of Dataverse file ids. Defaults to dryad2dataverse.transfer.Transfer.fileDelRecord. dvurl : str Base URL of Dataverse. Defaults to dryad2dataverse.constants.DVURL. key : str API key for Dataverse. Defaults to dryad2dataverse.constants.APIKEY. ''' #if not dvfids: # dvfids = self.fileDelRecord if not dvurl: dvurl = constants.DVURL if not key: key = constants.APIKEY for fid in dvfids: self.delete_dv_file(fid, dvurl, key)","title":"Transfer"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.auth","text":"Returns datavese authentication header dict. ie: {X-Dataverse-key' : 'APIKEYSTRING'}","title":"auth"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.doi","text":"Returns Dryad DOI.","title":"doi"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.dvpid","text":"Returns Dataverse study persistent ID as str.","title":"dvpid"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.fileJson","text":"Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated.","title":"fileJson"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.files","text":"Returns a list of lists with: [Download_location, filename, mimetype, size, description, md5digest] This is mutable; downloading a file will add md5 info if not available.","title":"files"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.oversize","text":"Returns list of files exceeding Dataverse ingest limit dryad2dataverse.constants.MAX_UPLOAD.","title":"oversize"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.__init__","text":"Creates a dryad2dataverse.transfer.Transfer instance. Parameters: dryad ( Serializer ) \u2013 Source code in src/dryad2dataverse/transfer.py def __init__(self, dryad): ''' Creates a dryad2dataverse.transfer.Transfer instance. Parameters ---------- dryad : dryad2dataverse.serializer.Serializer ''' self.dryad = dryad self._fileJson = None self._files = [list(f) for f in self.dryad.files] #self._files = copy.deepcopy(self.dryad.files) self.fileUpRecord = [] self.fileDelRecord = [] self.dvStudy = None self.jsonFlag = None #Whether or not new json uploaded self.session = requests.Session() self.session.mount('https://', HTTPAdapter(max_retries=constants.RETRY_STRATEGY))","title":"__init__"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.delete_dv_file","text":"Deletes files from Dataverse target given a dataverse file ID. This information is unknowable unless discovered by dryad2dataverse.monitor.Monitor or by other methods. Returns 1 on success (204 response), or 0 on other response. Parameters: dvurl ( str , default: None ) \u2013 Base URL of dataverse instance. Defaults to dryad2dataverse.constants.DVURL. dvfid ( str ) \u2013 Dataverse file ID number. key ( str , default: None ) \u2013 API key Source code in src/dryad2dataverse/transfer.py def delete_dv_file(self, dvfid, dvurl=None, key=None)->bool: #WTAF curl -u $API_TOKEN: -X DELETE #https://$HOSTNAME/dvn/api/data-deposit/v1.1/swordv2/edit-media/file/123 ''' Deletes files from Dataverse target given a dataverse file ID. This information is unknowable unless discovered by dryad2dataverse.monitor.Monitor or by other methods. Returns 1 on success (204 response), or 0 on other response. Parameters ---------- dvurl : str Base URL of dataverse instance. Defaults to dryad2dataverse.constants.DVURL. dvfid : str Dataverse file ID number. key : str API key ''' if not dvurl: dvurl = constants.DVURL if not key: key = constants.APIKEY delme = self.session.delete(f'{dvurl}/dvn/api/data-deposit/v1.1/swordv2/edit-media' f'/file/{dvfid}', auth=(key, '')) if delme.status_code == 204: self.fileDelRecord.append(dvfid) return 1 return 0","title":"delete_dv_file"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.delete_dv_files","text":"Deletes all files in list of Dataverse file ids from a Dataverse installation. Parameters: dvfids ( list , default: None ) \u2013 List of Dataverse file ids. Defaults to dryad2dataverse.transfer.Transfer.fileDelRecord. dvurl ( str , default: None ) \u2013 Base URL of Dataverse. Defaults to dryad2dataverse.constants.DVURL. key ( str , default: None ) \u2013 API key for Dataverse. Defaults to dryad2dataverse.constants.APIKEY. Source code in src/dryad2dataverse/transfer.py def delete_dv_files(self, dvfids=None, dvurl=None, key=None): ''' Deletes all files in list of Dataverse file ids from a Dataverse installation. Parameters ---------- dvfids : list List of Dataverse file ids. Defaults to dryad2dataverse.transfer.Transfer.fileDelRecord. dvurl : str Base URL of Dataverse. Defaults to dryad2dataverse.constants.DVURL. key : str API key for Dataverse. Defaults to dryad2dataverse.constants.APIKEY. ''' #if not dvfids: # dvfids = self.fileDelRecord if not dvurl: dvurl = constants.DVURL if not key: key = constants.APIKEY for fid in dvfids: self.delete_dv_file(fid, dvurl, key)","title":"delete_dv_files"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.download_file","text":"Downloads a file via requests streaming and saves to constants.TMP. returns checksum on success and an exception on failure. Parameters: url ( str , default: None ) \u2013 URL of download. filename ( str , default: None ) \u2013 Output file name. timeout ( int , default: 45 ) \u2013 Requests timeout. tmp ( str , default: None ) \u2013 Temporary directory for downloads. Defaults to dryad2dataverse.constants.TMP. size ( int , default: None ) \u2013 Reported file size in bytes. Defaults to dryad2dataverse.constants.MAX_UPLOAD. chk ( str , default: None ) \u2013 checksum of file (if available and known). timeout ( int , default: 45 ) \u2013 timeout in seconds kwargs ( dict , default: {} ) \u2013 digest_type ( str ) \u2013 checksum type (ie, md5, sha-256, etc) Source code in src/dryad2dataverse/transfer.py def download_file(self, url=None, filename=None, tmp=None, size=None, chk=None, timeout=45, **kwargs): ''' Downloads a file via requests streaming and saves to constants.TMP. returns checksum on success and an exception on failure. Parameters ---------- url : str URL of download. filename : str Output file name. timeout : int Requests timeout. tmp : str Temporary directory for downloads. Defaults to dryad2dataverse.constants.TMP. size : int Reported file size in bytes. Defaults to dryad2dataverse.constants.MAX_UPLOAD. chk : str checksum of file (if available and known). timeout : int timeout in seconds kwargs : dict Other parameters ---------------- digest_type : str checksum type (ie, md5, sha-256, etc) ''' LOGGER.debug('Start download sequence') LOGGER.debug('MAX SIZE = %s', constants.MAX_UPLOAD) LOGGER.debug('Filename: %s, size=%s', filename, size) if not tmp: tmp = constants.TMP if tmp.endswith(os.sep): tmp = tmp[:-1] if size: if size > constants.MAX_UPLOAD: #TOO BIG LOGGER.warning('%s: File %s exceeds ' 'Dataverse MAX_UPLOAD size. Skipping download.', self.doi, filename) md5 = 'this_file_is_too_big_to_upload__' #HA HA for i in self._files: if url == i[0]: i[-1] = md5 LOGGER.debug('Stop download sequence with large file skip') return md5 try: down = self.session.get(url, timeout=timeout, stream=True) down.raise_for_status() with open(f'{tmp}{os.sep}{filename}', 'wb') as fi: for chunk in down.iter_content(chunk_size=8192): fi.write(chunk) #verify size #https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python' if size: checkSize = os.stat(f'{tmp}{os.sep}{filename}').st_size if checkSize != size: try: raise exceptions.DownloadSizeError('Download size does not match ' 'reported size') except exceptions.DownloadSizeError as e: LOGGER.exception(e) raise #now check the md5 md5 = None if chk and kwargs.get('digest_type') in HASHTABLE: md5 = Transfer._check_md5(f'{tmp}{os.sep}{filename}', kwargs['digest_type']) if md5 != chk: try: raise exceptions.HashError(f'Hex digest mismatch: {md5} : {chk}') #is this really what I want to do on a bad checksum? except exceptions.HashError as e: LOGGER.exception(e) raise for i in self._files: if url == i[0]: i[-1] = md5 LOGGER.debug('Complete download sequence') #This doesn't actually return an md5, just the hash value return md5 except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.critical('Unable to download %s', url) LOGGER.exception(err) raise exceptions.DataverseDownloadError","title":"download_file"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.download_files","text":"Bulk downloader for files. Parameters: files ( list , default: None ) \u2013 Items in list can be tuples or list with a minimum of: (dryaddownloadurl, filenamewithoutpath, [md5sum]) The md5 sum should be the last member of the tuple. Defaults to self.files. Notes Normally used without arguments to download all the associated files with a Dryad study. Source code in src/dryad2dataverse/transfer.py def download_files(self, files=None): ''' Bulk downloader for files. Parameters ---------- files : list Items in list can be tuples or list with a minimum of: `(dryaddownloadurl, filenamewithoutpath, [md5sum])` The md5 sum should be the last member of the tuple. Defaults to self.files. Notes ----- Normally used without arguments to download all the associated files with a Dryad study. ''' if not files: files = self.files try: for f in files: self.download_file(url=f[0], filename=f[1], mimetype=f[2], size=f[3], descr=f[4], digest_type=f[5], chk=f[-1]) except exceptions.DataverseDownloadError as e: LOGGER.exception('Unable to download file with info %s\\n%s', f, e) raise","title":"download_files"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.file_lock_check","text":"Checks for a study lock Returns True if locked. Normally used to check if processing is completed. As tabular processing halts file ingest, there should be no locks on a Dataverse study before performing a data file upload. Parameters: study ( str ) \u2013 Persistent indentifer of study. dv_url ( str ) \u2013 URL to base Dataverse installation. apikey ( str , default: None ) \u2013 API key for user. If not present authorization defaults to self.auth. count ( int , default: 0 ) \u2013 Number of times the function has been called. Logs lock messages only on 0. Source code in src/dryad2dataverse/transfer.py def file_lock_check(self, study, dv_url, apikey=None, count=0): ''' Checks for a study lock Returns True if locked. Normally used to check if processing is completed. As tabular processing halts file ingest, there should be no locks on a Dataverse study before performing a data file upload. Parameters ---------- study : str Persistent indentifer of study. dv_url : str URL to base Dataverse installation. apikey : str API key for user. If not present authorization defaults to self.auth. count : int Number of times the function has been called. Logs lock messages only on 0. ''' if dv_url.endswith('/'): dv_url = dv_url[:-1] if apikey: headers = {'X-Dataverse-key': apikey} else: headers = self.auth headers.update(USER_AGENT) params = {'persistentId': study} try: lock_status = self.session.get(f'{dv_url}/api/datasets/:persistentId/locks', headers=headers, params=params, timeout=300) lock_status.raise_for_status() if lock_status.json().get('data'): if count == 0: LOGGER.warning('Study %s has been locked', study) LOGGER.warning('Lock info:\\n%s', lock_status.json()) return True return False except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('Unable to detect lock status for %s', study) LOGGER.error('ERROR message: %s', lock_status.text) LOGGER.exception(err) #return True #Should I raise here? raise","title":"file_lock_check"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.force_notab_unlock","text":"Checks for a study lock and forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Forcible unlocks require a superuser API key. Parameters: study ( str ) \u2013 Persistent indentifer of study. dv_url ( str ) \u2013 URL to base Dataverse installation. apikey ( str , default: None ) \u2013 API key for user. If not present authorization defaults to self.auth. Source code in src/dryad2dataverse/transfer.py def force_notab_unlock(self, study, dv_url, apikey=None): ''' Checks for a study lock and forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. **Forcible unlocks require a superuser API key.** Parameters ---------- study : str Persistent indentifer of study. dv_url : str URL to base Dataverse installation. apikey : str API key for user. If not present authorization defaults to self.auth. ''' if dv_url.endswith('/'): dv_url = dv_url[:-1] if apikey: headers = {'X-Dataverse-key': apikey} else: headers = self.auth headers.update(USER_AGENT) params = {'persistentId': study} lock_status = self.session.get(f'{dv_url}/api/datasets/:persistentId/locks', headers=headers, params=params, timeout=300) lock_status.raise_for_status() if lock_status.json()['data']: LOGGER.warning('Study %s has been locked', study) LOGGER.warning('Lock info:\\n%s', lock_status.json()) force_unlock = self.session.delete(f'{dv_url}/api/datasets/:persistentId/locks', params=params, headers=headers, timeout=300) force_unlock.raise_for_status() LOGGER.warning('Lock removed for %s', study) LOGGER.warning('Lock status:\\n %s', force_unlock.json())","title":"force_notab_unlock"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.set_correct_date","text":"Sets \u201ccorrect\u201d publication date for Dataverse. Parameters: url ( str , default: None ) \u2013 Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL hdl ( str , default: None ) \u2013 Persistent indentifier for Dataverse study. Defaults to Transfer.dvpid (which can be None if the study has not yet been uploaded). d_type ( str , default: 'distributionDate' ) \u2013 Date type. One of \u2018distributionDate\u2019, \u2018productionDate\u2019, `dateOfDeposit\u2019. Default \u2018distributionDate\u2019. apikey ( str , default: None ) \u2013 Default dryad2dataverse.constants.APIKEY. Notes dryad2dataverse.serializer maps Dryad \u2018publicationDate\u2019 to Dataverse \u2018distributionDate\u2019 (see serializer.py ~line 675). Dataverse citation date default is \u201c:publicationDate\u201d. See Dataverse API reference: https://guides.dataverse.org/en/4.20/api/native-api.html#id54 . Source code in src/dryad2dataverse/transfer.py def set_correct_date(self, url=None, hdl=None, d_type='distributionDate', apikey=None): ''' Sets \"correct\" publication date for Dataverse. Parameters ---------- url : str Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL hdl : str Persistent indentifier for Dataverse study. Defaults to Transfer.dvpid (which can be None if the study has not yet been uploaded). d_type : str Date type. One of 'distributionDate', 'productionDate', `dateOfDeposit'. Default 'distributionDate'. apikey : str Default dryad2dataverse.constants.APIKEY. Notes ----- dryad2dataverse.serializer maps Dryad 'publicationDate' to Dataverse 'distributionDate' (see serializer.py ~line 675). Dataverse citation date default is \":publicationDate\". See Dataverse API reference: <https://guides.dataverse.org/en/4.20/api/native-api.html#id54>. ''' try: if not url: url = constants.DVURL if not hdl: hdl = self.dvpid headers = {'X-Dataverse-key' : apikey} if apikey: headers = {'X-Dataverse-key' : apikey} else: headers = {'X-Dataverse-key' : constants.APIKEY} headers.update(USER_AGENT) params = {'persistentId': hdl} set_date = self.session.put(f'{url}/api/datasets/:persistentId/citationdate', headers=headers, data=d_type, params=params, timeout=45) set_date.raise_for_status() except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.warning('Unable to set citation date for %s', hdl) LOGGER.warning(err) LOGGER.warning(set_date.text)","title":"set_correct_date"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.test_api_key","text":"Tests for an expired API key and raises dryad2dataverse.exceptions.Dryad2dataverseBadApiKeyError the API key is bad. Ignores other HTTP errors. Parameters: url ( str , default: None ) \u2013 Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL apikey ( str , default: None ) \u2013 Default dryad2dataverse.constants.APIKEY. Source code in src/dryad2dataverse/transfer.py def test_api_key(self, url=None, apikey=None): ''' Tests for an expired API key and raises dryad2dataverse.exceptions.Dryad2dataverseBadApiKeyError the API key is bad. Ignores other HTTP errors. Parameters ---------- url : str Base URL to Dataverse installation. Defaults to dryad2dataverse.constants.DVURL apikey : str Default dryad2dataverse.constants.APIKEY. ''' #API validity check appears to come before a PID validity check params = {'persistentId': 'doi:000/000/000'} # PID is irrelevant if not url: url = constants.DVURL headers = {'X-Dataverse-key': apikey if apikey else constants.APIKEY} headers.update(USER_AGENT) bad_test = self.session.get(f'{url}/api/datasets/:persistentId', headers=headers, params=params) #There's an extra space in the message which Harvard #will probably find out about, so . . . if bad_test.json().get('message').startswith('Bad api key'): try: raise exceptions.DataverseBadApiKeyError('Bad API key') except exceptions.DataverseBadApiKeyError as e: LOGGER.critical('API key has expired or is otherwise invalid') LOGGER.exception(e) #LOGGER.exception(traceback.format_exc()) #not really necessary raise try: #other errors bad_test.raise_for_status() except requests.exceptions.HTTPError: pass except Exception as e: LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise","title":"test_api_key"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.upload_file","text":"Uploads file to Dataverse study. Returns a tuple of the dryadFid (or None) and Dataverse JSON from the POST request. Failures produce JSON with different status messages rather than raising an exception. Parameters: filename ( str , default: None ) \u2013 Filename (not including path). mimetype ( str , default: None ) \u2013 Mimetype of file. size ( int , default: None ) \u2013 Size in bytes. studyId ( str , default: None ) \u2013 Persistent Dataverse study identifier. Defaults to Transfer.dvpid. dest ( str , default: None ) \u2013 Destination dataverse installation url. Defaults to constants.DVURL. hashtype \u2013 original Dryad hash type fprefix ( str , default: None ) \u2013 Path to file, not including a trailing slash. timeout ( int , default: 300 ) \u2013 Timeout in seconds for POST request. Default 300. dryadUrl ( str , default: None ) \u2013 Dryad download URL if you want to include a Dryad file id. force_unlock ( bool , default: False ) \u2013 Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse /locks endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). Forcible unlock requires a superuser API key. Source code in src/dryad2dataverse/transfer.py def upload_file(self, dryadUrl=None, filename=None, mimetype=None, size=None, descr=None, hashtype=None, #md5=None, studyId=None, dest=None, digest=None, studyId=None, dest=None, fprefix=None, force_unlock=False, timeout=300): ''' Uploads file to Dataverse study. Returns a tuple of the dryadFid (or None) and Dataverse JSON from the POST request. Failures produce JSON with different status messages rather than raising an exception. Parameters ---------- filename : str Filename (not including path). mimetype : str Mimetype of file. size : int Size in bytes. studyId : str Persistent Dataverse study identifier. Defaults to Transfer.dvpid. dest : str Destination dataverse installation url. Defaults to constants.DVURL. hashtype: str original Dryad hash type fprefix : str Path to file, not including a trailing slash. timeout : int Timeout in seconds for POST request. Default 300. dryadUrl : str Dryad download URL if you want to include a Dryad file id. force_unlock : bool Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse `/locks` endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). **Forcible unlock requires a superuser API key.** ''' #return locals() #TODONE remove above if not studyId: studyId = self.dvpid if not dest: dest = constants.DVURL if not fprefix: fprefix = constants.TMP if dryadUrl: fid = dryadUrl.strip('/download') fid = int(fid[fid.rfind('/')+1:]) else: fid = 0 #dummy fid for non-Dryad use params = {'persistentId' : studyId} upfile = fprefix + os.sep + filename[:] badExt = filename[filename.rfind('.'):].lower() #Descriptions are technically possible, although how to add #them is buried in Dryad's API documentation dv4meta = {'label' : filename[:], 'description' : descr} #if mimetype == 'application/zip' or filename.lower().endswith('.zip'): if mimetype == 'application/zip' or badExt in constants.NOTAB: mimetype = 'application/octet-stream' # stop unzipping automatically filename += '.NOPROCESS' # Also screw with their naming convention #debug log about file names to see what is up with XSLX #see doi:10.5061/dryad.z8w9ghxb6 LOGGER.debug('File renamed to %s for upload', filename) if size >= constants.MAX_UPLOAD: fail = (fid, {'status' : 'Failure: MAX_UPLOAD size exceeded'}) self.fileUpRecord.append(fail) LOGGER.warning('%s: File %s of ' 'size %s exceeds ' 'Dataverse MAX_UPLOAD size. Skipping.', self.doi, filename, size) return fail fields = {'file': (filename, open(upfile, 'rb'), mimetype)} fields.update({'jsonData': f'{dv4meta}'}) multi = MultipartEncoder(fields=fields) ctype = {'Content-type' : multi.content_type} tmphead = self.auth.copy() tmphead.update(ctype) tmphead.update(USER_AGENT) url = dest + '/api/datasets/:persistentId/add' try: upload = self.session.post(url, params=params, headers=tmphead, data=multi, timeout=timeout) #print(upload.text) upload.raise_for_status() self.fileUpRecord.append((fid, upload.json())) upmd5 = upload.json()['data']['files'][0]['dataFile']['checksum']['value'] #Dataverse hash type _type = upload.json()['data']['files'][0]['dataFile']['checksum']['type'] if _type.lower() != hashtype.lower(): comparator = self._check_md5(upfile, _type.lower()) else: comparator = digest #if hashtype.lower () != 'md5': # #get an md5 because dataverse uses md5s. Or most of them do anyway. # #One day this will be rewritten properly. # md5 = self._check_md5(filename, 'md5') #else: # md5 = digest #if md5 and (upmd5 != md5): if upmd5 != comparator: try: raise exceptions.HashError(f'{_type} mismatch:\\nlocal: {comparator}\\nuploaded: {upmd5}') except exceptions.HashError as e: LOGGER.exception(e) raise #Make damn sure that the study isn't locked because of #tab file processing ##SPSS files still process despite spoofing MIME and extension ##so there's also a forcible unlock check #fid = upload.json()['data']['files'][0]['dataFile']['id'] #fid not required for unlock #self.force_notab_unlock(studyId, dest, fid) if force_unlock: self.force_notab_unlock(studyId, dest) else: count = 0 wait = True while wait: wait = self.file_lock_check(studyId, dest, count=count) if wait: time.sleep(15) # Don't hit it too often count += 1 return (fid, upload.json()) except Exception as e: LOGGER.exception(e) try: reason = upload.json()['message'] LOGGER.warning(upload.json()) return (fid, {'status' : f'Failure: {reason}'}) except Exception as e: LOGGER.warning('Further exceptions!') LOGGER.exception(e) LOGGER.warning(upload.text) return (fid, {'status' : f'Failure: Reason {upload.reason}'})","title":"upload_file"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.upload_files","text":"Uploads multiple files to study with persistentId pid. Returns a list of the original tuples plus JSON responses. Parameters: files ( list , default: None ) \u2013 List contains tuples with (dryadDownloadURL, filename, mimetype, size). pid ( str , default: None ) \u2013 Defaults to self.dvpid, which is generated by calling dryad2dataverse.transfer.Transfer.upload_study(). fprefix ( str , default: None ) \u2013 File location prefix. Defaults to dryad2dataverse.constants.TMP force_unlock ( bool , default: False ) \u2013 Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse /locks endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). Forcible unlock requires a superuser API key. Source code in src/dryad2dataverse/transfer.py def upload_files(self, files=None, pid=None, fprefix=None, force_unlock=False): ''' Uploads multiple files to study with persistentId pid. Returns a list of the original tuples plus JSON responses. Parameters ---------- files : list List contains tuples with (dryadDownloadURL, filename, mimetype, size). pid : str Defaults to self.dvpid, which is generated by calling dryad2dataverse.transfer.Transfer.upload_study(). fprefix : str File location prefix. Defaults to dryad2dataverse.constants.TMP force_unlock : bool Attempt forcible unlock instead of waiting for tabular file processing. Defaults to False. The Dataverse `/locks` endpoint blocks POST and DELETE requests from non-superusers (undocumented as of 31 March 2021). **Forcible unlock requires a superuser API key.** ''' if not files: files = self.files if not fprefix: fprefix = constants.TMP out = [] for f in files: #out.append(self.upload_file(f[0], f[1], f[2], f[3], # f[4], f[5], pid, fprefix=fprefix)) #out.append(self.upload_file(*[x for x in f], #last item in files is not necessary out.append(self.upload_file(*list(f)[:-1], studyId=pid, fprefix=fprefix, force_unlock=force_unlock)) return out","title":"upload_files"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.upload_json","text":"Uploads Dryad json as a separate file for archival purposes. Parameters: studyId ( str , default: None ) \u2013 Dataverse persistent identifier. Default dryad2dataverse.transfer.Transfer.dvpid, which is only generated on dryad2dataverse.transfer.Transfer.upload_study() dest ( str , default: None ) \u2013 Base URL for transfer. Default dryad2datavese.constants.DVURL Source code in src/dryad2dataverse/transfer.py def upload_json(self, studyId=None, dest=None): ''' Uploads Dryad json as a separate file for archival purposes. Parameters ---------- studyId : str Dataverse persistent identifier. Default dryad2dataverse.transfer.Transfer.dvpid, which is only generated on dryad2dataverse.transfer.Transfer.upload_study() dest : str Base URL for transfer. Default dryad2datavese.constants.DVURL ''' if not studyId: studyId = self.dvpid if not dest: dest = constants.DVURL if not self.jsonFlag: url = dest + '/api/datasets/:persistentId/add' pack = io.StringIO(json.dumps(self.dryad.dryadJson)) desc = {'description':'Original JSON from Dryad', 'categories':['Documentation', 'Code']} fname = self.doi[self.doi.rfind('/')+1:].replace('.', '_') payload = {'file': (f'{fname}.json', pack, 'text/plain;charset=UTF-8'), 'jsonData':f'{desc}'} params = {'persistentId':studyId} try: meta = self.session.post(f'{url}', params=params, headers=self.auth, files=payload) #0 because no dryad fid will be zero meta.raise_for_status() self.fileUpRecord.append((0, meta.json())) self.jsonFlag = (0, meta.json()) LOGGER.debug('Successfully uploaded Dryad JSON to %s', studyId) #JSON uploads randomly fail with a Dataverse server.log error of #\"A system exception occurred during an invocation on EJB . . .\" #Not reproducible, so errors will only be written to the log. #Jesus. except (requests.exceptions.HTTPError, requests.exceptions.ConnectionError) as err: LOGGER.error('Unable to upload Dryad JSON to %s', studyId) LOGGER.error('ERROR message: %s', meta.text) LOGGER.exception(err) #And further checking as to what is happening self.fileUpRecord.append((0, {'status':'Failure: Unable to upload Dryad JSON'})) if not isinstance(self.dryad.dryadJson, dict): LOGGER.error('Dryad JSON is not a dictionary') except Exception as err: LOGGER.error('Unable to upload Dryad JSON') LOGGER.exception(err)","title":"upload_json"},{"location":"api_reference/#dryad2dataverse.transfer.Transfer.upload_study","text":"Uploads Dryad study metadata to target Dataverse or updates existing. Supplying a targetDv kwarg creates a new study and supplying a dvpid kwarg updates a currently existing Dataverse study. Parameters: url ( str , default: None ) \u2013 URL of Dataverse instance. Defaults to constants.DVURL. apikey ( str , default: None ) \u2013 API key of user. Defaults to contants.APIKEY. timeout ( int , default: 45 ) \u2013 timeout on POST request. kwargs ( dict , default: {} ) \u2013 targetDv ( str ) \u2013 Short name of target dataverse. Required if new dataset. Specify as targetDV=value. dvpid ( str ) \u2013 Dataverse persistent ID (for updating metadata). This is not required for new uploads, specify as dvpid=value Notes One of targetDv or dvpid is required. Source code in src/dryad2dataverse/transfer.py def upload_study(self, url=None, apikey=None, timeout=45, **kwargs): ''' Uploads Dryad study metadata to target Dataverse or updates existing. Supplying a `targetDv` kwarg creates a new study and supplying a `dvpid` kwarg updates a currently existing Dataverse study. Parameters ---------- url : str URL of Dataverse instance. Defaults to constants.DVURL. apikey : str API key of user. Defaults to contants.APIKEY. timeout : int timeout on POST request. kwargs : dict Other parameters ---------------- targetDv : str Short name of target dataverse. Required if new dataset. Specify as targetDV=value. dvpid : str Dataverse persistent ID (for updating metadata). This is not required for new uploads, specify as dvpid=value Notes ----- One of targetDv or dvpid is required. ''' if not url: url = constants.DVURL if not apikey: apikey = constants.APIKEY headers = {'X-Dataverse-key' : apikey} headers.update(USER_AGENT) targetDv = kwargs.get('targetDv') dvpid = kwargs.get('dvpid') #dryFid = kwargs.get('dryFid') #Why did I put this here? if not targetDv and not dvpid: try: raise exceptions.NoTargetError('You must supply one of targetDv \\ (target dataverse) \\ or dvpid (Dataverse persistent ID)') except exceptions.NoTargetError as e: LOGGER.error('No target dataverse or dvpid supplied') LOGGER.exception(e) raise if targetDv and dvpid: try: raise ValueError('Supply only one of targetDv or dvpid') except ValueError as e: LOGGER.exception(e) raise if not dvpid: endpoint = f'{url}/api/dataverses/{targetDv}/datasets' upload = self.session.post(endpoint, headers=headers, json=self.dryad.dvJson, timeout=timeout) LOGGER.debug(upload.text) else: endpoint = f'{url}/api/datasets/:persistentId/versions/:draft' params = {'persistentId':dvpid} #Yes, dataverse uses *different* json for edits upload = self.session.put(endpoint, params=params, headers=headers, json=self.dryad.dvJson['datasetVersion'], timeout=timeout) #self._dvrecord = upload.json() LOGGER.debug(upload.text) try: updata = upload.json() self.dvStudy = updata if updata.get('status') != 'OK': try: raise exceptions.DataverseUploadError(('Status return is not OK.' f'{upload.status_code}: ' f'{upload.reason}. ' f'{upload.request.url} ' f'{upload.text}')) except exceptions.DataverseUploadError as e: LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise exceptions.DataverseUploadError(('Status return is not OK.' f'{upload.status_code}: ' f'{upload.reason}. ' f'{upload.request.url} ' f'{upload.text}')) upload.raise_for_status() except Exception as e: # Only accessible via non-requests exception LOGGER.exception(e) LOGGER.exception(traceback.format_exc()) raise if targetDv: self.dryad.dvpid = updata['data'].get('persistentId') if dvpid: self.dryad.dvpid = updata['data'].get('datasetPersistentId') return self.dvpid","title":"upload_study"},{"location":"api_reference/#dryad2dataverse.monitor","text":"Dryad/Dataverse status tracker. Monitor creates a singleton object which writes to a SQLite database. Methods will (generally) take either a dryad2dataverse.serializer.Serializer instance or dryad2dataverse.transfer.Transfer instance The monitor\u2019s primary function is to allow for state checking for Dryad studies so that files and studies aren\u2019t downloaded unneccessarily.","title":"monitor"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor","text":"The Monitor object is a tracker and database updater, so that Dryad files can be monitored and updated over time. Monitor is a singleton, but is not thread-safe. Source code in src/dryad2dataverse/monitor.py class Monitor(): ''' The Monitor object is a tracker and database updater, so that Dryad files can be monitored and updated over time. Monitor is a singleton, but is not thread-safe. ''' __instance = None def __new__(cls, dbase=None, *args, **kwargs): ''' Creates a new singleton instance of Monitor. Also creates a database if existing database is not present. Parameters ---------- dbase : str Path to sqlite3 database. That is: /path/to/file.sqlite3 *args : list **kwargs : dict ''' if cls.__instance is None: cls.__instance = super(Monitor, cls).__new__(cls) cls.__instance.__initialized = False cls.dbase = dbase if not cls.dbase: cls.dbase = constants.DBASE cls.conn = sqlite3.Connection(cls.dbase) cls.cursor = cls.conn.cursor() create = ['CREATE TABLE IF NOT EXISTS dryadStudy \\ (uid INTEGER PRIMARY KEY AUTOINCREMENT, \\ doi TEXT, lastmoddate TEXT, dryadjson TEXT, \\ dvjson TEXT);', 'CREATE TABLE IF NOT EXISTS dryadFiles \\ (dryaduid INTEGER REFERENCES dryadStudy (uid), \\ dryfilesjson TEXT);', 'CREATE TABLE IF NOT EXISTS dvStudy \\ (dryaduid INTEGER references dryadStudy (uid), \\ dvpid TEXT);', 'CREATE TABLE IF NOT EXISTS dvFiles \\ (dryaduid INTEGER references dryadStudy (uid), \\ dryfid INT, \\ drymd5 TEXT, dvfid TEXT, dvmd5 TEXT, \\ dvfilejson TEXT);', 'CREATE TABLE IF NOT EXISTS lastcheck \\ (checkdate TEXT);', 'CREATE TABLE IF NOT EXISTS failed_uploads \\ (dryaduid INTEGER references dryadstudy (uid), \\ dryfid INT, status TEXT);' ] for line in create: cls.cursor.execute(line) cls.conn.commit() LOGGER.info('Using database %s', cls.dbase) return cls.__instance def __init__(self, dbase=None, *args, **kwargs): # remove args and kwargs when you find out how init interacts with new. ''' Initialize the Monitor instance if not instantiated already (ie, Monitor is a singleton). Parameters ---------- dbase : str, default=dryad2datverse.constants.DBASE Complete path to desired location of tracking database (eg: /tmp/test.db). *args : list **kwargs : dict ''' if self.__initialized: return self.__initialized = True if not dbase: self.dbase = constants.DBASE else: self.dbase = dbase def __del__(self): ''' Commits all database transactions on object deletion and closes database. ''' self.conn.commit() self.conn.close() @property def lastmod(self): ''' Returns last modification date from monitor.dbase. ''' self.cursor.execute('SELECT checkdate FROM lastcheck ORDER BY rowid DESC;') last_mod = self.cursor.fetchall() if last_mod: return last_mod[0][0] return None def status(self, serial)->dict: ''' Returns a dictionary with keys 'status' and 'dvpid' and 'notes'. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- `{status :'updated', 'dvpid':'doi://some/ident'}`. Notes ------ `status` is one of 'new', 'identical', 'lastmodsame', 'updated' 'new' is a completely new file. 'identical' The metadata from Dryad is *identical* to the last time the check was run. 'lastmodsame' Dryad lastModificationDate == last modification date in database AND output JSON is different. This can indicate a Dryad API output change, reindexing or something else. But the lastModificationDate is supposed to be an indicator of meaningful change, so this option exists so you can decide what to do given this option 'updated' Indicates changes to lastModificationDate Note that Dryad constantly changes their API output, so the changes may not actually be meaningful. `dvpid` is a Dataverse persistent identifier. `None` in the case of status='new' `notes`: value of Dryad versionChanges field. One of `files_changed` or `metatdata_changed`. Non-null value present only when status is not `new` or `identical`. Note that Dryad has no way to indicate *both* a file and metadata change, so this value reflects only the *last* change in the Dryad state. ''' # Last mod date is indicator of change. # From email w/Ryan Scherle 10 Nov 2020 #The versionNumber updates for either a metadata change or a #file change. Although we save all of these changes internally, our web #interface only displays the versions that have file changes, along #with the most recent metadata. So a dataset that has only two versions #of files listed on the web may actually have several more versions in #the API. # #If your only need is to track when there are changes to a #dataset, you may want to use the `lastModificationDate`, which we have #recently added to our metadata. # #Note that the Dryad API output ISN'T STABLE; they add fields etc. #This means that a comparison of JSON may yield differences even though #metadata is technically \"the same\". Just comparing two dicts doesn't cut #it. ############################# ## Note: by inspection, Dryad outputs JSON that is different ## EVEN IF lastModificationDate is unchanged. (14 January 2022) ## So now what? ############################# doi = serial.dryadJson['identifier'] self.cursor.execute('SELECT * FROM dryadStudy WHERE doi = ?', (doi,)) result = self.cursor.fetchall() if not result: return {'status': 'new', 'dvpid': None, 'notes': ''} # dvjson = json.loads(result[-1][4]) # Check the fresh vs. updated jsons for the keys try: dryaduid = result[-1][0] self.cursor.execute('SELECT dvpid from dvStudy WHERE \\ dryaduid = ?', (dryaduid,)) dvpid = self.cursor.fetchall()[-1][0] serial.dvpid = dvpid except TypeError: try: raise exceptions.DatabaseError except exceptions.DatabaseError as e: LOGGER.error('Dryad DOI : %s. Error finding Dataverse PID', doi) LOGGER.exception(e) raise newfile = copy.deepcopy(serial.dryadJson) testfile = copy.deepcopy(json.loads(result[-1][3])) if newfile == testfile: return {'status': 'identical', 'dvpid': dvpid, 'notes': ''} if newfile['lastModificationDate'] != testfile['lastModificationDate']: return {'status': 'updated', 'dvpid': dvpid, 'notes': newfile['versionChanges']} return {'status': 'lastmodsame', 'dvpid': dvpid, 'notes': newfile.get('versionChanges')} def diff_metadata(self, serial): ''' Analyzes differences in metadata between current serializer instance and last updated serializer instance. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- Returns a list of field changes consisting of: [{key: (old_value, new_value}] or None if no changes. Notes ----- For example: ``` [{'title': ('Cascading effects of algal warming in a freshwater community', 'Cascading effects of algal warming in a freshwater community theatre')} ] ``` ''' if self.status(serial)['status'] == 'updated': self.cursor.execute('SELECT dryadjson from dryadStudy \\ WHERE doi = ?', (serial.dryadJson['identifier'],)) oldJson = json.loads(self.cursor.fetchall()[-1][0]) out = [] for k in serial.dryadJson: if serial.dryadJson[k] != oldJson.get(k): out.append({k: (oldJson.get(k), serial.dryadJson[k])}) return out return None @staticmethod def __added_hashes(oldFiles, newFiles): ''' Checks that two objects in dryad2dataverse.serializer.files format stripped of digestType and digest values are identical. Returns array of files with changed hash. Assumes name, mimeType, size, descr all unchanged, which is not necessarily a valid assumption Parameters ---------- oldFiles : Union[list, tuple] (name, mimeType, size, descr, digestType, digest) newFiles : Union[list, tuple] (name, mimeType, size, descr, digestType, digest) ''' hash_change = [] old = [x[1:-2] for x in oldFiles] #URLs are not permanent old_no_url = [x[1:] for x in oldFiles] for fi in newFiles: if fi[1:-2] in old and fi[1:] not in old_no_url: hash_change.append(fi) return hash_change def diff_files(self, serial): ''' Returns a dict with additions and deletions from previous Dryad to dataverse upload. Because checksums are not necessarily included in Dryad file metadata, this method uses dryad file IDs, size, or whatever is available. If dryad2dataverse.monitor.Monitor.status() indicates a change it will produce dictionary output with a list of additions, deletions or hash changes (ie, identical except for hash changes), as below: `{'add':[dyadfiletuples], 'delete:[dryadfiletuples], 'hash_change': [dryadfiletuples]}` Parameters ---------- serial : dryad2dataverse.serializer.Serializer ''' diffReport = {} if self.status(serial)['status'] == 'new': #do we want to show what needs to be added? return {'add': serial.files} #return {} self.cursor.execute('SELECT uid from dryadStudy WHERE doi = ?', (serial.doi,)) mostRecent = self.cursor.fetchall()[-1][0] self.cursor.execute('SELECT dryfilesjson from dryadFiles WHERE \\ dryaduid = ?', (mostRecent,)) oldFileList = self.cursor.fetchall()[-1][0] if not oldFileList: oldFileList = [] else: out = [] #With Dryad API change, files are paginated #now stored as list for old in json.loads(oldFileList): #for old in oldFileList: oldFiles = old['_embedded'].get('stash:files') # comparing file tuples from dryad2dataverse.serializer. # Maybe JSON is better? # because of code duplication below. for f in oldFiles: #Download links are not persistent. Be warned try: downLink = f['_links']['stash:file-download']['href'] except KeyError: downLink = f['_links']['stash:download']['href'] downLink = f'{constants.DRYURL}{downLink}' name = f['path'] mimeType = f['mimeType'] size = f['size'] descr = f.get('description', '') digestType = f.get('digestType', '') digest = f.get('digest', '') out.append((downLink, name, mimeType, size, descr, digestType, digest)) oldFiles = out newFiles = serial.files[:] # Tests go here #Check for identity first #if returned here there are definitely no changes if (set(oldFiles).issuperset(set(newFiles)) and set(newFiles).issuperset(oldFiles)): return diffReport #filenames for checking hash changes. #Can't use URL or hashes for comparisons because they can change #without warning, despite the fact that the API says that #file IDs are unique. They aren't. Verified by Ryan Scherle at #Dryad December 2021 old_map = {x:{'orig':y, 'no_hash':y[1:4]} for x,y in enumerate(oldFiles)} new_map = {x:{'orig':y, 'no_hash':y[1:4]} for x,y in enumerate(newFiles)} old_no_hash = [old_map[x]['no_hash'] for x in old_map] new_no_hash = [new_map[x]['no_hash'] for x in new_map] #check for added hash only hash_change = Monitor.__added_hashes(oldFiles, newFiles) must = set(old_no_hash).issuperset(set(new_no_hash)) if not must: needsadd = set(new_no_hash) - (set(old_no_hash) & set(new_no_hash)) #Use the map created above to return the full file info diffReport.update({'add': [new_map[new_no_hash.index(x)]['orig'] for x in needsadd]}) must = set(new_no_hash).issuperset(old_no_hash) if not must: needsdel = set(old_no_hash) - (set(new_no_hash) & set(old_no_hash)) diffReport.update({'delete' : [old_map[old_no_hash.index(x)]['orig'] for x in needsdel]}) if hash_change: diffReport.update({'hash_change': hash_change}) return diffReport def get_dv_fid(self, url): ''' Returns str \u2014 the Dataverse file ID from parsing a Dryad file download link. Normally used for determining dataverse file ids for *deletion* in case of dryad file changes. Parameters ---------- url : str *Dryad* file URL in form of 'https://datadryad.org/api/v2/files/385819/download'. ''' fid = url[url.rfind('/', 0, -10)+1:].strip('/download') try: fid = int(fid) except ValueError as e: LOGGER.error('File ID %s is not an integer', fid) LOGGER.exception(e) raise #File IDs are *CHANGEABLE* according to Dryad, Dec 2021 #SQLite default returns are by ROWID ASC, so the last record #returned should still be the correct, ie. most recent, one. #However, just in case, this is now done explicitly. self.cursor.execute('SELECT dvfid, ROWID FROM dvFiles WHERE \\ dryfid = ? ORDER BY ROWID ASC;', (fid,)) dvfid = self.cursor.fetchall() if dvfid: return dvfid[-1][0] return None def get_dv_fids(self, filelist): ''' Returns Dataverse file IDs from a list of Dryad file tuples. Generally, you would use the output from dryad2dataverse.monitor.Monitor.diff_files['delete'] to discover Dataverse file ids for deletion. Parameters ---------- filelist : list List of Dryad file tuples: eg: ``` [('https://datadryad.org/api/v2/files/385819/download', 'GCB_ACG_Mortality_2020.zip', 'application/x-zip-compressed', 23787587), ('https://datadryad.org/api/v2/files/385820/download', 'Readme_ACG_Mortality.txt', 'text/plain', 1350)] ``` ''' fids = [] for f in filelist: fids.append(self.get_dv_fid(f[0])) return fids # return [self.get_dv_fid(f[0]) for f in filelist] def get_json_dvfids(self, serial)->list: ''' Return a list of Dataverse file ids for Dryad JSONs which were uploaded to Dataverse. Normally used to discover the file IDs to remove Dryad JSONs which have changed. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- list ''' self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE doi=?', (serial.doi,)) try: uid = self.cursor.fetchone()[0] self.cursor.execute('SELECT dvfid FROM dvFiles WHERE \\ dryaduid = ? AND dryfid=?', (uid, 0)) jsonfid = [f[0] for f in self.cursor.fetchall()] return jsonfid except TypeError: return [] def update(self, transfer): ''' Updates the Monitor database with information from a dryad2dataverse.transfer.Transfer instance. If a Dryad primary metadata record has changes, it will be deleted from the database. This method should be called after all transfers are completed, including Dryad JSON updates, as the last action for transfer. Parameters ---------- transfer : dryad2dataverse.transfer.Transfer ''' # get the pre-update dryad uid in case we need it. self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE doi = ?', (transfer.dryad.dryadJson['identifier'],)) olduid = self.cursor.fetchone()[0] if olduid: olduid = int(olduid) if self.status(transfer.dryad)['status'] != 'unchanged': doi = transfer.doi lastmod = transfer.dryad.dryadJson.get('lastModificationDate') dryadJson = json.dumps(transfer.dryad.dryadJson) dvJson = json.dumps(transfer.dvStudy) # Update study metadata self.cursor.execute('INSERT INTO dryadStudy \\ (doi, lastmoddate, dryadjson, dvjson) \\ VALUES (?, ?, ?, ?)', (doi, lastmod, dryadJson, dvJson)) self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE \\ doi = ?', (doi,)) dryaduid = self.cursor.fetchone()[0] #if type(dryaduid) != int: if not isinstance(dryaduid, int): try: raise TypeError('Dryad UID is not an integer') except TypeError as e: LOGGER.error(e) raise # Update dryad file json self.cursor.execute('INSERT INTO dryadFiles VALUES (?, ?)', (dryaduid, json.dumps(transfer.dryad.fileJson))) # Update dataverse study map self.cursor.execute('SELECT dvpid FROM dvStudy WHERE \\ dvpid = ?', (transfer.dryad.dvpid,)) if not self.cursor.fetchone(): self.cursor.execute('INSERT INTO dvStudy VALUES (?, ?)', (dryaduid, transfer.dryad.dvpid)) else: self.cursor.execute('UPDATE dvStudy SET dryaduid=?, \\ dvpid=? WHERE dvpid =?', (dryaduid, transfer.dryad.dvpid, transfer.dryad.dvpid)) # Update the files table # Because we want to have a *complete* file list for each # dryaduid, we have to copy any existing old files, # then add and delete. if olduid: self.cursor.execute('SELECT * FROM dvFiles WHERE \\ dryaduid=?', (olduid,)) inserter = self.cursor.fetchall() for rec in inserter: # TODONE FIX THIS #I think it's fixed 11 Feb 21 self.cursor.execute('INSERT INTO dvFiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, rec[1], rec[2], rec[3], rec[4], rec[5])) # insert newly uploaded files for rec in transfer.fileUpRecord: try: dvfid = rec[1]['data']['files'][0]['dataFile']['id'] # Screw you for burying the file ID this deep recMd5 = rec[1]['data']['files'][0]['dataFile']['checksum']['value'] except (KeyError, IndexError) as err: #write to failed uploads table instead status = rec[1].get('status') if not status: LOGGER.error('JSON read error for Dryad file ID %s', rec[0]) LOGGER.error('File %s for DOI %s may not be uploaded', rec[0], transfer.doi) LOGGER.exception(err) msg = {'status': 'Failure: Other non-specific ' 'failure. Check logs'} self.cursor.execute('INSERT INTO failed_uploads VALUES \\ (?, ?, ?);', (dryaduid, rec[0], json.dumps(msg))) continue self.cursor.execute('INSERT INTO failed_uploads VALUES \\ (?, ?, ?);', (dryaduid, rec[0], json.dumps(rec[1]))) LOGGER.warning(type(err)) LOGGER.warning('%s. DOI %s, File ID %s', rec[1].get('status'), transfer.doi, rec[0]) continue # md5s verified during upload step, so they should # match already self.cursor.execute('INSERT INTO dvFiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, rec[0], recMd5, dvfid, recMd5, json.dumps(rec[1]))) # Now the deleted files for rec in transfer.fileDelRecord: # fileDelRecord consists only of [fid,fid2, ...] # Dryad record ID is int not str self.cursor.execute('DELETE FROM dvFiles WHERE dvfid=? \\ AND dryaduid=?', (int(rec), dryaduid)) LOGGER.debug('deleted dryfid = %s, dryaduid = %s', rec, dryaduid) # And lastly, any JSON metadata updates: # NOW WHAT? # JSON has dryfid==0 self.cursor.execute('SELECT * FROM dvfiles WHERE \\ dryfid=? and dryaduid=?', (0, dryaduid)) try: exists = self.cursor.fetchone()[0] # Old metadata must be deleted on a change. if exists: shouldDel = self.status(transfer.dryad)['status'] if shouldDel == 'updated': self.cursor.execute('DELETE FROM dvfiles WHERE \\ dryfid=? and dryaduid=?', (0, dryaduid)) except TypeError: pass if transfer.jsonFlag: # update dryad JSON djson5 = transfer.jsonFlag[1]['data']['files'][0]['dataFile']['checksum']['value'] dfid = transfer.jsonFlag[1]['data']['files'][0]['dataFile']['id'] self.cursor.execute('INSERT INTO dvfiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, 0, djson5, dfid, djson5, json.dumps(transfer.jsonFlag[1]))) self.conn.commit() def set_timestamp(self, curdate=None): ''' Adds current time to the database table. Can be queried and be used for subsequent checking for updates. To query last modification time, use the dataverse2dryad.monitor.Monitor.lastmod attribute. Parameters ---------- curdate : str UTC datetime string in the format suitable for the Dryad API. eg. 2021-01-21T21:42:40Z or .strftime('%Y-%m-%dT%H:%M:%SZ'). ''' #Dryad API uses Zulu time if not curdate: curdate = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ') self.cursor.execute('INSERT INTO lastcheck VALUES (?)', (curdate,)) self.conn.commit()","title":"Monitor"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.lastmod","text":"Returns last modification date from monitor.dbase.","title":"lastmod"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.__del__","text":"Commits all database transactions on object deletion and closes database. Source code in src/dryad2dataverse/monitor.py def __del__(self): ''' Commits all database transactions on object deletion and closes database. ''' self.conn.commit() self.conn.close()","title":"__del__"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.__init__","text":"Initialize the Monitor instance if not instantiated already (ie, Monitor is a singleton). Parameters: dbase ( str , default: dryad2datverse.constants.DBASE ) \u2013 Complete path to desired location of tracking database (eg: /tmp/test.db). *args ( list , default: () ) \u2013 **kwargs ( dict , default: {} ) \u2013 Source code in src/dryad2dataverse/monitor.py def __init__(self, dbase=None, *args, **kwargs): # remove args and kwargs when you find out how init interacts with new. ''' Initialize the Monitor instance if not instantiated already (ie, Monitor is a singleton). Parameters ---------- dbase : str, default=dryad2datverse.constants.DBASE Complete path to desired location of tracking database (eg: /tmp/test.db). *args : list **kwargs : dict ''' if self.__initialized: return self.__initialized = True if not dbase: self.dbase = constants.DBASE else: self.dbase = dbase","title":"__init__"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.__new__","text":"Creates a new singleton instance of Monitor. Also creates a database if existing database is not present. Parameters: dbase ( str , default: None ) \u2013 Path to sqlite3 database. That is: /path/to/file.sqlite3 *args ( list , default: () ) \u2013 **kwargs ( dict , default: {} ) \u2013 Source code in src/dryad2dataverse/monitor.py def __new__(cls, dbase=None, *args, **kwargs): ''' Creates a new singleton instance of Monitor. Also creates a database if existing database is not present. Parameters ---------- dbase : str Path to sqlite3 database. That is: /path/to/file.sqlite3 *args : list **kwargs : dict ''' if cls.__instance is None: cls.__instance = super(Monitor, cls).__new__(cls) cls.__instance.__initialized = False cls.dbase = dbase if not cls.dbase: cls.dbase = constants.DBASE cls.conn = sqlite3.Connection(cls.dbase) cls.cursor = cls.conn.cursor() create = ['CREATE TABLE IF NOT EXISTS dryadStudy \\ (uid INTEGER PRIMARY KEY AUTOINCREMENT, \\ doi TEXT, lastmoddate TEXT, dryadjson TEXT, \\ dvjson TEXT);', 'CREATE TABLE IF NOT EXISTS dryadFiles \\ (dryaduid INTEGER REFERENCES dryadStudy (uid), \\ dryfilesjson TEXT);', 'CREATE TABLE IF NOT EXISTS dvStudy \\ (dryaduid INTEGER references dryadStudy (uid), \\ dvpid TEXT);', 'CREATE TABLE IF NOT EXISTS dvFiles \\ (dryaduid INTEGER references dryadStudy (uid), \\ dryfid INT, \\ drymd5 TEXT, dvfid TEXT, dvmd5 TEXT, \\ dvfilejson TEXT);', 'CREATE TABLE IF NOT EXISTS lastcheck \\ (checkdate TEXT);', 'CREATE TABLE IF NOT EXISTS failed_uploads \\ (dryaduid INTEGER references dryadstudy (uid), \\ dryfid INT, status TEXT);' ] for line in create: cls.cursor.execute(line) cls.conn.commit() LOGGER.info('Using database %s', cls.dbase) return cls.__instance","title":"__new__"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.diff_files","text":"Returns a dict with additions and deletions from previous Dryad to dataverse upload. Because checksums are not necessarily included in Dryad file metadata, this method uses dryad file IDs, size, or whatever is available. If dryad2dataverse.monitor.Monitor.status() indicates a change it will produce dictionary output with a list of additions, deletions or hash changes (ie, identical except for hash changes), as below: {'add':[dyadfiletuples], 'delete:[dryadfiletuples], 'hash_change': [dryadfiletuples]} Parameters: serial ( Serializer ) \u2013 Source code in src/dryad2dataverse/monitor.py def diff_files(self, serial): ''' Returns a dict with additions and deletions from previous Dryad to dataverse upload. Because checksums are not necessarily included in Dryad file metadata, this method uses dryad file IDs, size, or whatever is available. If dryad2dataverse.monitor.Monitor.status() indicates a change it will produce dictionary output with a list of additions, deletions or hash changes (ie, identical except for hash changes), as below: `{'add':[dyadfiletuples], 'delete:[dryadfiletuples], 'hash_change': [dryadfiletuples]}` Parameters ---------- serial : dryad2dataverse.serializer.Serializer ''' diffReport = {} if self.status(serial)['status'] == 'new': #do we want to show what needs to be added? return {'add': serial.files} #return {} self.cursor.execute('SELECT uid from dryadStudy WHERE doi = ?', (serial.doi,)) mostRecent = self.cursor.fetchall()[-1][0] self.cursor.execute('SELECT dryfilesjson from dryadFiles WHERE \\ dryaduid = ?', (mostRecent,)) oldFileList = self.cursor.fetchall()[-1][0] if not oldFileList: oldFileList = [] else: out = [] #With Dryad API change, files are paginated #now stored as list for old in json.loads(oldFileList): #for old in oldFileList: oldFiles = old['_embedded'].get('stash:files') # comparing file tuples from dryad2dataverse.serializer. # Maybe JSON is better? # because of code duplication below. for f in oldFiles: #Download links are not persistent. Be warned try: downLink = f['_links']['stash:file-download']['href'] except KeyError: downLink = f['_links']['stash:download']['href'] downLink = f'{constants.DRYURL}{downLink}' name = f['path'] mimeType = f['mimeType'] size = f['size'] descr = f.get('description', '') digestType = f.get('digestType', '') digest = f.get('digest', '') out.append((downLink, name, mimeType, size, descr, digestType, digest)) oldFiles = out newFiles = serial.files[:] # Tests go here #Check for identity first #if returned here there are definitely no changes if (set(oldFiles).issuperset(set(newFiles)) and set(newFiles).issuperset(oldFiles)): return diffReport #filenames for checking hash changes. #Can't use URL or hashes for comparisons because they can change #without warning, despite the fact that the API says that #file IDs are unique. They aren't. Verified by Ryan Scherle at #Dryad December 2021 old_map = {x:{'orig':y, 'no_hash':y[1:4]} for x,y in enumerate(oldFiles)} new_map = {x:{'orig':y, 'no_hash':y[1:4]} for x,y in enumerate(newFiles)} old_no_hash = [old_map[x]['no_hash'] for x in old_map] new_no_hash = [new_map[x]['no_hash'] for x in new_map] #check for added hash only hash_change = Monitor.__added_hashes(oldFiles, newFiles) must = set(old_no_hash).issuperset(set(new_no_hash)) if not must: needsadd = set(new_no_hash) - (set(old_no_hash) & set(new_no_hash)) #Use the map created above to return the full file info diffReport.update({'add': [new_map[new_no_hash.index(x)]['orig'] for x in needsadd]}) must = set(new_no_hash).issuperset(old_no_hash) if not must: needsdel = set(old_no_hash) - (set(new_no_hash) & set(old_no_hash)) diffReport.update({'delete' : [old_map[old_no_hash.index(x)]['orig'] for x in needsdel]}) if hash_change: diffReport.update({'hash_change': hash_change}) return diffReport","title":"diff_files"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.diff_metadata","text":"Analyzes differences in metadata between current serializer instance and last updated serializer instance. Parameters: serial ( Serializer ) \u2013 Returns: Returns a list of field changes consisting of: \u2013 [{key: (old_value, new_value}] or None if no changes. \u2013 Notes For example: [{'title': ('Cascading effects of algal warming in a freshwater community', 'Cascading effects of algal warming in a freshwater community theatre')} ] Source code in src/dryad2dataverse/monitor.py def diff_metadata(self, serial): ''' Analyzes differences in metadata between current serializer instance and last updated serializer instance. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- Returns a list of field changes consisting of: [{key: (old_value, new_value}] or None if no changes. Notes ----- For example: ``` [{'title': ('Cascading effects of algal warming in a freshwater community', 'Cascading effects of algal warming in a freshwater community theatre')} ] ``` ''' if self.status(serial)['status'] == 'updated': self.cursor.execute('SELECT dryadjson from dryadStudy \\ WHERE doi = ?', (serial.dryadJson['identifier'],)) oldJson = json.loads(self.cursor.fetchall()[-1][0]) out = [] for k in serial.dryadJson: if serial.dryadJson[k] != oldJson.get(k): out.append({k: (oldJson.get(k), serial.dryadJson[k])}) return out return None","title":"diff_metadata"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.get_dv_fid","text":"Returns str \u2014 the Dataverse file ID from parsing a Dryad file download link. Normally used for determining dataverse file ids for deletion in case of dryad file changes. Parameters: url ( str ) \u2013 Dryad file URL in form of \u2018https://datadryad.org/api/v2/files/385819/download\u2019. Source code in src/dryad2dataverse/monitor.py def get_dv_fid(self, url): ''' Returns str \u2014 the Dataverse file ID from parsing a Dryad file download link. Normally used for determining dataverse file ids for *deletion* in case of dryad file changes. Parameters ---------- url : str *Dryad* file URL in form of 'https://datadryad.org/api/v2/files/385819/download'. ''' fid = url[url.rfind('/', 0, -10)+1:].strip('/download') try: fid = int(fid) except ValueError as e: LOGGER.error('File ID %s is not an integer', fid) LOGGER.exception(e) raise #File IDs are *CHANGEABLE* according to Dryad, Dec 2021 #SQLite default returns are by ROWID ASC, so the last record #returned should still be the correct, ie. most recent, one. #However, just in case, this is now done explicitly. self.cursor.execute('SELECT dvfid, ROWID FROM dvFiles WHERE \\ dryfid = ? ORDER BY ROWID ASC;', (fid,)) dvfid = self.cursor.fetchall() if dvfid: return dvfid[-1][0] return None","title":"get_dv_fid"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.get_dv_fids","text":"Returns Dataverse file IDs from a list of Dryad file tuples. Generally, you would use the output from dryad2dataverse.monitor.Monitor.diff_files[\u2018delete\u2019] to discover Dataverse file ids for deletion. Parameters: filelist ( list ) \u2013 List of Dryad file tuples: eg: [('https://datadryad.org/api/v2/files/385819/download', 'GCB_ACG_Mortality_2020.zip', 'application/x-zip-compressed', 23787587), ('https://datadryad.org/api/v2/files/385820/download', 'Readme_ACG_Mortality.txt', 'text/plain', 1350)] Source code in src/dryad2dataverse/monitor.py def get_dv_fids(self, filelist): ''' Returns Dataverse file IDs from a list of Dryad file tuples. Generally, you would use the output from dryad2dataverse.monitor.Monitor.diff_files['delete'] to discover Dataverse file ids for deletion. Parameters ---------- filelist : list List of Dryad file tuples: eg: ``` [('https://datadryad.org/api/v2/files/385819/download', 'GCB_ACG_Mortality_2020.zip', 'application/x-zip-compressed', 23787587), ('https://datadryad.org/api/v2/files/385820/download', 'Readme_ACG_Mortality.txt', 'text/plain', 1350)] ``` ''' fids = [] for f in filelist: fids.append(self.get_dv_fid(f[0])) return fids","title":"get_dv_fids"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.get_json_dvfids","text":"Return a list of Dataverse file ids for Dryad JSONs which were uploaded to Dataverse. Normally used to discover the file IDs to remove Dryad JSONs which have changed. Parameters: serial ( Serializer ) \u2013 Returns: list \u2013 Source code in src/dryad2dataverse/monitor.py def get_json_dvfids(self, serial)->list: ''' Return a list of Dataverse file ids for Dryad JSONs which were uploaded to Dataverse. Normally used to discover the file IDs to remove Dryad JSONs which have changed. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- list ''' self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE doi=?', (serial.doi,)) try: uid = self.cursor.fetchone()[0] self.cursor.execute('SELECT dvfid FROM dvFiles WHERE \\ dryaduid = ? AND dryfid=?', (uid, 0)) jsonfid = [f[0] for f in self.cursor.fetchall()] return jsonfid except TypeError: return []","title":"get_json_dvfids"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.set_timestamp","text":"Adds current time to the database table. Can be queried and be used for subsequent checking for updates. To query last modification time, use the dataverse2dryad.monitor.Monitor.lastmod attribute. Parameters: curdate ( str , default: None ) \u2013 UTC datetime string in the format suitable for the Dryad API. eg. 2021-01-21T21:42:40Z or .strftime(\u2018%Y-%m-%dT%H:%M:%SZ\u2019). Source code in src/dryad2dataverse/monitor.py def set_timestamp(self, curdate=None): ''' Adds current time to the database table. Can be queried and be used for subsequent checking for updates. To query last modification time, use the dataverse2dryad.monitor.Monitor.lastmod attribute. Parameters ---------- curdate : str UTC datetime string in the format suitable for the Dryad API. eg. 2021-01-21T21:42:40Z or .strftime('%Y-%m-%dT%H:%M:%SZ'). ''' #Dryad API uses Zulu time if not curdate: curdate = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%dT%H:%M:%SZ') self.cursor.execute('INSERT INTO lastcheck VALUES (?)', (curdate,)) self.conn.commit()","title":"set_timestamp"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.status","text":"Returns a dictionary with keys \u2018status\u2019 and \u2018dvpid\u2019 and \u2018notes\u2019. Parameters: serial ( dryad2dataverse.serializer.Serializer ) \u2013 Returns: `{status :'updated', 'dvpid':'doi://some/ident'}`. \u2013 Notes status is one of \u2018new\u2019, \u2018identical\u2019, \u2018lastmodsame\u2019, \u2018updated\u2019 \u2018new\u2019 is a completely new file. \u2018identical\u2019 The metadata from Dryad is identical to the last time the check was run. \u2018lastmodsame\u2019 Dryad lastModificationDate == last modification date in database AND output JSON is different. This can indicate a Dryad API output change, reindexing or something else. But the lastModificationDate is supposed to be an indicator of meaningful change, so this option exists so you can decide what to do given this option \u2018updated\u2019 Indicates changes to lastModificationDate Note that Dryad constantly changes their API output, so the changes may not actually be meaningful. dvpid is a Dataverse persistent identifier. None in the case of status=\u2019new\u2019 notes : value of Dryad versionChanges field. One of files_changed or metatdata_changed . Non-null value present only when status is not new or identical . Note that Dryad has no way to indicate both a file and metadata change, so this value reflects only the last change in the Dryad state. Source code in src/dryad2dataverse/monitor.py def status(self, serial)->dict: ''' Returns a dictionary with keys 'status' and 'dvpid' and 'notes'. Parameters ---------- serial : dryad2dataverse.serializer.Serializer Returns ------- `{status :'updated', 'dvpid':'doi://some/ident'}`. Notes ------ `status` is one of 'new', 'identical', 'lastmodsame', 'updated' 'new' is a completely new file. 'identical' The metadata from Dryad is *identical* to the last time the check was run. 'lastmodsame' Dryad lastModificationDate == last modification date in database AND output JSON is different. This can indicate a Dryad API output change, reindexing or something else. But the lastModificationDate is supposed to be an indicator of meaningful change, so this option exists so you can decide what to do given this option 'updated' Indicates changes to lastModificationDate Note that Dryad constantly changes their API output, so the changes may not actually be meaningful. `dvpid` is a Dataverse persistent identifier. `None` in the case of status='new' `notes`: value of Dryad versionChanges field. One of `files_changed` or `metatdata_changed`. Non-null value present only when status is not `new` or `identical`. Note that Dryad has no way to indicate *both* a file and metadata change, so this value reflects only the *last* change in the Dryad state. ''' # Last mod date is indicator of change. # From email w/Ryan Scherle 10 Nov 2020 #The versionNumber updates for either a metadata change or a #file change. Although we save all of these changes internally, our web #interface only displays the versions that have file changes, along #with the most recent metadata. So a dataset that has only two versions #of files listed on the web may actually have several more versions in #the API. # #If your only need is to track when there are changes to a #dataset, you may want to use the `lastModificationDate`, which we have #recently added to our metadata. # #Note that the Dryad API output ISN'T STABLE; they add fields etc. #This means that a comparison of JSON may yield differences even though #metadata is technically \"the same\". Just comparing two dicts doesn't cut #it. ############################# ## Note: by inspection, Dryad outputs JSON that is different ## EVEN IF lastModificationDate is unchanged. (14 January 2022) ## So now what? ############################# doi = serial.dryadJson['identifier'] self.cursor.execute('SELECT * FROM dryadStudy WHERE doi = ?', (doi,)) result = self.cursor.fetchall() if not result: return {'status': 'new', 'dvpid': None, 'notes': ''} # dvjson = json.loads(result[-1][4]) # Check the fresh vs. updated jsons for the keys try: dryaduid = result[-1][0] self.cursor.execute('SELECT dvpid from dvStudy WHERE \\ dryaduid = ?', (dryaduid,)) dvpid = self.cursor.fetchall()[-1][0] serial.dvpid = dvpid except TypeError: try: raise exceptions.DatabaseError except exceptions.DatabaseError as e: LOGGER.error('Dryad DOI : %s. Error finding Dataverse PID', doi) LOGGER.exception(e) raise newfile = copy.deepcopy(serial.dryadJson) testfile = copy.deepcopy(json.loads(result[-1][3])) if newfile == testfile: return {'status': 'identical', 'dvpid': dvpid, 'notes': ''} if newfile['lastModificationDate'] != testfile['lastModificationDate']: return {'status': 'updated', 'dvpid': dvpid, 'notes': newfile['versionChanges']} return {'status': 'lastmodsame', 'dvpid': dvpid, 'notes': newfile.get('versionChanges')}","title":"status"},{"location":"api_reference/#dryad2dataverse.monitor.Monitor.update","text":"Updates the Monitor database with information from a dryad2dataverse.transfer.Transfer instance. If a Dryad primary metadata record has changes, it will be deleted from the database. This method should be called after all transfers are completed, including Dryad JSON updates, as the last action for transfer. Parameters: transfer ( Transfer ) \u2013 Source code in src/dryad2dataverse/monitor.py def update(self, transfer): ''' Updates the Monitor database with information from a dryad2dataverse.transfer.Transfer instance. If a Dryad primary metadata record has changes, it will be deleted from the database. This method should be called after all transfers are completed, including Dryad JSON updates, as the last action for transfer. Parameters ---------- transfer : dryad2dataverse.transfer.Transfer ''' # get the pre-update dryad uid in case we need it. self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE doi = ?', (transfer.dryad.dryadJson['identifier'],)) olduid = self.cursor.fetchone()[0] if olduid: olduid = int(olduid) if self.status(transfer.dryad)['status'] != 'unchanged': doi = transfer.doi lastmod = transfer.dryad.dryadJson.get('lastModificationDate') dryadJson = json.dumps(transfer.dryad.dryadJson) dvJson = json.dumps(transfer.dvStudy) # Update study metadata self.cursor.execute('INSERT INTO dryadStudy \\ (doi, lastmoddate, dryadjson, dvjson) \\ VALUES (?, ?, ?, ?)', (doi, lastmod, dryadJson, dvJson)) self.cursor.execute('SELECT max(uid) FROM dryadStudy WHERE \\ doi = ?', (doi,)) dryaduid = self.cursor.fetchone()[0] #if type(dryaduid) != int: if not isinstance(dryaduid, int): try: raise TypeError('Dryad UID is not an integer') except TypeError as e: LOGGER.error(e) raise # Update dryad file json self.cursor.execute('INSERT INTO dryadFiles VALUES (?, ?)', (dryaduid, json.dumps(transfer.dryad.fileJson))) # Update dataverse study map self.cursor.execute('SELECT dvpid FROM dvStudy WHERE \\ dvpid = ?', (transfer.dryad.dvpid,)) if not self.cursor.fetchone(): self.cursor.execute('INSERT INTO dvStudy VALUES (?, ?)', (dryaduid, transfer.dryad.dvpid)) else: self.cursor.execute('UPDATE dvStudy SET dryaduid=?, \\ dvpid=? WHERE dvpid =?', (dryaduid, transfer.dryad.dvpid, transfer.dryad.dvpid)) # Update the files table # Because we want to have a *complete* file list for each # dryaduid, we have to copy any existing old files, # then add and delete. if olduid: self.cursor.execute('SELECT * FROM dvFiles WHERE \\ dryaduid=?', (olduid,)) inserter = self.cursor.fetchall() for rec in inserter: # TODONE FIX THIS #I think it's fixed 11 Feb 21 self.cursor.execute('INSERT INTO dvFiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, rec[1], rec[2], rec[3], rec[4], rec[5])) # insert newly uploaded files for rec in transfer.fileUpRecord: try: dvfid = rec[1]['data']['files'][0]['dataFile']['id'] # Screw you for burying the file ID this deep recMd5 = rec[1]['data']['files'][0]['dataFile']['checksum']['value'] except (KeyError, IndexError) as err: #write to failed uploads table instead status = rec[1].get('status') if not status: LOGGER.error('JSON read error for Dryad file ID %s', rec[0]) LOGGER.error('File %s for DOI %s may not be uploaded', rec[0], transfer.doi) LOGGER.exception(err) msg = {'status': 'Failure: Other non-specific ' 'failure. Check logs'} self.cursor.execute('INSERT INTO failed_uploads VALUES \\ (?, ?, ?);', (dryaduid, rec[0], json.dumps(msg))) continue self.cursor.execute('INSERT INTO failed_uploads VALUES \\ (?, ?, ?);', (dryaduid, rec[0], json.dumps(rec[1]))) LOGGER.warning(type(err)) LOGGER.warning('%s. DOI %s, File ID %s', rec[1].get('status'), transfer.doi, rec[0]) continue # md5s verified during upload step, so they should # match already self.cursor.execute('INSERT INTO dvFiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, rec[0], recMd5, dvfid, recMd5, json.dumps(rec[1]))) # Now the deleted files for rec in transfer.fileDelRecord: # fileDelRecord consists only of [fid,fid2, ...] # Dryad record ID is int not str self.cursor.execute('DELETE FROM dvFiles WHERE dvfid=? \\ AND dryaduid=?', (int(rec), dryaduid)) LOGGER.debug('deleted dryfid = %s, dryaduid = %s', rec, dryaduid) # And lastly, any JSON metadata updates: # NOW WHAT? # JSON has dryfid==0 self.cursor.execute('SELECT * FROM dvfiles WHERE \\ dryfid=? and dryaduid=?', (0, dryaduid)) try: exists = self.cursor.fetchone()[0] # Old metadata must be deleted on a change. if exists: shouldDel = self.status(transfer.dryad)['status'] if shouldDel == 'updated': self.cursor.execute('DELETE FROM dvfiles WHERE \\ dryfid=? and dryaduid=?', (0, dryaduid)) except TypeError: pass if transfer.jsonFlag: # update dryad JSON djson5 = transfer.jsonFlag[1]['data']['files'][0]['dataFile']['checksum']['value'] dfid = transfer.jsonFlag[1]['data']['files'][0]['dataFile']['id'] self.cursor.execute('INSERT INTO dvfiles VALUES \\ (?, ?, ?, ?, ?, ?)', (dryaduid, 0, djson5, dfid, djson5, json.dumps(transfer.jsonFlag[1]))) self.conn.commit()","title":"update"},{"location":"api_reference/#dryad2dataverse.handlers","text":"Custom log handlers for sending log information to recipients.","title":"handlers"},{"location":"api_reference/#dryad2dataverse.handlers.SSLSMTPHandler","text":"Bases: SMTPHandler An SSL handler for logging.handlers Source code in src/dryad2dataverse/handlers.py class SSLSMTPHandler(SMTPHandler): ''' An SSL handler for logging.handlers ''' def emit(self, record:logging.LogRecord): ''' Emit a record while using an SSL mail server. Parameters ---------- record : logging.LogRecord ''' #Praise be to #https://stackoverflow.com/questions/36937461/ #how-can-i-send-an-email-using-python-loggings- #smtphandler-and-ssl try: port = self.mailport if not port: port = smtplib.SMTP_PORT smtp = smtplib.SMTP_SSL(self.mailhost, port) msg = self.format(record) out = EmailMessage() out['Subject'] = self.getSubject(record) out['From'] = self.fromaddr out['To'] = self.toaddrs out.set_content(msg) #global rec2 #rec2 = record if self.username: smtp.login(self.username, self.password) #smtp.sendmail(self.fromaddr, self.toaddrs, msg) #Attempting to send using smtp.sendmail as above #results in messages with no text, so use smtp.send_message(out) smtp.quit() except (KeyboardInterrupt, SystemExit): raise except: # pylint: disable=bare-except self.handleError(record)","title":"SSLSMTPHandler"},{"location":"api_reference/#dryad2dataverse.handlers.SSLSMTPHandler.emit","text":"Emit a record while using an SSL mail server. Parameters: record ( LogRecord ) \u2013 Source code in src/dryad2dataverse/handlers.py def emit(self, record:logging.LogRecord): ''' Emit a record while using an SSL mail server. Parameters ---------- record : logging.LogRecord ''' #Praise be to #https://stackoverflow.com/questions/36937461/ #how-can-i-send-an-email-using-python-loggings- #smtphandler-and-ssl try: port = self.mailport if not port: port = smtplib.SMTP_PORT smtp = smtplib.SMTP_SSL(self.mailhost, port) msg = self.format(record) out = EmailMessage() out['Subject'] = self.getSubject(record) out['From'] = self.fromaddr out['To'] = self.toaddrs out.set_content(msg) #global rec2 #rec2 = record if self.username: smtp.login(self.username, self.password) #smtp.sendmail(self.fromaddr, self.toaddrs, msg) #Attempting to send using smtp.sendmail as above #results in messages with no text, so use smtp.send_message(out) smtp.quit() except (KeyboardInterrupt, SystemExit): raise except: # pylint: disable=bare-except self.handleError(record)","title":"emit"},{"location":"api_reference/#dryad2dataverse.exceptions","text":"Custom exceptions for error handling.","title":"exceptions"},{"location":"api_reference/#dryad2dataverse.exceptions.DatabaseError","text":"Bases: Dryad2DataverseError Tracking database error. Source code in src/dryad2dataverse/exceptions.py class DatabaseError(Dryad2DataverseError): ''' Tracking database error. '''","title":"DatabaseError"},{"location":"api_reference/#dryad2dataverse.exceptions.DataverseBadApiKeyError","text":"Bases: Dryad2DataverseError Returned on not OK respose (ie, request.request.json()[\u2018message\u2019] == \u2018Bad api key \u2018). Source code in src/dryad2dataverse/exceptions.py class DataverseBadApiKeyError(Dryad2DataverseError): ''' Returned on not OK respose (ie, request.request.json()['message'] == 'Bad api key '). '''","title":"DataverseBadApiKeyError"},{"location":"api_reference/#dryad2dataverse.exceptions.DataverseDownloadError","text":"Bases: Dryad2DataverseError Returned on not OK respose (ie, not requests.status_code == 200). Source code in src/dryad2dataverse/exceptions.py class DataverseDownloadError(Dryad2DataverseError): ''' Returned on not OK respose (ie, not requests.status_code == 200). '''","title":"DataverseDownloadError"},{"location":"api_reference/#dryad2dataverse.exceptions.DataverseUploadError","text":"Bases: Dryad2DataverseError Returned on not OK respose (ie, not requests.status_code == 200). Source code in src/dryad2dataverse/exceptions.py class DataverseUploadError(Dryad2DataverseError): ''' Returned on not OK respose (ie, not requests.status_code == 200). '''","title":"DataverseUploadError"},{"location":"api_reference/#dryad2dataverse.exceptions.DownloadSizeError","text":"Bases: Dryad2DataverseError Raised when download sizes don\u2019t match reported Dryad file size. Source code in src/dryad2dataverse/exceptions.py class DownloadSizeError(Dryad2DataverseError): ''' Raised when download sizes don't match reported Dryad file size. '''","title":"DownloadSizeError"},{"location":"api_reference/#dryad2dataverse.exceptions.Dryad2DataverseError","text":"Bases: Exception Base exception class for Dryad2Dataverse errors. Source code in src/dryad2dataverse/exceptions.py class Dryad2DataverseError(Exception): ''' Base exception class for Dryad2Dataverse errors. '''","title":"Dryad2DataverseError"},{"location":"api_reference/#dryad2dataverse.exceptions.HashError","text":"Bases: Dryad2DataverseError Raised on hex digest mismatch. Source code in src/dryad2dataverse/exceptions.py class HashError(Dryad2DataverseError): ''' Raised on hex digest mismatch. '''","title":"HashError"},{"location":"api_reference/#dryad2dataverse.exceptions.NoTargetError","text":"Bases: Dryad2DataverseError No dataverse target supplied error. Source code in src/dryad2dataverse/exceptions.py class NoTargetError(Dryad2DataverseError): ''' No dataverse target supplied error. '''","title":"NoTargetError"},{"location":"api_reference/#dryad2dataverse.constants","text":"This module contains the information that configures all the parameters required to transfer data from Dryad to Dataverse. \u201cConstants\u201d may be a bit strong, but the only constant is the presence of change.","title":"constants"},{"location":"changelog/","text":"Dryad2dataverse changelog \u00b6 Perfection on the first attempt is rare. v0.6.1 - 11 May 2023 \u00b6 ** dryad2dataverse.constants** Eliminated deprecated parameter in call to urllib3.util.Retry as per https://github.com/urllib3/urllib3/issues/2092 v0.6.0 - 2 May 2023 \u00b6 Updated to current Python packaging standards Installation can now be done straight from pip without resorting to git+http. . . Test framework now unittest Documentation updated dryad2dataverse.constants constants.dvurl now defaults to https://borealisdata.ca pathlib instead of os.path for greater cross-platform compatibility. v0.5.8 - 10 February 2023 \u00b6 Good lord I misspelled \u201cFebruary\u201d initially certifi requirements updated somewhat better error logging for odd requests failures Dataverse JSON change to be in compliance with new standards for v5.12+ Note that binaries will no longer be produced for dryad2dataverse. If you want one, you can either contact me and I will create one and add it to the release, or you can make one yourself using PyInstaller or Nuitka. The effort of making them vs the lack of downloads has led to this decision. If this is a problem please create an issue for discussion. v0.5.0 - 7 December 2022 \u00b6 dataverse.handlers New handler component Custom SSL log handler SSLSMTPHandler added which reduces frequency of email problems dryadd.py Mail formatting changed to ensure lines are less than 1000 characters in length to adhere to [RFC 2825 4.5.3.5] (https://www.rfc-editor.org/rfc/rfc2821#section-4.5.3.1). Logging messages contain more information Default mail service changed to Yahoo mail Default dataverse server destination changed to https://borealisdata.ca Help text cleanup v0.3.1 - 4 February 2022 \u00b6 Changes to the Dryad API sparked a few changes to dryad2dataverse and dryadd.py. More specifically, the Dryad /search API endpoint can produce false positive results, which can result in bulk study replacement when none is required. Additionally, as file IDs are not unique in Dryad (contrary to the documentation), files are no longer identified on the basis of Dryad file ID. dryad2dataverse.serializer Serial.files output now includes explicit hash type dryad2dataverse.monitor Monitor.status() now returns values of new, unchanged, updated, filesonly Monitor.status() now includes notes key Monitor.diff_files() now outputs a list of files for new studies using the add key instead of producing an empty dict. Monitor.diff_files() outputs of hash_change key listing files whose names and sizes are identical but have either a changed hash or a new one.. Note that this does not necessarily indicate a changed file as hashes have been added to existing files. Monitor.get_dv_fid() now explicitly selects highest ROWID when returning a Dryad UID as UIDs are not considered persistent identifiers (as per email from Dryad January 2022) dryadd.py Dates are now filtered by metadata lastModificationDate as Dryad search API endpoint does not respect date parameter (as per Dryad email, January 2022). Databases are now backed up with suffix of .YYYY-MM-DD-HHMM instead of generic .bak Number of backups can be specified as a parameter Switch added to halt process on excessive number of study updates Study threshold added to specify what is considered \u201cexcessive\u201d Recipients are emailed on halt due to excessive updates Verbosity increased Output now explicity includes lists of new files instead of empty dict Updates now skipped on report of unchanged or lastmodsame ; ie. metadata is identical or the lastModificationDate field in the Dryad JSON unchanged. Other Binary files are now only included as part of a Github release Binary release now includes linux x86-64 Dataverse utilities scripts removed; use dataverse_utils instead. v0.1.4 - 22 Sept 2021 \u00b6 requirements.txt Updated version requirements for urllib3 and requests to plug dependabot alert hole. dryadd.py Updated associated dryadd.py binaries to use newer versions of requests and urllib3 v0.1.3 - 10 August 2021 \u00b6 setup.py Enhanced information dryadd.py Script repair for better functioning on Windows platforms v0.1.2 - 4 May 2021 \u00b6 fixed error in setup.py added binaries of dryadd for Windows and Mac v0.1.1 - 30 April 2021 \u00b6 dryad2dataverse improved versioning system dryad2dataverse.serializer Fixed bug where keywords were only serialized when grants were present dryad2dataverse.transfer Added better defaults for transfer.set_correct_date dryad2dataverse.monitor Added meaningless change to monitor.update for internal consistency scripts/dryadd.py Show version option added transfer.set_correct_date() added to set citation to match Dryad citation. v0.1.0 - 08 April 2021 \u00b6 Initial release","title":"Changelog"},{"location":"changelog/#dryad2dataverse-changelog","text":"Perfection on the first attempt is rare.","title":"Dryad2dataverse changelog"},{"location":"changelog/#v061-11-may-2023","text":"** dryad2dataverse.constants** Eliminated deprecated parameter in call to urllib3.util.Retry as per https://github.com/urllib3/urllib3/issues/2092","title":"v0.6.1 - 11 May 2023"},{"location":"changelog/#v060-2-may-2023","text":"Updated to current Python packaging standards Installation can now be done straight from pip without resorting to git+http. . . Test framework now unittest Documentation updated dryad2dataverse.constants constants.dvurl now defaults to https://borealisdata.ca pathlib instead of os.path for greater cross-platform compatibility.","title":"v0.6.0 - 2 May 2023"},{"location":"changelog/#v058-10-february-2023","text":"Good lord I misspelled \u201cFebruary\u201d initially certifi requirements updated somewhat better error logging for odd requests failures Dataverse JSON change to be in compliance with new standards for v5.12+ Note that binaries will no longer be produced for dryad2dataverse. If you want one, you can either contact me and I will create one and add it to the release, or you can make one yourself using PyInstaller or Nuitka. The effort of making them vs the lack of downloads has led to this decision. If this is a problem please create an issue for discussion.","title":"v0.5.8 - 10 February 2023"},{"location":"changelog/#v050-7-december-2022","text":"dataverse.handlers New handler component Custom SSL log handler SSLSMTPHandler added which reduces frequency of email problems dryadd.py Mail formatting changed to ensure lines are less than 1000 characters in length to adhere to [RFC 2825 4.5.3.5] (https://www.rfc-editor.org/rfc/rfc2821#section-4.5.3.1). Logging messages contain more information Default mail service changed to Yahoo mail Default dataverse server destination changed to https://borealisdata.ca Help text cleanup","title":"v0.5.0 - 7 December 2022"},{"location":"changelog/#v031-4-february-2022","text":"Changes to the Dryad API sparked a few changes to dryad2dataverse and dryadd.py. More specifically, the Dryad /search API endpoint can produce false positive results, which can result in bulk study replacement when none is required. Additionally, as file IDs are not unique in Dryad (contrary to the documentation), files are no longer identified on the basis of Dryad file ID. dryad2dataverse.serializer Serial.files output now includes explicit hash type dryad2dataverse.monitor Monitor.status() now returns values of new, unchanged, updated, filesonly Monitor.status() now includes notes key Monitor.diff_files() now outputs a list of files for new studies using the add key instead of producing an empty dict. Monitor.diff_files() outputs of hash_change key listing files whose names and sizes are identical but have either a changed hash or a new one.. Note that this does not necessarily indicate a changed file as hashes have been added to existing files. Monitor.get_dv_fid() now explicitly selects highest ROWID when returning a Dryad UID as UIDs are not considered persistent identifiers (as per email from Dryad January 2022) dryadd.py Dates are now filtered by metadata lastModificationDate as Dryad search API endpoint does not respect date parameter (as per Dryad email, January 2022). Databases are now backed up with suffix of .YYYY-MM-DD-HHMM instead of generic .bak Number of backups can be specified as a parameter Switch added to halt process on excessive number of study updates Study threshold added to specify what is considered \u201cexcessive\u201d Recipients are emailed on halt due to excessive updates Verbosity increased Output now explicity includes lists of new files instead of empty dict Updates now skipped on report of unchanged or lastmodsame ; ie. metadata is identical or the lastModificationDate field in the Dryad JSON unchanged. Other Binary files are now only included as part of a Github release Binary release now includes linux x86-64 Dataverse utilities scripts removed; use dataverse_utils instead.","title":"v0.3.1 - 4 February 2022"},{"location":"changelog/#v014-22-sept-2021","text":"requirements.txt Updated version requirements for urllib3 and requests to plug dependabot alert hole. dryadd.py Updated associated dryadd.py binaries to use newer versions of requests and urllib3","title":"v0.1.4 - 22 Sept 2021"},{"location":"changelog/#v013-10-august-2021","text":"setup.py Enhanced information dryadd.py Script repair for better functioning on Windows platforms","title":"v0.1.3 - 10 August 2021"},{"location":"changelog/#v012-4-may-2021","text":"fixed error in setup.py added binaries of dryadd for Windows and Mac","title":"v0.1.2 - 4 May 2021"},{"location":"changelog/#v011-30-april-2021","text":"dryad2dataverse improved versioning system dryad2dataverse.serializer Fixed bug where keywords were only serialized when grants were present dryad2dataverse.transfer Added better defaults for transfer.set_correct_date dryad2dataverse.monitor Added meaningless change to monitor.update for internal consistency scripts/dryadd.py Show version option added transfer.set_correct_date() added to set citation to match Dryad citation.","title":"v0.1.1 - 30 April 2021"},{"location":"changelog/#v010-08-april-2021","text":"Initial release","title":"v0.1.0 - 08 April 2021"},{"location":"compiling/","text":"Compiling and/or packaging the dryadd script \u00b6 While binaries for Windows, Mac and Linux are supplied , should you wish to create them yourself from the dryadd.py script, you can so so following the procedure below. This can be done with either nuitka or PyInstaller . Note that nuitka will compile to a C application, but PyInstaller packages everything to create a standalone application that is not a pure C application. Whether this matters is open for debate. Windows \u00b6 With nuitka : nuitka --onefile --windows-product-name=dryadd --windows-product-version=0.1.1 --windows-company-name=\"University of British Columbia Library\" \\path\\to\\dryad2dataverse\\scripts\\dryadd.py Note that the \u2013windows-product-[x] options are required, but there\u2019s nothing preventing you from using whatever information you prefer. Also, the version string listed on this page is only an example; use the current version. with PyInstaller : python -m PyInstaller -F \\path\\to\\dryad2dataverse\\scripts\\dryadd.py Mac and Linux \u00b6 with nuitka : nuitka --onefile /path/to/dryad2dataverse/scripts/dryadd.py Note that with some installations of linux, you may be required to supply a Python icon if one is not found on your system. Either download or create one (and a PNG will work) and suppy it with --linux-onefile-icon=ICON_PATH with PyInstaller : python -m PyInstaller -F \\path\\to\\dryad2dataverse\\scripts\\dryadd.py","title":"Compiling or packaging the application"},{"location":"compiling/#compiling-andor-packaging-the-dryadd-script","text":"While binaries for Windows, Mac and Linux are supplied , should you wish to create them yourself from the dryadd.py script, you can so so following the procedure below. This can be done with either nuitka or PyInstaller . Note that nuitka will compile to a C application, but PyInstaller packages everything to create a standalone application that is not a pure C application. Whether this matters is open for debate.","title":"Compiling and/or packaging the dryadd script"},{"location":"compiling/#windows","text":"With nuitka : nuitka --onefile --windows-product-name=dryadd --windows-product-version=0.1.1 --windows-company-name=\"University of British Columbia Library\" \\path\\to\\dryad2dataverse\\scripts\\dryadd.py Note that the \u2013windows-product-[x] options are required, but there\u2019s nothing preventing you from using whatever information you prefer. Also, the version string listed on this page is only an example; use the current version. with PyInstaller : python -m PyInstaller -F \\path\\to\\dryad2dataverse\\scripts\\dryadd.py","title":"Windows"},{"location":"compiling/#mac-and-linux","text":"with nuitka : nuitka --onefile /path/to/dryad2dataverse/scripts/dryadd.py Note that with some installations of linux, you may be required to supply a Python icon if one is not found on your system. Either download or create one (and a PNG will work) and suppy it with --linux-onefile-icon=ICON_PATH with PyInstaller : python -m PyInstaller -F \\path\\to\\dryad2dataverse\\scripts\\dryadd.py","title":"Mac and Linux"},{"location":"credits/","text":"Credits \u00b6 Contact \u00b6 dryad2dataverse was written by Paul Lesack of the University of British Columbia Library Research Commons . Acknowledgements \u00b6 No software development is done in a vacuum, and this project is no exception. Thanks to Eugene Barsky,Doug Brigham and Jeremy Buhler of the University of British Columbia library Research Commons for their assistance and support, to Ryan Scherle of Dryad for his help with the Dryad API, and the helpful people at Dataverse . Without the fabulous requests library and the requests toolbelt everything would have taken a great deal longer. Without pydoc-markdown , mkdocs and schemaspy the documentation would have taken much, much longer to write.","title":"Credits"},{"location":"credits/#credits","text":"","title":"Credits"},{"location":"credits/#contact","text":"dryad2dataverse was written by Paul Lesack of the University of British Columbia Library Research Commons .","title":"Contact"},{"location":"credits/#acknowledgements","text":"No software development is done in a vacuum, and this project is no exception. Thanks to Eugene Barsky,Doug Brigham and Jeremy Buhler of the University of British Columbia library Research Commons for their assistance and support, to Ryan Scherle of Dryad for his help with the Dryad API, and the helpful people at Dataverse . Without the fabulous requests library and the requests toolbelt everything would have taken a great deal longer. Without pydoc-markdown , mkdocs and schemaspy the documentation would have taken much, much longer to write.","title":"Acknowledgements"},{"location":"faq/","text":"Frequently asked questions \u00b6 Why did dryadd just download everything again? \u00b6 The Dryad API has not yet reached a stable state and the output from the API is subject to format changes. This has the possibility of triggering a potentially false positive change indication in dryad2dataverse.monitor.Monitor . Using dryad2dataverse >= v0.3.1 uses (hopefully) a more robust change checking mechanism which will eliminate (or at least drastically reduce) the number of false positive hits. In addition dryadd >= v0.4.1 includes a warning mechanism and auto-shutdown if the number of studies to be uploaded exceeds a user specified threshold, allowing the user to examine the nature of the problem to see if, in fact, there are multiple studies with changes. Note that the false positives do not upload incorrect data; they will just create a new version of the same data set. This is can be annoying and potentially use a lot of storage space, but for small collections it is more of an annoyance than a problem. Why is the upload script (dryadd.py) constantly crashing with SMTP errors? \u00b6 Updated 7 December 2022 Version 0.5.0 + should solve this issue. Google authentication using app passwords is now supported, but requires that the account use two-factor authentication. If you are not at v0.5.0+, the old, deprecated answer is: If you are using Gmail to send messages about your migration, there are a few potential hurdles. You must enable less secure app access . Even when you do that, it can still crash with no obvious explanation or a mysterious authentication error. In this case, the script may be encountering a Captcha security measure. You can remove this by going to https://accounts.google.com/DisplayUnlockCaptcha before running your script (when logged into the account which you are using, of course). The settings revert back to normal after some period of time of which I am not aware. Daily or weekly updates should be OK, but monthly ones will probably fail with SMTP errors as the service reverts to defaults. Your other option is to not use Gmail. smtplib exceptions will cause a script crash, so if you are experiencing persistent mail problems and still wish to use the script, you may wish disable emailing log messages. This is easily accomplished by commenting out the section starting with elog = email_log( in scripts/dryadd.py . Obviously you can\u2019t do this if you\u2019re using a binary dryadd. Currently email notifications are a mandatory part of the dryadd.py app, but this may be optional and/or more advanced mail handling may be available in later releases. All error messages are written to the log anyway, so if you disable emailing of log messages you can still see them in the transfer log. Why is the upload script (dryadd.py) is crashing with dryad2dataverse.exceptions.DownloadSizeError size errors? \u00b6 There are a few instances when the script will crash on an exception and this is one of them. This occurs when the download size does not match reported size of the file in Dryad. There are two obvious alternatives here. The first is that the download was corrupted in some way. In this case, you should go to your temporary file location and delete the offending file(s). Run the script again and it should continue as normal. The other, much more insidious error comes from Dryad. A very few Dryad studies have files with duplicate names. These are not visible on the web page for the study, but are visible via the file API. As the files are named on download with the names given to them by Dryad, this is a problem because two files cannot have the same name. Additionally, because only one of the files appears on the Dryad page without any associated metadata, it\u2019s not possible to tell which one is which without a manual inspection. Presumably this should not be happening, as the number of files on the Dryad web page and the number of files available via API should match. There is no way to resolve this error without consulting the people at Dryad. In this case, the only workable solution is to exclude the problematic Dryad study from the upload. Do this by noting the Dryad DOI and then using the -x, --exclude switch. python3 dryadd.py [bunch of stuff] -x doi:10.5061/dryad.7pd82 & Why is the upload script (dryadd.py) is crashing with 404 errors for data files? \u00b6 Related to the error above, in a very few instances the Dryad web page is displaying an embargo on the page, but the Dryad JSON does not have {curationStatus: Embargoed} , which means that dryad2dataverse.serializer.Serializer.embargo == False . This means that instead of skipping the download, it is attempted. But the embargo flag is incorrect and the files are unavailable, generating a 404 error. As there is no other way to determine embargo status other than by inspecting the Dryad study web page (and even then perhaps not), the solution is to exclude the DOI using the -x switch. python3 dryadd.py [bunch of stuff] -x doi:10.5061/dryad.b74d971 & Why is my transfer to Dataverse not showing up as published? \u00b6 dryad2dataverse does not publish the dataset. That must still be done via the Dataverse GUI or API. Publication functionality has been omitted by design : File size limits within a default Dataverse installation that do not apply to Dryad, so it\u2019s possible that some files need to be moved with the assistance of a Datverse system administrator Although every attempt has been made to map metadata schemas appropriately, it\u2019s undoubtedly not perfect. A quick once-over by a human can notice any unusual or unforeseen errors Metadata quality standards can vary between Dryad and a Dataverse installations. A manual curation step is sometimes desirable to ensure that all published records meet the same standards. But I don\u2019t want to manually curate my data \u00b6 It\u2019s possible to publish via the Dataverse API. If you really want to publish automatically, you can obtain a list of unpublished studies from Dataverse and publish them programatically. This is left as an exercise for the reader. Why does my large file download/upload fail? \u00b6 By default, Dataverse limits file sizes to 3 Gb, but that can vary by installation. dryad2dataverse.constants.MAX_UPLOAD contains the value which should correspond to the maximum upload size in Dataverse. If you don\u2019t know what the upload size is, contact the system administrator of your target Dataverse installation to find out. To upload files exceeding the API upload limit, you will need to speak to a Dataverse administrator. Why does my upload of files fail halfway? \u00b6 Dataverse will automatically cease ingest and lock a study when encountering a file which is suitable for tabular processing. The only way to stop this behaviour is to prohibit ingest in the Dataverse configuration, which is probably not possible for many users of the software. To circumvent this, dryad2dataverse attempts to fool Dataverse into not processing the tabular file, by changing the extension or MIME type at upload time. If this doesn\u2019t work and tabular processing starts anyway, by default the dryadd.py script will wait for tabular processing to finish before continuing with the next file. As you may imagine, that can add some time to the process. If you are a super-user , you can attempt a forcible unlock allow uploads to continue. This process, unfortunately, is not perfect as for some reason Dataverse returns 403 errors instead of unlocking, albeit infrequently. Why is a file which should not be a tabular file a tabular file? \u00b6 As a direct result of the above, tabular file processing has (hopefully) been eliminated. It\u2019s still possible to create a tabular file by reingesting it. Unless you are are the administrator of a Dataverse installation, you likely don\u2019t have control over what is or is not considered a tabular file. dryad2dataverse attempts to block all tabular file processing, but the process is imperfect. The only way to guarantee that tabular processing won\u2019t occur is to stop it on the Dataverse server. If you are not a Dataverse super-user, then you are out of luck and my poor spoofing attempts are what you get. Sic vita. Why does the code use camel case instead of snake case for variables? \u00b6 By the time I realized I should be using snake case, it was too late and I was already consistently using camel case. https://www.python.org/dev/peps/pep-0008/#a-foolish-consistency-is-the-hobgoblin-of-little-minds","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"faq/#why-did-dryadd-just-download-everything-again","text":"The Dryad API has not yet reached a stable state and the output from the API is subject to format changes. This has the possibility of triggering a potentially false positive change indication in dryad2dataverse.monitor.Monitor . Using dryad2dataverse >= v0.3.1 uses (hopefully) a more robust change checking mechanism which will eliminate (or at least drastically reduce) the number of false positive hits. In addition dryadd >= v0.4.1 includes a warning mechanism and auto-shutdown if the number of studies to be uploaded exceeds a user specified threshold, allowing the user to examine the nature of the problem to see if, in fact, there are multiple studies with changes. Note that the false positives do not upload incorrect data; they will just create a new version of the same data set. This is can be annoying and potentially use a lot of storage space, but for small collections it is more of an annoyance than a problem.","title":"Why did dryadd just download everything again?"},{"location":"faq/#why-is-the-upload-script-dryaddpy-constantly-crashing-with-smtp-errors","text":"Updated 7 December 2022 Version 0.5.0 + should solve this issue. Google authentication using app passwords is now supported, but requires that the account use two-factor authentication. If you are not at v0.5.0+, the old, deprecated answer is: If you are using Gmail to send messages about your migration, there are a few potential hurdles. You must enable less secure app access . Even when you do that, it can still crash with no obvious explanation or a mysterious authentication error. In this case, the script may be encountering a Captcha security measure. You can remove this by going to https://accounts.google.com/DisplayUnlockCaptcha before running your script (when logged into the account which you are using, of course). The settings revert back to normal after some period of time of which I am not aware. Daily or weekly updates should be OK, but monthly ones will probably fail with SMTP errors as the service reverts to defaults. Your other option is to not use Gmail. smtplib exceptions will cause a script crash, so if you are experiencing persistent mail problems and still wish to use the script, you may wish disable emailing log messages. This is easily accomplished by commenting out the section starting with elog = email_log( in scripts/dryadd.py . Obviously you can\u2019t do this if you\u2019re using a binary dryadd. Currently email notifications are a mandatory part of the dryadd.py app, but this may be optional and/or more advanced mail handling may be available in later releases. All error messages are written to the log anyway, so if you disable emailing of log messages you can still see them in the transfer log.","title":"Why is the upload script (dryadd.py) constantly crashing with SMTP errors?"},{"location":"faq/#why-is-the-upload-script-dryaddpy-is-crashing-with-dryad2dataverseexceptionsdownloadsizeerror-size-errors","text":"There are a few instances when the script will crash on an exception and this is one of them. This occurs when the download size does not match reported size of the file in Dryad. There are two obvious alternatives here. The first is that the download was corrupted in some way. In this case, you should go to your temporary file location and delete the offending file(s). Run the script again and it should continue as normal. The other, much more insidious error comes from Dryad. A very few Dryad studies have files with duplicate names. These are not visible on the web page for the study, but are visible via the file API. As the files are named on download with the names given to them by Dryad, this is a problem because two files cannot have the same name. Additionally, because only one of the files appears on the Dryad page without any associated metadata, it\u2019s not possible to tell which one is which without a manual inspection. Presumably this should not be happening, as the number of files on the Dryad web page and the number of files available via API should match. There is no way to resolve this error without consulting the people at Dryad. In this case, the only workable solution is to exclude the problematic Dryad study from the upload. Do this by noting the Dryad DOI and then using the -x, --exclude switch. python3 dryadd.py [bunch of stuff] -x doi:10.5061/dryad.7pd82 &","title":"Why is the upload script (dryadd.py) is crashing with dryad2dataverse.exceptions.DownloadSizeError size errors?"},{"location":"faq/#why-is-the-upload-script-dryaddpy-is-crashing-with-404-errors-for-data-files","text":"Related to the error above, in a very few instances the Dryad web page is displaying an embargo on the page, but the Dryad JSON does not have {curationStatus: Embargoed} , which means that dryad2dataverse.serializer.Serializer.embargo == False . This means that instead of skipping the download, it is attempted. But the embargo flag is incorrect and the files are unavailable, generating a 404 error. As there is no other way to determine embargo status other than by inspecting the Dryad study web page (and even then perhaps not), the solution is to exclude the DOI using the -x switch. python3 dryadd.py [bunch of stuff] -x doi:10.5061/dryad.b74d971 &","title":"Why is the upload script (dryadd.py) is crashing with 404 errors for data files?"},{"location":"faq/#why-is-my-transfer-to-dataverse-not-showing-up-as-published","text":"dryad2dataverse does not publish the dataset. That must still be done via the Dataverse GUI or API. Publication functionality has been omitted by design : File size limits within a default Dataverse installation that do not apply to Dryad, so it\u2019s possible that some files need to be moved with the assistance of a Datverse system administrator Although every attempt has been made to map metadata schemas appropriately, it\u2019s undoubtedly not perfect. A quick once-over by a human can notice any unusual or unforeseen errors Metadata quality standards can vary between Dryad and a Dataverse installations. A manual curation step is sometimes desirable to ensure that all published records meet the same standards.","title":"Why is my transfer to Dataverse not showing up as published?"},{"location":"faq/#but-i-dont-want-to-manually-curate-my-data","text":"It\u2019s possible to publish via the Dataverse API. If you really want to publish automatically, you can obtain a list of unpublished studies from Dataverse and publish them programatically. This is left as an exercise for the reader.","title":"But I don&rsquo;t want to manually curate my data"},{"location":"faq/#why-does-my-large-file-downloadupload-fail","text":"By default, Dataverse limits file sizes to 3 Gb, but that can vary by installation. dryad2dataverse.constants.MAX_UPLOAD contains the value which should correspond to the maximum upload size in Dataverse. If you don\u2019t know what the upload size is, contact the system administrator of your target Dataverse installation to find out. To upload files exceeding the API upload limit, you will need to speak to a Dataverse administrator.","title":"Why does my large file download/upload fail?"},{"location":"faq/#why-does-my-upload-of-files-fail-halfway","text":"Dataverse will automatically cease ingest and lock a study when encountering a file which is suitable for tabular processing. The only way to stop this behaviour is to prohibit ingest in the Dataverse configuration, which is probably not possible for many users of the software. To circumvent this, dryad2dataverse attempts to fool Dataverse into not processing the tabular file, by changing the extension or MIME type at upload time. If this doesn\u2019t work and tabular processing starts anyway, by default the dryadd.py script will wait for tabular processing to finish before continuing with the next file. As you may imagine, that can add some time to the process. If you are a super-user , you can attempt a forcible unlock allow uploads to continue. This process, unfortunately, is not perfect as for some reason Dataverse returns 403 errors instead of unlocking, albeit infrequently.","title":"Why does my upload of files fail halfway?"},{"location":"faq/#why-is-a-file-which-should-not-be-a-tabular-file-a-tabular-file","text":"As a direct result of the above, tabular file processing has (hopefully) been eliminated. It\u2019s still possible to create a tabular file by reingesting it. Unless you are are the administrator of a Dataverse installation, you likely don\u2019t have control over what is or is not considered a tabular file. dryad2dataverse attempts to block all tabular file processing, but the process is imperfect. The only way to guarantee that tabular processing won\u2019t occur is to stop it on the Dataverse server. If you are not a Dataverse super-user, then you are out of luck and my poor spoofing attempts are what you get. Sic vita.","title":"Why is a file which should not be a tabular file a tabular file?"},{"location":"faq/#why-does-the-code-use-camel-case-instead-of-snake-case-for-variables","text":"By the time I realized I should be using snake case, it was too late and I was already consistently using camel case. https://www.python.org/dev/peps/pep-0008/#a-foolish-consistency-is-the-hobgoblin-of-little-minds","title":"Why does the code use camel case instead of snake case for variables?"},{"location":"installation/","text":"Installation \u00b6 This is not a complete list of installation methods. For a complete guide to Python package installation, please see https://packaging.python.org/tutorials/installing-packages/ . As a baseline, you will need to install a version of Python >= 3.6. Simple installation using Pip \u00b6 Once you\u2019ve installed Python, installation via pip is very simple: pip install dryad2dataverse Of course, if you want to use a branch other than master , you can switch master for the branch you want. This is not recommended, though, as the master branch contains the most current [stable] release. Installing from the Github repository \u00b6 If you require a specific commit, branch, etc, you can install directly from Github using pip : pip install git+https://github.com/ubc-library-rc/dryad2dataverse.git@master Of course, you can also install other branches or specific commits as required; see the documentation for pip on how to do that. Manual Download \u00b6 Precompiled binaries \u00b6 Compiled versions of the dryadd migrator for selected operating systems and architectures are available at the releases page . Note that binary releases may lag behind the Python, and of course the binary files don\u2019t include the Python package. From local source code \u00b6 The source code for this project is available at https://github.com/ubc-library-rc/dryad2dataverse To install, first clone the repository: git clone https://github.com/ubc-library-rc/dryad2dataverse.git This will place the source at whatever/directory/you/are/in/dryad2dataverse If you wish to install with pip , you can use: cd dryad2dataverse pip install . or, if you are planning to tinker with the source code: pip install -e . Using dryad2dataverse with a virtual environment \u00b6 First create a directory that will hold your virtual environment In a terminal, change to that directory install the virtual environment using: python3 -m venv . Activate the virtual environment: source bin/activate (Linux and Mac) or .\\Scripts\\activate on Windows. Install as per one of the methods above. More information on virtual environments can be found on the Python website: https://docs.python.org/3.6/tutorial/venv.html Keeping up to date \u00b6 If you have installed with pip, upgrading is easy: pip install --upgrade dryad2dataverse.git","title":"Installation"},{"location":"installation/#installation","text":"This is not a complete list of installation methods. For a complete guide to Python package installation, please see https://packaging.python.org/tutorials/installing-packages/ . As a baseline, you will need to install a version of Python >= 3.6.","title":"Installation"},{"location":"installation/#simple-installation-using-pip","text":"Once you\u2019ve installed Python, installation via pip is very simple: pip install dryad2dataverse Of course, if you want to use a branch other than master , you can switch master for the branch you want. This is not recommended, though, as the master branch contains the most current [stable] release.","title":"Simple installation using Pip"},{"location":"installation/#installing-from-the-github-repository","text":"If you require a specific commit, branch, etc, you can install directly from Github using pip : pip install git+https://github.com/ubc-library-rc/dryad2dataverse.git@master Of course, you can also install other branches or specific commits as required; see the documentation for pip on how to do that.","title":"Installing from the Github repository"},{"location":"installation/#manual-download","text":"","title":"Manual Download"},{"location":"installation/#precompiled-binaries","text":"Compiled versions of the dryadd migrator for selected operating systems and architectures are available at the releases page . Note that binary releases may lag behind the Python, and of course the binary files don\u2019t include the Python package.","title":"Precompiled binaries"},{"location":"installation/#from-local-source-code","text":"The source code for this project is available at https://github.com/ubc-library-rc/dryad2dataverse To install, first clone the repository: git clone https://github.com/ubc-library-rc/dryad2dataverse.git This will place the source at whatever/directory/you/are/in/dryad2dataverse If you wish to install with pip , you can use: cd dryad2dataverse pip install . or, if you are planning to tinker with the source code: pip install -e .","title":"From local source code"},{"location":"installation/#using-dryad2dataverse-with-a-virtual-environment","text":"First create a directory that will hold your virtual environment In a terminal, change to that directory install the virtual environment using: python3 -m venv . Activate the virtual environment: source bin/activate (Linux and Mac) or .\\Scripts\\activate on Windows. Install as per one of the methods above. More information on virtual environments can be found on the Python website: https://docs.python.org/3.6/tutorial/venv.html","title":"Using dryad2dataverse with a virtual environment"},{"location":"installation/#keeping-up-to-date","text":"If you have installed with pip, upgrading is easy: pip install --upgrade dryad2dataverse.git","title":"Keeping up to date"},{"location":"other_utils/","text":"Other useful dataverse utilities \u00b6 Other utilities, such as bulk release and delete utilities, can be very useful when dealing with Dataverse installations. These utilities are now in one collection, installable as a separate product: Get dataverse_utils The tools formerly included on this page are there, as well as other potentially useful utilities.","title":"Other utilities"},{"location":"other_utils/#other-useful-dataverse-utilities","text":"Other utilities, such as bulk release and delete utilities, can be very useful when dealing with Dataverse installations. These utilities are now in one collection, installable as a separate product: Get dataverse_utils The tools formerly included on this page are there, as well as other potentially useful utilities.","title":"Other useful dataverse utilities"},{"location":"reference/","text":"General Reference \u00b6 This page covers material that isn\u2019t automatically generated from the source code, that is, the API reference section. Information regarding specific modules is below. dryad2dataverse.constants \u00b6 And by \u201cconstants\u201d, you should change these as required. This module contains the information that configures all the parameters required to transfer data from Dryad to Dataverse. As \u2018constants\u2019 don\u2019t generally change, there\u2019s a non-zero chance that the name of this module will change. General variables \u00b6 RETRY_STRATEGY + This is a urllib3.util Retry object which controls the connection attemps of a requests.Session object. Not all connections are guaranteed to be successful the first time round, and the Retry object will allow multiple connection attempts before raising an exception. + Default: 10 attempts, with exponentially increased times between attempts. + For more information/a tutorial on how to use the Retry object, please see https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/#retry-on-failure DRYURL + Base URL for the Dryad data repository. + Default = \u2018https://datadryad.org\u2019 It\u2019s unlikely you will ever change this, but Dryad is an open source project, so it\u2019s not out of the realm of possibility that there will be another Dryad-style repository. TMP + Temporary file download location. Note that downloaded files have the potential of being very large, so select a directory which has sufficient space. + Default =\u2019/tmp\u2019 This is configured for *nix style environments. Windows does not, by default, have /tmp directory, for instance. Data transfer variables \u00b6 DVURL + Base URL for dataverse installation + Default = \u2018https://dataverse.scholarsportal.info\u2019 Obviously, if you are not transferring your data to the Scholars Portal , you will need to change this. APIKEY + Dataverse API key for user performing transfer. Sufficient privileges for upload and metadata manipulation must be attached to the user. See Dataverse API documentation for an explanation of the privilege level required. + Default = None To avoid issues, using an API key which has administrator privileges for the target dataverse is the easiest apprach. MAX_UPLOAD + Maximum upload file size in bytes. Files exceeding this size will be ignored. By default, Dataverse has a 3GB upload size limit + Default = 3221225472 Files will not be downloaded or uploaded if their (reported) size exceeds this limit. DV_CONTACT_EMAIL + Dataverse \u201cContact\u201d email address. Required as part of Dataverse metadata. This would generally be the email address of the data set curator + Default= None API uploads to Dataverse fail without a contact email. While dryad2dataverse attempts to read email addresses from Dryad records, they are not required in Dryad. DV_CONTACT_NAME + Dataverse \u201cContact\u201d name. Required as part of Dataverse metadata. Generally the name of the data set curator, whether individual or an organization + Default = None As with contact email addresses, contact names are required in Dataverse, but not in Dryad. NOTAB + File extensions which should have tabular processing disabled. Lower case only. + Dataverse will immediately cease ingest and lock a dataset when encountering a file which can be processed to .tab format. This causes upload crashes unless disabled. + Files may be converted to .tab format after upload using Dataverse\u2019s reingest endpoint: https://guides.dataverse.org/en/latest/api/native-api.html#reingest-a-file + Default = [\u2018.sav\u2019, \u2018.por\u2019, \u2018.zip\u2019, \u2018.csv\u2019, \u2018.tsv\u2019, \u2018.dta\u2019, \u2018.rdata\u2019, \u2018.xslx\u2019] If one of the files in the upload triggers tabular processing the upload will suddenly cease and fail. This behaviour is built into Dataverse (unfortunately), and can be only overcome through workarounds such as double-zipping files, or, in this case, spoofing MIME types and extensions. Because Dataverse\u2019s tabular file processing capabilities are subject to change, this is not an exhaustive list and some files may be processed regardless. See also dryad2dataverse.transfer.Transfer.force_notab_unlock(). Monitoring database variables \u00b6 HOME + Home directory path for user + Default = os.path.expanduser(\u2018~\u2019) Home directory for the user. There is probably no reason to change this. DBASE + Full path for transfer monitoring sqlite3 database + Default = HOME + os.sep + \u2018dryad_dataverse_monitor.sqlite3\u2019 By default, the monitoring/tracking database will be created in the user\u2019s home directory, which is convenient but not necessarily not ideal. The location can also be set on instantiation of dryad2dataverse.monitor.Monitor : eg monitor = dryad2dataverse.monitor.Monitor('/path/to/tracking/directory/databasename.sqlite3')","title":"General Reference"},{"location":"reference/#general-reference","text":"This page covers material that isn\u2019t automatically generated from the source code, that is, the API reference section. Information regarding specific modules is below.","title":"General Reference"},{"location":"reference/#dryad2dataverseconstants","text":"And by \u201cconstants\u201d, you should change these as required. This module contains the information that configures all the parameters required to transfer data from Dryad to Dataverse. As \u2018constants\u2019 don\u2019t generally change, there\u2019s a non-zero chance that the name of this module will change.","title":"dryad2dataverse.constants"},{"location":"reference/#general-variables","text":"RETRY_STRATEGY + This is a urllib3.util Retry object which controls the connection attemps of a requests.Session object. Not all connections are guaranteed to be successful the first time round, and the Retry object will allow multiple connection attempts before raising an exception. + Default: 10 attempts, with exponentially increased times between attempts. + For more information/a tutorial on how to use the Retry object, please see https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/#retry-on-failure DRYURL + Base URL for the Dryad data repository. + Default = \u2018https://datadryad.org\u2019 It\u2019s unlikely you will ever change this, but Dryad is an open source project, so it\u2019s not out of the realm of possibility that there will be another Dryad-style repository. TMP + Temporary file download location. Note that downloaded files have the potential of being very large, so select a directory which has sufficient space. + Default =\u2019/tmp\u2019 This is configured for *nix style environments. Windows does not, by default, have /tmp directory, for instance.","title":"General variables"},{"location":"reference/#data-transfer-variables","text":"DVURL + Base URL for dataverse installation + Default = \u2018https://dataverse.scholarsportal.info\u2019 Obviously, if you are not transferring your data to the Scholars Portal , you will need to change this. APIKEY + Dataverse API key for user performing transfer. Sufficient privileges for upload and metadata manipulation must be attached to the user. See Dataverse API documentation for an explanation of the privilege level required. + Default = None To avoid issues, using an API key which has administrator privileges for the target dataverse is the easiest apprach. MAX_UPLOAD + Maximum upload file size in bytes. Files exceeding this size will be ignored. By default, Dataverse has a 3GB upload size limit + Default = 3221225472 Files will not be downloaded or uploaded if their (reported) size exceeds this limit. DV_CONTACT_EMAIL + Dataverse \u201cContact\u201d email address. Required as part of Dataverse metadata. This would generally be the email address of the data set curator + Default= None API uploads to Dataverse fail without a contact email. While dryad2dataverse attempts to read email addresses from Dryad records, they are not required in Dryad. DV_CONTACT_NAME + Dataverse \u201cContact\u201d name. Required as part of Dataverse metadata. Generally the name of the data set curator, whether individual or an organization + Default = None As with contact email addresses, contact names are required in Dataverse, but not in Dryad. NOTAB + File extensions which should have tabular processing disabled. Lower case only. + Dataverse will immediately cease ingest and lock a dataset when encountering a file which can be processed to .tab format. This causes upload crashes unless disabled. + Files may be converted to .tab format after upload using Dataverse\u2019s reingest endpoint: https://guides.dataverse.org/en/latest/api/native-api.html#reingest-a-file + Default = [\u2018.sav\u2019, \u2018.por\u2019, \u2018.zip\u2019, \u2018.csv\u2019, \u2018.tsv\u2019, \u2018.dta\u2019, \u2018.rdata\u2019, \u2018.xslx\u2019] If one of the files in the upload triggers tabular processing the upload will suddenly cease and fail. This behaviour is built into Dataverse (unfortunately), and can be only overcome through workarounds such as double-zipping files, or, in this case, spoofing MIME types and extensions. Because Dataverse\u2019s tabular file processing capabilities are subject to change, this is not an exhaustive list and some files may be processed regardless. See also dryad2dataverse.transfer.Transfer.force_notab_unlock().","title":"Data transfer variables"},{"location":"reference/#monitoring-database-variables","text":"HOME + Home directory path for user + Default = os.path.expanduser(\u2018~\u2019) Home directory for the user. There is probably no reason to change this. DBASE + Full path for transfer monitoring sqlite3 database + Default = HOME + os.sep + \u2018dryad_dataverse_monitor.sqlite3\u2019 By default, the monitoring/tracking database will be created in the user\u2019s home directory, which is convenient but not necessarily not ideal. The location can also be set on instantiation of dryad2dataverse.monitor.Monitor : eg monitor = dryad2dataverse.monitor.Monitor('/path/to/tracking/directory/databasename.sqlite3')","title":"Monitoring database variables"},{"location":"script/","text":"Automated migrator and tracker - dryadd \u00b6 While it\u2019s all very nice that there\u2019s code that can migrate Dryad material to Dataverse, many users are not familiar enough with Python/programming or, just as likely, don\u2019t want to have to program things themselves. Anyone transferring from Dryad to Dataverse is likely doing a variant of the same thing, which consists of: Finding new Dryad material, usually from their own institution Moving it to Dataverse and possibly: Checking for updates and handling those automatically Included with dryad2dataverse package is a console application called dryadd which does all of this. Or, if you don\u2019t even want to install dryad2dtaverse, binary files for Windows, MacOS and Linux . Depending on what computing platform and installation method you use, the application will be called dryadd.py, dryadd , dryadd_linux or dryadd.exe . Note that there are a wide variety of system architectures available, but not all of them. The most current version of dryadd will always be available if you install via pip . The binary files may lag behind and/or not get every release Note that these utilities are console programs. That is, they do not have a GUI and are meant to be run from the command line in a Windows DOS prompt or PowerShell session or a terminal in the case of other platforms. An important caveat \u00b6 This product will not publish anything in a Dataverse installation (at this time, at least). This is intentional to allow a human-based curatorial step before releasing any data onto an unsuspecting audience. There\u2019s no error like systemic error, so not automatically releasing material should help alleviate this. Usage \u00b6 The implementation is relatively straightforward. Simply supply the required parameters and the software should do the rest. The help menu below is available from the command line by either running the script without inputs or by using the -h switch. usage: dryadd [-h] [-u URL] -k KEY -t TARGET -e EMAIL -s USER -r RECIPIENTS [RECIPIENTS ...] -p PWD [--server MAILSERV] [--port PORT] -c CONTACT -n CNAME [-v] -i ROR [--tmpfile TMP] [--db DBASE] [--log LOG] [-l] [-x EXCLUDE [EXCLUDE ...]] [-b NUM_BACKUPS] [-w] [--warn-threshold WARN] [--testmode-on] [--testmode-limit TESTLIMIT] [--version] Dryad to Dataverse importer/monitor. All arguments NOT enclosed by square brackets are required for the script to run but some may already have defaults, specified by \"Default\". The \"optional arguments\" below refers to the use of the option switch, (like -u), meaning \"not a positional argument.\" options: -h, --help show this help message and exit -u URL, --dv-url URL Destination Dataverse root url. Default: https://borealisdata.ca -k KEY, --key KEY REQUIRED: API key for dataverse user -t TARGET, --target TARGET REQUIRED: Target dataverse short name -e EMAIL, --email EMAIL REQUIRED: Email address which sends update notifications. ie: \"user@website.invalid\". -s USER, --user USER REQUIRED: User name for SMTP server. Check your server for details. -r RECIPIENTS [RECIPIENTS ...], --recipient RECIPIENTS [RECIPIENTS ...] REQUIRED: Recipient(s) of email notification. Separate addresses with spaces -p PWD, --pwd PWD REQUIRED: Password for sending email account. Enclose in single quotes to avoid OS errors with special characters. --server MAILSERV Mail server for sending account. Default: smtp.mail.yahoo.com --port PORT Mail server port. Default: 465. Mail is sent using SSL. -c CONTACT, --contact CONTACT REQUIRED: Contact email address for Dataverse records. Must pass Dataverse email validation rules (so \"test@test.invalid\" is not acceptable). -n CNAME, --contact-name CNAME REQUIRED: Contact name for Dataverse records -v, --verbosity Verbose output -i ROR, --ror ROR REQUIRED: Institutional ROR URL. Eg: \"https://ror.org/03rmrcq20\". This identifies the institution in Dryad repositories. --tmpfile TMP Temporary file location. Default: /tmp --db DBASE Tracking database location and name. Default: $HOME/dryad_dataverse_monitor.sqlite3 --log LOG Complete path to log. Default: /var/log/dryadd.log -l, --no_force_unlock No forcible file unlock. Required if /lock endpint is restricted -x EXCLUDE [EXCLUDE ...], --exclude EXCLUDE [EXCLUDE ...] Exclude these DOIs. Separate by spaces -b NUM_BACKUPS, --num-backups NUM_BACKUPS Number of database backups to keep. Default 3 -w, --warn-too-many Warn and halt execution if abnormally large number of updates present. --warn-threshold WARN Do not transfer studies if number of updates is greater than or equal to this number. Default: 15 --testmode-on Turn on test mode. Number of transfers will be limited to the value in --testmode-limit or 5 if you don't set --testmode-limit --testmode-limit TESTLIMIT Test mode - only transfer first [n] of the total number of (new) records. Old ones will still be updated, though. Default: 5 --version Show version number and exit Requirements \u00b6 Software If you installed using pip the requirements will be filled by default (see the installation document for more details). If using a binary file, it must be supported by your operating system and system architecture (eg. Intel Mac). Hardware You will need sufficient storage space on your system to hold the contents of the largest Dryad record that you are transferring. This is not necessarily a small amount; Dryad studies can range into the tens or hundreds of Gb, which means that a \u201cnormal\u201d /tmp directory will normally not have enough space allocated to it. The software will work on one study at a time and delete the files as it goes, but there are studies in the Dryad repository that are huge, even if most of them are quite small. Other A destination Dataverse must exist, and you should know its short name. The API key must have sufficient privileges to create new studies and upload data. You will need an email address for contact information as this is a required field in Dataverse (but not necessarily in Dryad) and a name to go with it. For example, i_heart_data@test.invalid and Dataverse Support . Note: Use a valid email address (unlike the example) because uploads will also fail if the address is invalid. Information for an email address which sends notifications The sending email address (\u201cuser@test.invalid\u201d) The user name (usually, but not always, \u201cuser\u201d from \u201cuser@test.invalid\u201d) The password for this account The smtp server address which sends mail. For example, if using gmail, it\u2019s smtp.gmail.com The port required to send email via SSL. At least one email address to receive update and error notifications. This can be the same as the sender. A place to store your sqlite3 tracking database. A note about GMail Dryad2dataverse is now set up to use yahoo email by default, because it doesn\u2019t require two-factor authentication to use. If you decide to use Google mail, you will need to follow the procedure outlined here https://support.google.com/accounts/answer/185833?hl=en . Note that it will require enabling two-factor authentication. Updates to Dryad studies The software is designed to automatically update changed studies. Simply run the utility with the same parameters as the previous run and any studies in Dataverse will be updated Miscellaneous The dryadd/.py/.exe works best if run at intervals. This can easily be achieved by adding it to your system\u2019s crontab or using the Windows scheduler. Currently it does not run as a system or service, although it may in the future. Dryad itself is constantly changing, as is Dataverse. Although the software should work predictably, changes in both the Dryad API and Dataverse API can cause unforeseen consequences. To act as a backup against catastrophic error, the monitoring database is automatically copied and renamed with a timestamp. Although the default number of backups is 3 by default, any number of backups can be kept. Obviously, if you run the software once a minute this isn\u2019t helpful, but it could be if you update once a month.","title":"The dryadd console application"},{"location":"script/#automated-migrator-and-tracker-dryadd","text":"While it\u2019s all very nice that there\u2019s code that can migrate Dryad material to Dataverse, many users are not familiar enough with Python/programming or, just as likely, don\u2019t want to have to program things themselves. Anyone transferring from Dryad to Dataverse is likely doing a variant of the same thing, which consists of: Finding new Dryad material, usually from their own institution Moving it to Dataverse and possibly: Checking for updates and handling those automatically Included with dryad2dataverse package is a console application called dryadd which does all of this. Or, if you don\u2019t even want to install dryad2dtaverse, binary files for Windows, MacOS and Linux . Depending on what computing platform and installation method you use, the application will be called dryadd.py, dryadd , dryadd_linux or dryadd.exe . Note that there are a wide variety of system architectures available, but not all of them. The most current version of dryadd will always be available if you install via pip . The binary files may lag behind and/or not get every release Note that these utilities are console programs. That is, they do not have a GUI and are meant to be run from the command line in a Windows DOS prompt or PowerShell session or a terminal in the case of other platforms.","title":"Automated migrator and tracker - dryadd"},{"location":"script/#an-important-caveat","text":"This product will not publish anything in a Dataverse installation (at this time, at least). This is intentional to allow a human-based curatorial step before releasing any data onto an unsuspecting audience. There\u2019s no error like systemic error, so not automatically releasing material should help alleviate this.","title":"An important caveat"},{"location":"script/#usage","text":"The implementation is relatively straightforward. Simply supply the required parameters and the software should do the rest. The help menu below is available from the command line by either running the script without inputs or by using the -h switch. usage: dryadd [-h] [-u URL] -k KEY -t TARGET -e EMAIL -s USER -r RECIPIENTS [RECIPIENTS ...] -p PWD [--server MAILSERV] [--port PORT] -c CONTACT -n CNAME [-v] -i ROR [--tmpfile TMP] [--db DBASE] [--log LOG] [-l] [-x EXCLUDE [EXCLUDE ...]] [-b NUM_BACKUPS] [-w] [--warn-threshold WARN] [--testmode-on] [--testmode-limit TESTLIMIT] [--version] Dryad to Dataverse importer/monitor. All arguments NOT enclosed by square brackets are required for the script to run but some may already have defaults, specified by \"Default\". The \"optional arguments\" below refers to the use of the option switch, (like -u), meaning \"not a positional argument.\" options: -h, --help show this help message and exit -u URL, --dv-url URL Destination Dataverse root url. Default: https://borealisdata.ca -k KEY, --key KEY REQUIRED: API key for dataverse user -t TARGET, --target TARGET REQUIRED: Target dataverse short name -e EMAIL, --email EMAIL REQUIRED: Email address which sends update notifications. ie: \"user@website.invalid\". -s USER, --user USER REQUIRED: User name for SMTP server. Check your server for details. -r RECIPIENTS [RECIPIENTS ...], --recipient RECIPIENTS [RECIPIENTS ...] REQUIRED: Recipient(s) of email notification. Separate addresses with spaces -p PWD, --pwd PWD REQUIRED: Password for sending email account. Enclose in single quotes to avoid OS errors with special characters. --server MAILSERV Mail server for sending account. Default: smtp.mail.yahoo.com --port PORT Mail server port. Default: 465. Mail is sent using SSL. -c CONTACT, --contact CONTACT REQUIRED: Contact email address for Dataverse records. Must pass Dataverse email validation rules (so \"test@test.invalid\" is not acceptable). -n CNAME, --contact-name CNAME REQUIRED: Contact name for Dataverse records -v, --verbosity Verbose output -i ROR, --ror ROR REQUIRED: Institutional ROR URL. Eg: \"https://ror.org/03rmrcq20\". This identifies the institution in Dryad repositories. --tmpfile TMP Temporary file location. Default: /tmp --db DBASE Tracking database location and name. Default: $HOME/dryad_dataverse_monitor.sqlite3 --log LOG Complete path to log. Default: /var/log/dryadd.log -l, --no_force_unlock No forcible file unlock. Required if /lock endpint is restricted -x EXCLUDE [EXCLUDE ...], --exclude EXCLUDE [EXCLUDE ...] Exclude these DOIs. Separate by spaces -b NUM_BACKUPS, --num-backups NUM_BACKUPS Number of database backups to keep. Default 3 -w, --warn-too-many Warn and halt execution if abnormally large number of updates present. --warn-threshold WARN Do not transfer studies if number of updates is greater than or equal to this number. Default: 15 --testmode-on Turn on test mode. Number of transfers will be limited to the value in --testmode-limit or 5 if you don't set --testmode-limit --testmode-limit TESTLIMIT Test mode - only transfer first [n] of the total number of (new) records. Old ones will still be updated, though. Default: 5 --version Show version number and exit","title":"Usage"},{"location":"script/#requirements","text":"Software If you installed using pip the requirements will be filled by default (see the installation document for more details). If using a binary file, it must be supported by your operating system and system architecture (eg. Intel Mac). Hardware You will need sufficient storage space on your system to hold the contents of the largest Dryad record that you are transferring. This is not necessarily a small amount; Dryad studies can range into the tens or hundreds of Gb, which means that a \u201cnormal\u201d /tmp directory will normally not have enough space allocated to it. The software will work on one study at a time and delete the files as it goes, but there are studies in the Dryad repository that are huge, even if most of them are quite small. Other A destination Dataverse must exist, and you should know its short name. The API key must have sufficient privileges to create new studies and upload data. You will need an email address for contact information as this is a required field in Dataverse (but not necessarily in Dryad) and a name to go with it. For example, i_heart_data@test.invalid and Dataverse Support . Note: Use a valid email address (unlike the example) because uploads will also fail if the address is invalid. Information for an email address which sends notifications The sending email address (\u201cuser@test.invalid\u201d) The user name (usually, but not always, \u201cuser\u201d from \u201cuser@test.invalid\u201d) The password for this account The smtp server address which sends mail. For example, if using gmail, it\u2019s smtp.gmail.com The port required to send email via SSL. At least one email address to receive update and error notifications. This can be the same as the sender. A place to store your sqlite3 tracking database. A note about GMail Dryad2dataverse is now set up to use yahoo email by default, because it doesn\u2019t require two-factor authentication to use. If you decide to use Google mail, you will need to follow the procedure outlined here https://support.google.com/accounts/answer/185833?hl=en . Note that it will require enabling two-factor authentication. Updates to Dryad studies The software is designed to automatically update changed studies. Simply run the utility with the same parameters as the previous run and any studies in Dataverse will be updated Miscellaneous The dryadd/.py/.exe works best if run at intervals. This can easily be achieved by adding it to your system\u2019s crontab or using the Windows scheduler. Currently it does not run as a system or service, although it may in the future. Dryad itself is constantly changing, as is Dataverse. Although the software should work predictably, changes in both the Dryad API and Dataverse API can cause unforeseen consequences. To act as a backup against catastrophic error, the monitoring database is automatically copied and renamed with a timestamp. Although the default number of backups is 3 by default, any number of backups can be kept. Obviously, if you run the software once a minute this isn\u2019t helpful, but it could be if you update once a month.","title":"Requirements"},{"location":"track/","text":"The tracking database documentation was automatically generated by SchemasSpy and doesn\u2019t fit in nicely with the structure of this documentation. The link below will open in a new page/tab. Tracking database information","title":"Tracking Database Structure"}]}