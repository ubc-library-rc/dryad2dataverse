{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dryad2dataverse - translate, transfer and track \u00b6 \u00b6 Introduction \u00b6 dryad2dataverse is an oddly specific Python programming language library and an associated application which allows easier transfer of metadata and data from a Dryad data repository (ie, https://datadryad.org ) to a Dataverse repository. With these tools it\u2019s possible to: a) Serialize Dryad metadata to Dataverse JSON b) Transfer Dryad studies to Dataverse without any knowledge of the somewhat complex Dataverse API c) Monitor changes in status The minimum required Python version for dryad2dataverse is Python 3.6 , as it is the earliest version which supports f-strings . It was developed using version 3.7.2, and it runs just fine under the [now current] 3.9.2. You can find Python at https://www.python.org/downloads/ . No testing was done with Anaconda, only stock Python . That said, it probably works fine with Anaconda. Not just a library - no programming required \u00b6 If you hate programming, are pressed for time and/or don\u2019t want to read any documentation, dryad2dataverse also comes with a command line tool to do all of these things without requiring any knowledge of Python or programming. The dryad2dataverse library is free and open source, released under the MIT license. It\u2019s also not written by anyone with a degree in computer science, so as the MIT license says: Software is provided \"as is\", without warranty of any kind Why would I need this? \u00b6 There are a few reasons why you might find this product useful. You are a researcher and you wish to deposit via API into Dataverse repository. You\u2019ve used Dryad, but the Dataverse JSON and API is unfamiliar and complex . You want to write your Dryad JSON and have it convert automatically. Your institution has researchers who have deposited data into Dryad and you wish to copy them into the Dataverse repository which contains the bulk of your institution\u2019s research data (for example, the Dataverse repository at https://dataverse.scholarsportal.info ). And on top of that, you don\u2019t want to keep checking to see if there were any updates, so you wish to automate the process. Quick install \u00b6 git clone https://github.com/ubc-library-rc/dryad2dataverse.git cd dryad2dataverse pip install . Basic usage \u00b6 Converting JSON \u00b6 >>> #Convert Dryad JSON to Dataverse JSON and save to a file >>> import dryad2dataverse.serializer >>> i_heart_dryad = dryad2dataverse.serializer.Serializer('doi:10.5061/dryad.2rbnzs7jp') >>> with open('dataverse_json.json', 'w') as f: f.write(f'{i_heart_dryad.dvJson}') >>> #Or just view it this way in a Python session >>> i_heard_dryad.dvJson Transferring data \u00b6 Note: a number of variables must be set [correctly] for this to work, such as your target dataverse. This example continues with the Serializer instance above. >>> import dryad2dataverse.transfer >>> dv = dryad2dataverse.transfer.Transfer(i_heart_dryad) >>> # Files must first be downloaded; there is no direct transfer >>> dv.download_files() >>> # 'dryad' is the short name of the target dataverse >>> # Yours may be different >>> # First, create the study metadata >>> dv.upload_study(targetDv='dryad') >>> # Then upload the files >>> dv.upload_files() Change monitoring \u00b6 Because monitoring the status of something over time requires persistence, the dryad2dataverse.monitor.Monitor object uses an SQLite3 database, which has the enormous advantage of being a single file that is portable between systems. This allows monitoring without laborious database configuration on a host system, and updates can be run on any system that has sufficient storage space to act as an intermediary between Dryad and Dataverse. This is quite a simple database, as the documentation on its structure shows. If you need to change systems just swap the database to the new system. In theory you could run it from a Raspberry Pi Zero that you have in a desk drawer, although that may not be the wisest idea. Maybe use your cell phone. Monitoring changes requires both the Serializer and Transfer objects from above. >>> # Create the Monitor instance >>> monitor = dryad2dataverse.monitor.Monitor() >>> # Check status of your serializer object >>> monitor.status(i_heart_dryad) {'status': 'new', 'dvpid': None} >>> # imagine, now that i_still_heart_dryad is a study >>> # that was uploaded previously >>> monitor.status(i_still_heart_dryad) {'status': 'unchanged', 'dvpid': 'doi:99.99999/FK2/FAKER'} >>> #Check the difference in files >>> monitor.diff_files(i_still_heart_dryad) {} >>> # After the transfer dv above: >>> monitor.update(transfer) >>> # And then, to make your life easier, update the last time you checked Dryad >>> monitor.set_timestamp()","title":"Overview"},{"location":"#dryad2dataverse-translate-transfer-and-track","text":"","title":"dryad2dataverse -  translate, transfer and track"},{"location":"#_1","text":"","title":""},{"location":"#introduction","text":"dryad2dataverse is an oddly specific Python programming language library and an associated application which allows easier transfer of metadata and data from a Dryad data repository (ie, https://datadryad.org ) to a Dataverse repository. With these tools it\u2019s possible to: a) Serialize Dryad metadata to Dataverse JSON b) Transfer Dryad studies to Dataverse without any knowledge of the somewhat complex Dataverse API c) Monitor changes in status The minimum required Python version for dryad2dataverse is Python 3.6 , as it is the earliest version which supports f-strings . It was developed using version 3.7.2, and it runs just fine under the [now current] 3.9.2. You can find Python at https://www.python.org/downloads/ . No testing was done with Anaconda, only stock Python . That said, it probably works fine with Anaconda.","title":"Introduction"},{"location":"#not-just-a-library-no-programming-required","text":"If you hate programming, are pressed for time and/or don\u2019t want to read any documentation, dryad2dataverse also comes with a command line tool to do all of these things without requiring any knowledge of Python or programming. The dryad2dataverse library is free and open source, released under the MIT license. It\u2019s also not written by anyone with a degree in computer science, so as the MIT license says: Software is provided \"as is\", without warranty of any kind","title":"Not just a library - no programming required"},{"location":"#why-would-i-need-this","text":"There are a few reasons why you might find this product useful. You are a researcher and you wish to deposit via API into Dataverse repository. You\u2019ve used Dryad, but the Dataverse JSON and API is unfamiliar and complex . You want to write your Dryad JSON and have it convert automatically. Your institution has researchers who have deposited data into Dryad and you wish to copy them into the Dataverse repository which contains the bulk of your institution\u2019s research data (for example, the Dataverse repository at https://dataverse.scholarsportal.info ). And on top of that, you don\u2019t want to keep checking to see if there were any updates, so you wish to automate the process.","title":"Why would I need this?"},{"location":"#quick-install","text":"git clone https://github.com/ubc-library-rc/dryad2dataverse.git cd dryad2dataverse pip install .","title":"Quick install"},{"location":"#basic-usage","text":"","title":"Basic usage"},{"location":"#converting-json","text":">>> #Convert Dryad JSON to Dataverse JSON and save to a file >>> import dryad2dataverse.serializer >>> i_heart_dryad = dryad2dataverse.serializer.Serializer('doi:10.5061/dryad.2rbnzs7jp') >>> with open('dataverse_json.json', 'w') as f: f.write(f'{i_heart_dryad.dvJson}') >>> #Or just view it this way in a Python session >>> i_heard_dryad.dvJson","title":"Converting JSON"},{"location":"#transferring-data","text":"Note: a number of variables must be set [correctly] for this to work, such as your target dataverse. This example continues with the Serializer instance above. >>> import dryad2dataverse.transfer >>> dv = dryad2dataverse.transfer.Transfer(i_heart_dryad) >>> # Files must first be downloaded; there is no direct transfer >>> dv.download_files() >>> # 'dryad' is the short name of the target dataverse >>> # Yours may be different >>> # First, create the study metadata >>> dv.upload_study(targetDv='dryad') >>> # Then upload the files >>> dv.upload_files()","title":"Transferring data"},{"location":"#change-monitoring","text":"Because monitoring the status of something over time requires persistence, the dryad2dataverse.monitor.Monitor object uses an SQLite3 database, which has the enormous advantage of being a single file that is portable between systems. This allows monitoring without laborious database configuration on a host system, and updates can be run on any system that has sufficient storage space to act as an intermediary between Dryad and Dataverse. This is quite a simple database, as the documentation on its structure shows. If you need to change systems just swap the database to the new system. In theory you could run it from a Raspberry Pi Zero that you have in a desk drawer, although that may not be the wisest idea. Maybe use your cell phone. Monitoring changes requires both the Serializer and Transfer objects from above. >>> # Create the Monitor instance >>> monitor = dryad2dataverse.monitor.Monitor() >>> # Check status of your serializer object >>> monitor.status(i_heart_dryad) {'status': 'new', 'dvpid': None} >>> # imagine, now that i_still_heart_dryad is a study >>> # that was uploaded previously >>> monitor.status(i_still_heart_dryad) {'status': 'unchanged', 'dvpid': 'doi:99.99999/FK2/FAKER'} >>> #Check the difference in files >>> monitor.diff_files(i_still_heart_dryad) {} >>> # After the transfer dv above: >>> monitor.update(transfer) >>> # And then, to make your life easier, update the last time you checked Dryad >>> monitor.set_timestamp()","title":"Change monitoring"},{"location":"api_reference/","text":"Complete API reference \u00b6 dryad2dataverse \u00b6 Dryad to Dataverse utilities. No modules are loaded by default, so import dryad2dataverse will work, but will have no effect. Modules included: dryad2dataverse.constants : \"Constants\" for all modules. URLs, API keys, etc are all here. dryad2dataverse.serializer : Download and serialize Dryad JSON to Dataverse JSON. dryad2dataverse.transfer : metadata and file transfer utilities. dryad2dataverse.monitor : Monitoring and database tools for maintaining a pipeline to Dataverse without unnecessary downloading and file duplication. dryad2dataverse.exceptions : Custom exceptions. dryad2dataverse.constants \u00b6 This module contains the information that configures all the parameters required to transfer data from Dryad to Dataverse. \u201cConstants\u201d may be a bit strong, but the only constant is the presence of change. dryad2dataverse.serializer \u00b6 Serializes Dryad study JSON to Dataverse JSON, as well as producing associated file information. Serializer Objects \u00b6 class Serializer() Serializes Dryad JSON to Dataverse JSON __init__ \u00b6 | __init__(doi) Creates Dryad study metadata instance. Arguments : doi : str \u2014 DOI of Dryad study. Required for downloading. - eg - \u2018doi:10.5061/dryad.2rbnzs7jp\u2019 fetch_record \u00b6 | fetch_record(url=None, timeout=45) Fetches Dryad study record JSON from Dryad V2 API at https://datadryad.org/api/v2/datasets/. Saves to self._dryadJson. Querying Serializer.dryadJson will call this function automatically. Arguments : url : str \u2014 Dryad instance base URL (eg: \u2018https://datadryad.org\u2019). timeout : int \u2014 Timeout in seconds. Default 45. id \u00b6 | @property | id() Returns Dryad unique database ID, not the DOI. Where the original Dryad JSON is dryadJson, it\u2019s the integer trailing portion of: self.dryadJson['_links']['stash:version']['href'] dryadJson \u00b6 | @property | dryadJson() Returns Dryad study JSON. Will call Serializer.fetch_record() if no JSON is present. dryadJson \u00b6 | @dryadJson.setter | dryadJson(value=None) Fetches Dryad JSON from Dryad website if not supplied. If supplying it, make sure it\u2019s correct or you will run into trouble with processing later. Arguments : value : dict \u2014 Dryad JSON. embargo \u00b6 | @property | embargo() Check embargo status. Returns boolean True if embargoed. dvJson \u00b6 | @property | dvJson() Returns Dataverse study JSON as dict. fileJson \u00b6 | @property | fileJson(timeout=45) Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated, so the return consists of a list of dicts, one per page. Arguments : timeout : int \u2014 Request timeout in seconds. files \u00b6 | @property | files() Returns a list of tuples with: (Download_location, filename, mimetype, size, description, digestType, md5sum) At this time only md5 is supported. oversize \u00b6 | @property | oversize(maxsize=None) Returns a list of Dryad files whose size value exceeds maxsize. Maximum size defaults to dryad2dataverse.constants.MAX_UPLOAD Arguments : maxsize : int \u2014 Size in bytes in which to flag as oversize. Defaults to constants.MAX_UPLOAD. dryad2dataverse.transfer \u00b6 This module handles data downloads and uploads from a Dryad instance to a Dataverse instance Transfer Objects \u00b6 class Transfer() Transfers metadata and data files from a Dryad installation to Dataverse installation. __init__ \u00b6 | __init__(dryad) Creates a dryad2dataverse.transfer.Transfer instance. Arguments : dryad : dryad2dataverse.serializer.Serializer instance _del__ \u00b6 | _del__() Expunges files from constants.TMP on deletion dvpid \u00b6 | @property | dvpid() Returns Dataverse study persistent ID as str. auth \u00b6 | @property | auth() Returns datavese authentication header dict. ie: {X-Dataverse-key' : 'APIKEYSTRING'} fileJson \u00b6 | @property | fileJson() Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated. files \u00b6 | @property | files() Returns a list of lists with: [Download_location, filename, mimetype, size, description, md5digest] This is mutable; downloading a file will add md5 info if not available. oversize \u00b6 | @property | oversize() Returns list of files exceeding Dataverse ingest limit dryad2dataverse.constants.MAX_UPLOAD. doi \u00b6 | @property | doi() Returns Dryad DOI. set_correct_date \u00b6 | set_correct_date(url, hdl, d_type='distributionDate', apikey=None) Sets \u201ccorrect\u201d publication date for Dataverse. Note: dryad2dataverse.serializer maps Dryad \u2018publicationDate\u2019 to Dataverse \u2018distributionDate\u2019 (see serializer.py ~line 675). Dataverse citation date default is \u201c:publicationDate\u201d. See Dataverse API reference: https://guides.dataverse.org/en/4.20/api/native-api.html#id54. Arguments : url : str \u2014 Base URL to Dataverse installation. hdl : str \u2014 Persistent indentifier for Dataverse study. d_type : str \u2014 Date type. One of \u2018distributionDate\u2019, \u2018productionDate\u2019, \u2018dateOfDeposit\u2019. Default \u2018distributionDate\u2019. apikey : str \u2014 Default dryad2dataverse.constants.APIKEY. upload_study \u00b6 | upload_study(url=None, apikey=None, timeout=45, **kwargs) Uploads Dryad study metadata to target Dataverse or updates existing. Supplying a targetDv kwarg creates a new study and supplying a dvpid kwarg updates a currently existing Dataverse study. Arguments : url : str \u2014 URL of Dataverse instance. Defaults to constants.DVURL. apikey : str \u2014 API key of user. Defaults to contants.APIKEY. timeout : int \u2014 timeout on POST request. KEYWORD ARGUMENTS One of these is required. Supplying both or neither raises a NoTargetError targetDv : str \u2014 Short name of target dataverse. Required if new dataset. Specify as targetDV=value. dvpid = str \u2014 Dataverse persistent ID (for updating metadata). This is not required for new uploads, specify as dvpid=value download_file \u00b6 | download_file(url, filename, tmp=None, size=None, chk=None, timeout=45) Downloads a file via requests streaming and saves to constants.TMP. returns md5sum on success and an exception on failure. Arguments : url : str \u2014 URL of download. filename : str \u2014 Output file name. timeout : int \u2014 Requests timeout. tmp : str \u2014 Temporary directory for downloads. Defaults to dryad2dataverse.constants.TMP. size : int \u2014 Reported file size in bytes. Defaults to dryad2dataverse.constants.MAX_UPLOAD. chk : str - md5 sum of file (if available and known). download_files \u00b6 | download_files(files=None) Bulk downloader for files. Arguments : files : list \u2014 Items in list can be tuples or list with a minimum of: (dryaddownloadurl, filenamewithoutpath, [md5sum]) The md5 sum should be the last member of the tuple. Defaults to self.files. Normally used without arguments to download all the associated files with a Dryad study. force_notab_unlock \u00b6 | force_notab_unlock(study, dv_url, apikey=None) Checks for a study lock and forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Arguments : study : str \u2014 Persistent indentifer of study. dv_url : str \u2014 URL to base Dataverse installation. apikey : str \u2014 API key for user. If not present authorization defaults to self.auth. upload_file \u00b6 | upload_file(dryadUrl=None, filename=None, mimetype=None, size=None, descr=None, md5=None, studyId=None, dest=None, fprefix=None, timeout=300) Uploads file to Dataverse study. Returns a tuple of the dryadFid (or None) and Dataverse JSON from the POST request. Failures produce JSON with different status messages rather than raising an exception. Arguments : filename : str \u2014 Filename (not including path). mimetype : str \u2014 Mimetype of file. size : int \u2014 Size in bytes. studyId : str \u2014 Persistent Dataverse study identifier. Defaults to Transfer.dvpid. dest : str \u2014 Destination dataverse installation url. Defaults to constants.DVURL. md5 : str \u2014 md5 checksum for file. fprefix : str \u2014 Path to file, not including a trailing slash. timeout : int - Timeout in seconds for POST request. Default 300. dryadUrl : str - Dryad download URL if you want to include a Dryad file id. upload_files \u00b6 | upload_files(files=None, pid=None, fprefix=None) Uploads multiple files to study with persistentId pid. Returns a list of the original tuples plus JSON responses. Arguments : files : list \u2014 List contains tuples with (dryadDownloadURL, filename, mimetype, size). pid : str \u2014 Defaults to self.dvpid, which is generated by calling dryad2dataverse.transfer.Transfer.upload_study(). upload_json \u00b6 | upload_json(studyId=None, dest=None) Uploads Dryad json as a separate file for archival purposes. Arguments : studyId : str \u2014 Dataverse persistent identifier. Default dryad2dataverse.transfer.Transfer.dvpid, which is only generated on dryad2dataverse.transfer.Transfer.upload_study() dest : str \u2014 Base URL for transfer. Default dryad2datavese.constants.DVURL delete_dv_file \u00b6 | delete_dv_file(dvfid, dvurl=None, key=None) Deletes files from Dataverse target given a dataverse file ID. This information is unknowable unless discovered by dryad2dataverse.monitor.Monitor or by other methods. Returns 1 on success (204 response), or 0 on other response. Arguments : dvurl : str \u2014 Base URL of dataverse instance. Defaults to dryad2dataverse.constants.DVURL. dvfid : str \u2014 Dataverse file ID number. delete_dv_files \u00b6 | delete_dv_files(dvfids=None, dvurl=None, key=None) Deletes all files in list of Dataverse file ids from a Dataverse installation. Arguments : dvfids : list \u2014 List of Dataverse file ids. Defaults to dryad2dataverse.transfer.Transfer.fileDelRecord. dvurl : str \u2014 Base URL of Dataverse. Defaults to dryad2dataverse.constants.DVURL. key : str \u2014 API key for Dataverse. Defaults to dryad2dataverse.constants.APIKEY. dryad2dataverse.monitor \u00b6 Dryad/Dataverse status tracker. Monitor creates a singleton object which writes to a SQLite database. Methods will (generally) take either a dryad2dataverse.serializer.Serializer instance or dryad2dataverse.transfer.Transfer instance The monitor\u2019s primary function is to allow for state checking for Dryad studies so that files and studies aren\u2019t downloaded unneccessarily. Monitor Objects \u00b6 class Monitor() The Monitor object is a tracker and database updater, so that Dryad files can be monitored and updated over time. Monitor is a singleton, but is not thread-safe. __new__ \u00b6 | __new__(cls, dbase=None, *args, **kwargs) Creates a new singleton instance of Monitor. Also creates a database if existing database is not present. Arguments : dbase : str \u2014 Path to sqlite3 database. That is: /path/to/file.sqlite3 __init__ \u00b6 | __init__(dbase=None, *args, **kwargs) Initialize the Monitor instance if not instantiated already (ie, Monitor is a singleton). Arguments : dbase : str \u2014 Complete path to desired location of tracking database - (eg - /tmp/test.db). Defaults to dryad2dataverse.constants.DBASE. __del__ \u00b6 | __del__() Commits all database transactions on object deletion and closes database. lastmod \u00b6 | @property | lastmod() Returns last modification date from monitor.dbase. status \u00b6 | status(serial) Returns a dictionary with keys \u2018status\u2019 and \u2018dvpid\u2019. {status :'updated', 'dvpid':'doi://some/ident'} . status is one of \u2018new\u2019, \u2018unchanged\u2019, \u2018updated\u2019 or \u2018filesonly\u2019. \u2018new\u2019 is a completely new file. \u2018unchanged\u2019 is no changes at all. \u2018updated\u2019 is changes to lastModificationDate AND metadata changes. \u2018filesonly\u2019 is changes to lastModificationDate only (which presumably indicates a file change. dvpid is a Dataverse persistent identifier. None in the case of status=\u2019new\u2019 Arguments : serial : dryad2dataverse.serializer instance diff_metadata \u00b6 | diff_metadata(serial) Analyzes differences in metadata between current serializer instance and last updated serializer instance. Returns a list of field changes consisting of: [{key: (old_value, new_value}] or None if no changes. For example: [{'title': ('Cascading effects of algal warming in a freshwater community', 'Cascading effects of algal warming in a freshwater community theatre')} ] Arguments : serial : dryad2dataverse.serializer.Serializer instance diff_files \u00b6 | diff_files(serial) Returns a dict with additions and deletions from previous Dryad to dataverse upload. Because checksums are not necessarily included in Dryad file metadata, this method uses dryad file IDs, size, or whatever is available. If dryad2dataverse.monitor.Monitor.status() indicates a change it will produce dictionary output with a list of additions or deletions, as below: {'add':[dyadfiletuples], 'delete:[dryadfiletuples]} Arguments : serial : dryad2dataverse.serializer.Serializer instance get_dv_fid \u00b6 | get_dv_fid(url) Returns str \u2014 the Dataverse file ID from parsing a Dryad file download link. Normally used for determining dataverse file ids for deletion in case of dryad file changes. Arguments : url : str \u2014 Dryad file URL in form of \u2018https://datadryad.org/api/v2/files/385819/download\u2019. get_dv_fids \u00b6 | get_dv_fids(filelist) Returns Dataverse file IDs from a list of Dryad file tuples. Generally, you would use the output from dryad2dataverse.monitor.Monitor.diff_files[\u2018delete\u2019] to discover Dataverse file ids for deletion. Arguments : filelist : list \u2014 List of Dryad file tuples: eg: [('https://datadryad.org/api/v2/files/385819/download', 'GCB_ACG_Mortality_2020.zip', 'application/x-zip-compressed', 23787587), ('https://datadryad.org/api/v2/files/385820/download', 'Readme_ACG_Mortality.txt', 'text/plain', 1350)] get_json_dvfids \u00b6 | get_json_dvfids(serial) Return a list of Dataverse file ids for Dryad JSONs which were uploaded to Dataverse. Normally used to discover the file IDs to remove Dryad JSONs which have changed. Arguments : serial : dryad2dataverse.serializer.Serializer instance update \u00b6 | update(transfer) Updates the Monitor database with information from a dryad2dataverse.transfer.Transfer instance. If a Dryad primary metadata record has changes, it will be deleted from the database. This method should be called after all transfers are completed, including Dryad JSON updates, as the last action for transfer. Arguments : transfer : dryad2dataverse.transfer.Transfer instance set_timestamp \u00b6 | set_timestamp(curdate=None) Adds current time to the database table. Can be queried and be used for subsequent checking for updates. To query last modification time, use the dataverse2dryad.monitor.Monitor.lastmod attribute. Arguments : curdate : str \u2014 UTC datetime string in the format suitable for the Dryad API. eg. 2021-01-21T21:42:40Z or .strftime(\u2018%Y-%m-%dT%H:%M:%SZ\u2019). dryad2dataverse.exceptions \u00b6 Custom exceptions for error handling. Dryad2DataverseError Objects \u00b6 class Dryad2DataverseError(Exception) Base exception class for Dryad2Dataverse errors. NoTargetError Objects \u00b6 class NoTargetError(Dryad2DataverseError) No dataverse target supplied error. DownloadSizeError Objects \u00b6 class DownloadSizeError(Dryad2DataverseError) Raised when download sizes don\u2019t match reported Dryad file size. HashError Objects \u00b6 class HashError(Dryad2DataverseError) Raised on hex digest mismatch. DatabaseError Objects \u00b6 class DatabaseError(Dryad2DataverseError) Tracking database error. DataverseUploadError Objects \u00b6 class DataverseUploadError(Dryad2DataverseError) Returned on not OK respose (ie, not requests.status_code == 200). DataverseDownloadError Objects \u00b6 class DataverseDownloadError(Dryad2DataverseError) Returned on not OK respose (ie, not requests.status_code == 200).","title":"API Reference"},{"location":"api_reference/#complete-api-reference","text":"","title":"Complete API reference"},{"location":"api_reference/#dryad2dataverse","text":"Dryad to Dataverse utilities. No modules are loaded by default, so import dryad2dataverse will work, but will have no effect. Modules included: dryad2dataverse.constants : \"Constants\" for all modules. URLs, API keys, etc are all here. dryad2dataverse.serializer : Download and serialize Dryad JSON to Dataverse JSON. dryad2dataverse.transfer : metadata and file transfer utilities. dryad2dataverse.monitor : Monitoring and database tools for maintaining a pipeline to Dataverse without unnecessary downloading and file duplication. dryad2dataverse.exceptions : Custom exceptions.","title":"dryad2dataverse"},{"location":"api_reference/#dryad2dataverseconstants","text":"This module contains the information that configures all the parameters required to transfer data from Dryad to Dataverse. \u201cConstants\u201d may be a bit strong, but the only constant is the presence of change.","title":"dryad2dataverse.constants"},{"location":"api_reference/#dryad2dataverseserializer","text":"Serializes Dryad study JSON to Dataverse JSON, as well as producing associated file information.","title":"dryad2dataverse.serializer"},{"location":"api_reference/#serializer-objects","text":"class Serializer() Serializes Dryad JSON to Dataverse JSON","title":"Serializer Objects"},{"location":"api_reference/#__init__","text":"| __init__(doi) Creates Dryad study metadata instance. Arguments : doi : str \u2014 DOI of Dryad study. Required for downloading. - eg - \u2018doi:10.5061/dryad.2rbnzs7jp\u2019","title":"__init__"},{"location":"api_reference/#fetch_record","text":"| fetch_record(url=None, timeout=45) Fetches Dryad study record JSON from Dryad V2 API at https://datadryad.org/api/v2/datasets/. Saves to self._dryadJson. Querying Serializer.dryadJson will call this function automatically. Arguments : url : str \u2014 Dryad instance base URL (eg: \u2018https://datadryad.org\u2019). timeout : int \u2014 Timeout in seconds. Default 45.","title":"fetch_record"},{"location":"api_reference/#id","text":"| @property | id() Returns Dryad unique database ID, not the DOI. Where the original Dryad JSON is dryadJson, it\u2019s the integer trailing portion of: self.dryadJson['_links']['stash:version']['href']","title":"id"},{"location":"api_reference/#dryadjson","text":"| @property | dryadJson() Returns Dryad study JSON. Will call Serializer.fetch_record() if no JSON is present.","title":"dryadJson"},{"location":"api_reference/#dryadjson_1","text":"| @dryadJson.setter | dryadJson(value=None) Fetches Dryad JSON from Dryad website if not supplied. If supplying it, make sure it\u2019s correct or you will run into trouble with processing later. Arguments : value : dict \u2014 Dryad JSON.","title":"dryadJson"},{"location":"api_reference/#embargo","text":"| @property | embargo() Check embargo status. Returns boolean True if embargoed.","title":"embargo"},{"location":"api_reference/#dvjson","text":"| @property | dvJson() Returns Dataverse study JSON as dict.","title":"dvJson"},{"location":"api_reference/#filejson","text":"| @property | fileJson(timeout=45) Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated, so the return consists of a list of dicts, one per page. Arguments : timeout : int \u2014 Request timeout in seconds.","title":"fileJson"},{"location":"api_reference/#files","text":"| @property | files() Returns a list of tuples with: (Download_location, filename, mimetype, size, description, digestType, md5sum) At this time only md5 is supported.","title":"files"},{"location":"api_reference/#oversize","text":"| @property | oversize(maxsize=None) Returns a list of Dryad files whose size value exceeds maxsize. Maximum size defaults to dryad2dataverse.constants.MAX_UPLOAD Arguments : maxsize : int \u2014 Size in bytes in which to flag as oversize. Defaults to constants.MAX_UPLOAD.","title":"oversize"},{"location":"api_reference/#dryad2dataversetransfer","text":"This module handles data downloads and uploads from a Dryad instance to a Dataverse instance","title":"dryad2dataverse.transfer"},{"location":"api_reference/#transfer-objects","text":"class Transfer() Transfers metadata and data files from a Dryad installation to Dataverse installation.","title":"Transfer Objects"},{"location":"api_reference/#__init___1","text":"| __init__(dryad) Creates a dryad2dataverse.transfer.Transfer instance. Arguments : dryad : dryad2dataverse.serializer.Serializer instance","title":"__init__"},{"location":"api_reference/#_del__","text":"| _del__() Expunges files from constants.TMP on deletion","title":"_del__"},{"location":"api_reference/#dvpid","text":"| @property | dvpid() Returns Dataverse study persistent ID as str.","title":"dvpid"},{"location":"api_reference/#auth","text":"| @property | auth() Returns datavese authentication header dict. ie: {X-Dataverse-key' : 'APIKEYSTRING'}","title":"auth"},{"location":"api_reference/#filejson_1","text":"| @property | fileJson() Returns a list of file JSONs from call to Dryad API /files/{id}, where the ID is parsed from the Dryad JSON. Dryad file listings are paginated.","title":"fileJson"},{"location":"api_reference/#files_1","text":"| @property | files() Returns a list of lists with: [Download_location, filename, mimetype, size, description, md5digest] This is mutable; downloading a file will add md5 info if not available.","title":"files"},{"location":"api_reference/#oversize_1","text":"| @property | oversize() Returns list of files exceeding Dataverse ingest limit dryad2dataverse.constants.MAX_UPLOAD.","title":"oversize"},{"location":"api_reference/#doi","text":"| @property | doi() Returns Dryad DOI.","title":"doi"},{"location":"api_reference/#set_correct_date","text":"| set_correct_date(url, hdl, d_type='distributionDate', apikey=None) Sets \u201ccorrect\u201d publication date for Dataverse. Note: dryad2dataverse.serializer maps Dryad \u2018publicationDate\u2019 to Dataverse \u2018distributionDate\u2019 (see serializer.py ~line 675). Dataverse citation date default is \u201c:publicationDate\u201d. See Dataverse API reference: https://guides.dataverse.org/en/4.20/api/native-api.html#id54. Arguments : url : str \u2014 Base URL to Dataverse installation. hdl : str \u2014 Persistent indentifier for Dataverse study. d_type : str \u2014 Date type. One of \u2018distributionDate\u2019, \u2018productionDate\u2019, \u2018dateOfDeposit\u2019. Default \u2018distributionDate\u2019. apikey : str \u2014 Default dryad2dataverse.constants.APIKEY.","title":"set_correct_date"},{"location":"api_reference/#upload_study","text":"| upload_study(url=None, apikey=None, timeout=45, **kwargs) Uploads Dryad study metadata to target Dataverse or updates existing. Supplying a targetDv kwarg creates a new study and supplying a dvpid kwarg updates a currently existing Dataverse study. Arguments : url : str \u2014 URL of Dataverse instance. Defaults to constants.DVURL. apikey : str \u2014 API key of user. Defaults to contants.APIKEY. timeout : int \u2014 timeout on POST request. KEYWORD ARGUMENTS One of these is required. Supplying both or neither raises a NoTargetError targetDv : str \u2014 Short name of target dataverse. Required if new dataset. Specify as targetDV=value. dvpid = str \u2014 Dataverse persistent ID (for updating metadata). This is not required for new uploads, specify as dvpid=value","title":"upload_study"},{"location":"api_reference/#download_file","text":"| download_file(url, filename, tmp=None, size=None, chk=None, timeout=45) Downloads a file via requests streaming and saves to constants.TMP. returns md5sum on success and an exception on failure. Arguments : url : str \u2014 URL of download. filename : str \u2014 Output file name. timeout : int \u2014 Requests timeout. tmp : str \u2014 Temporary directory for downloads. Defaults to dryad2dataverse.constants.TMP. size : int \u2014 Reported file size in bytes. Defaults to dryad2dataverse.constants.MAX_UPLOAD. chk : str - md5 sum of file (if available and known).","title":"download_file"},{"location":"api_reference/#download_files","text":"| download_files(files=None) Bulk downloader for files. Arguments : files : list \u2014 Items in list can be tuples or list with a minimum of: (dryaddownloadurl, filenamewithoutpath, [md5sum]) The md5 sum should be the last member of the tuple. Defaults to self.files. Normally used without arguments to download all the associated files with a Dryad study.","title":"download_files"},{"location":"api_reference/#force_notab_unlock","text":"| force_notab_unlock(study, dv_url, apikey=None) Checks for a study lock and forcibly unlocks and uningests to prevent tabular file processing. Required if mime and filename spoofing is not sufficient. Arguments : study : str \u2014 Persistent indentifer of study. dv_url : str \u2014 URL to base Dataverse installation. apikey : str \u2014 API key for user. If not present authorization defaults to self.auth.","title":"force_notab_unlock"},{"location":"api_reference/#upload_file","text":"| upload_file(dryadUrl=None, filename=None, mimetype=None, size=None, descr=None, md5=None, studyId=None, dest=None, fprefix=None, timeout=300) Uploads file to Dataverse study. Returns a tuple of the dryadFid (or None) and Dataverse JSON from the POST request. Failures produce JSON with different status messages rather than raising an exception. Arguments : filename : str \u2014 Filename (not including path). mimetype : str \u2014 Mimetype of file. size : int \u2014 Size in bytes. studyId : str \u2014 Persistent Dataverse study identifier. Defaults to Transfer.dvpid. dest : str \u2014 Destination dataverse installation url. Defaults to constants.DVURL. md5 : str \u2014 md5 checksum for file. fprefix : str \u2014 Path to file, not including a trailing slash. timeout : int - Timeout in seconds for POST request. Default 300. dryadUrl : str - Dryad download URL if you want to include a Dryad file id.","title":"upload_file"},{"location":"api_reference/#upload_files","text":"| upload_files(files=None, pid=None, fprefix=None) Uploads multiple files to study with persistentId pid. Returns a list of the original tuples plus JSON responses. Arguments : files : list \u2014 List contains tuples with (dryadDownloadURL, filename, mimetype, size). pid : str \u2014 Defaults to self.dvpid, which is generated by calling dryad2dataverse.transfer.Transfer.upload_study().","title":"upload_files"},{"location":"api_reference/#upload_json","text":"| upload_json(studyId=None, dest=None) Uploads Dryad json as a separate file for archival purposes. Arguments : studyId : str \u2014 Dataverse persistent identifier. Default dryad2dataverse.transfer.Transfer.dvpid, which is only generated on dryad2dataverse.transfer.Transfer.upload_study() dest : str \u2014 Base URL for transfer. Default dryad2datavese.constants.DVURL","title":"upload_json"},{"location":"api_reference/#delete_dv_file","text":"| delete_dv_file(dvfid, dvurl=None, key=None) Deletes files from Dataverse target given a dataverse file ID. This information is unknowable unless discovered by dryad2dataverse.monitor.Monitor or by other methods. Returns 1 on success (204 response), or 0 on other response. Arguments : dvurl : str \u2014 Base URL of dataverse instance. Defaults to dryad2dataverse.constants.DVURL. dvfid : str \u2014 Dataverse file ID number.","title":"delete_dv_file"},{"location":"api_reference/#delete_dv_files","text":"| delete_dv_files(dvfids=None, dvurl=None, key=None) Deletes all files in list of Dataverse file ids from a Dataverse installation. Arguments : dvfids : list \u2014 List of Dataverse file ids. Defaults to dryad2dataverse.transfer.Transfer.fileDelRecord. dvurl : str \u2014 Base URL of Dataverse. Defaults to dryad2dataverse.constants.DVURL. key : str \u2014 API key for Dataverse. Defaults to dryad2dataverse.constants.APIKEY.","title":"delete_dv_files"},{"location":"api_reference/#dryad2dataversemonitor","text":"Dryad/Dataverse status tracker. Monitor creates a singleton object which writes to a SQLite database. Methods will (generally) take either a dryad2dataverse.serializer.Serializer instance or dryad2dataverse.transfer.Transfer instance The monitor\u2019s primary function is to allow for state checking for Dryad studies so that files and studies aren\u2019t downloaded unneccessarily.","title":"dryad2dataverse.monitor"},{"location":"api_reference/#monitor-objects","text":"class Monitor() The Monitor object is a tracker and database updater, so that Dryad files can be monitored and updated over time. Monitor is a singleton, but is not thread-safe.","title":"Monitor Objects"},{"location":"api_reference/#__new__","text":"| __new__(cls, dbase=None, *args, **kwargs) Creates a new singleton instance of Monitor. Also creates a database if existing database is not present. Arguments : dbase : str \u2014 Path to sqlite3 database. That is: /path/to/file.sqlite3","title":"__new__"},{"location":"api_reference/#__init___2","text":"| __init__(dbase=None, *args, **kwargs) Initialize the Monitor instance if not instantiated already (ie, Monitor is a singleton). Arguments : dbase : str \u2014 Complete path to desired location of tracking database - (eg - /tmp/test.db). Defaults to dryad2dataverse.constants.DBASE.","title":"__init__"},{"location":"api_reference/#__del__","text":"| __del__() Commits all database transactions on object deletion and closes database.","title":"__del__"},{"location":"api_reference/#lastmod","text":"| @property | lastmod() Returns last modification date from monitor.dbase.","title":"lastmod"},{"location":"api_reference/#status","text":"| status(serial) Returns a dictionary with keys \u2018status\u2019 and \u2018dvpid\u2019. {status :'updated', 'dvpid':'doi://some/ident'} . status is one of \u2018new\u2019, \u2018unchanged\u2019, \u2018updated\u2019 or \u2018filesonly\u2019. \u2018new\u2019 is a completely new file. \u2018unchanged\u2019 is no changes at all. \u2018updated\u2019 is changes to lastModificationDate AND metadata changes. \u2018filesonly\u2019 is changes to lastModificationDate only (which presumably indicates a file change. dvpid is a Dataverse persistent identifier. None in the case of status=\u2019new\u2019 Arguments : serial : dryad2dataverse.serializer instance","title":"status"},{"location":"api_reference/#diff_metadata","text":"| diff_metadata(serial) Analyzes differences in metadata between current serializer instance and last updated serializer instance. Returns a list of field changes consisting of: [{key: (old_value, new_value}] or None if no changes. For example: [{'title': ('Cascading effects of algal warming in a freshwater community', 'Cascading effects of algal warming in a freshwater community theatre')} ] Arguments : serial : dryad2dataverse.serializer.Serializer instance","title":"diff_metadata"},{"location":"api_reference/#diff_files","text":"| diff_files(serial) Returns a dict with additions and deletions from previous Dryad to dataverse upload. Because checksums are not necessarily included in Dryad file metadata, this method uses dryad file IDs, size, or whatever is available. If dryad2dataverse.monitor.Monitor.status() indicates a change it will produce dictionary output with a list of additions or deletions, as below: {'add':[dyadfiletuples], 'delete:[dryadfiletuples]} Arguments : serial : dryad2dataverse.serializer.Serializer instance","title":"diff_files"},{"location":"api_reference/#get_dv_fid","text":"| get_dv_fid(url) Returns str \u2014 the Dataverse file ID from parsing a Dryad file download link. Normally used for determining dataverse file ids for deletion in case of dryad file changes. Arguments : url : str \u2014 Dryad file URL in form of \u2018https://datadryad.org/api/v2/files/385819/download\u2019.","title":"get_dv_fid"},{"location":"api_reference/#get_dv_fids","text":"| get_dv_fids(filelist) Returns Dataverse file IDs from a list of Dryad file tuples. Generally, you would use the output from dryad2dataverse.monitor.Monitor.diff_files[\u2018delete\u2019] to discover Dataverse file ids for deletion. Arguments : filelist : list \u2014 List of Dryad file tuples: eg: [('https://datadryad.org/api/v2/files/385819/download', 'GCB_ACG_Mortality_2020.zip', 'application/x-zip-compressed', 23787587), ('https://datadryad.org/api/v2/files/385820/download', 'Readme_ACG_Mortality.txt', 'text/plain', 1350)]","title":"get_dv_fids"},{"location":"api_reference/#get_json_dvfids","text":"| get_json_dvfids(serial) Return a list of Dataverse file ids for Dryad JSONs which were uploaded to Dataverse. Normally used to discover the file IDs to remove Dryad JSONs which have changed. Arguments : serial : dryad2dataverse.serializer.Serializer instance","title":"get_json_dvfids"},{"location":"api_reference/#update","text":"| update(transfer) Updates the Monitor database with information from a dryad2dataverse.transfer.Transfer instance. If a Dryad primary metadata record has changes, it will be deleted from the database. This method should be called after all transfers are completed, including Dryad JSON updates, as the last action for transfer. Arguments : transfer : dryad2dataverse.transfer.Transfer instance","title":"update"},{"location":"api_reference/#set_timestamp","text":"| set_timestamp(curdate=None) Adds current time to the database table. Can be queried and be used for subsequent checking for updates. To query last modification time, use the dataverse2dryad.monitor.Monitor.lastmod attribute. Arguments : curdate : str \u2014 UTC datetime string in the format suitable for the Dryad API. eg. 2021-01-21T21:42:40Z or .strftime(\u2018%Y-%m-%dT%H:%M:%SZ\u2019).","title":"set_timestamp"},{"location":"api_reference/#dryad2dataverseexceptions","text":"Custom exceptions for error handling.","title":"dryad2dataverse.exceptions"},{"location":"api_reference/#dryad2dataverseerror-objects","text":"class Dryad2DataverseError(Exception) Base exception class for Dryad2Dataverse errors.","title":"Dryad2DataverseError Objects"},{"location":"api_reference/#notargeterror-objects","text":"class NoTargetError(Dryad2DataverseError) No dataverse target supplied error.","title":"NoTargetError Objects"},{"location":"api_reference/#downloadsizeerror-objects","text":"class DownloadSizeError(Dryad2DataverseError) Raised when download sizes don\u2019t match reported Dryad file size.","title":"DownloadSizeError Objects"},{"location":"api_reference/#hasherror-objects","text":"class HashError(Dryad2DataverseError) Raised on hex digest mismatch.","title":"HashError Objects"},{"location":"api_reference/#databaseerror-objects","text":"class DatabaseError(Dryad2DataverseError) Tracking database error.","title":"DatabaseError Objects"},{"location":"api_reference/#dataverseuploaderror-objects","text":"class DataverseUploadError(Dryad2DataverseError) Returned on not OK respose (ie, not requests.status_code == 200).","title":"DataverseUploadError Objects"},{"location":"api_reference/#dataversedownloaderror-objects","text":"class DataverseDownloadError(Dryad2DataverseError) Returned on not OK respose (ie, not requests.status_code == 200).","title":"DataverseDownloadError Objects"},{"location":"credits/","text":"Credits \u00b6 Contact \u00b6 dryad2dataverse was written by Paul Lesack of the University of British Columbia Library Research Commons . Acknowledgements \u00b6 No software development is done in a vacuum, and this project is no exception. Thanks to Eugene Barsky and Doug Brigham of the University of British Columbia library Research Commons for their assistance and support, to Ryan Scherle of Dryad for his help with the Dryad API, and the helpful people at Dataverse . Without the fabulous requests library and the requests toolbelt everything would have taken a great deal longer. Without pydoc-markdown , mkdocs and schemaspy the documentation would have taken much, much longer to write.","title":"Credits"},{"location":"credits/#credits","text":"","title":"Credits"},{"location":"credits/#contact","text":"dryad2dataverse was written by Paul Lesack of the University of British Columbia Library Research Commons .","title":"Contact"},{"location":"credits/#acknowledgements","text":"No software development is done in a vacuum, and this project is no exception. Thanks to Eugene Barsky and Doug Brigham of the University of British Columbia library Research Commons for their assistance and support, to Ryan Scherle of Dryad for his help with the Dryad API, and the helpful people at Dataverse . Without the fabulous requests library and the requests toolbelt everything would have taken a great deal longer. Without pydoc-markdown , mkdocs and schemaspy the documentation would have taken much, much longer to write.","title":"Acknowledgements"},{"location":"faq/","text":"Frequently asked questions \u00b6 Why is the upload script (dryadd.py) constantly crashing with SMTP errors? \u00b6 If you are using Gmail to send messages about your migration, there are a few potential hurdles. You must enable less secure app access . Even when you do that, it can still crash with no obvious explanation or a mysterious authentication error. In this case, the script may be encountering a Captcha security measure. You can remove this by going to https://accounts.google.com/DisplayUnlockCaptcha before running your script (when logged into the account which you are using, of course). The settings revert back to normal after some period of time of which I am not aware. Daily or weekly updates should be OK, but monthly ones will probably fail with SMTP errors as the service reverts to defaults. Your other option is to not use Gmail. smtplib exceptions will cause a script crash, so if you are experiencing persistent mail problems and still wish to use the script, you may wish disable emailing log messages. This is easily accomplished by commenting out the section starting with elog = email_log( in scripts/dryadd.py . Obviously you can\u2019t do this if you\u2019re using a binary dryadd. Currently email notifications are a mandatory part of the dryadd.py app, but this may be optional and/or more advanced mail handling may be available in later releases. All error messages are written to the log anyway, so if you disable emailing of log messages you can still see them in the transfer log. Why is my transfer to Dataverse not showing up as published? \u00b6 dryad2dataverse does not publish the dataset. That must still be done via the Dataverse GUI or API. Publication functionality has been omitted by design : File size limits within a default Dataverse installation that do not apply to Dryad, so it\u2019s possible that some files need to be moved with the assistance of a Datverse system administrator Although every attempt has been made to map metadata schemas appropriately, it\u2019s undoubtedly not perfect. A quick once-over by a human can notice any unusual or unforeseen errors Metadata quality standards can vary between Dryad and a Dataverse installations. A manual curation step is sometimes desirable to ensure that all published records meet the same standards. But I don\u2019t want to manually curate my data \u00b6 It\u2019s possible to publish via the Dataverse API. If you really want to publish automatically, you can obtain a list of unpublished studies from Dataverse and publish them programatically. This is left as an exercise for the reader. Why does my large file download/upload fail? \u00b6 By default, Dataverse limits file sizes to 3 Gb, but that can vary by installation. dryad2dataverse.constants.MAX_UPLOAD contains the value which should correspond to the maximum upload size in Dataverse. If you don\u2019t know what the upload size is, contact the system administrator of your target Dataverse installation to find out. To upload files exceeding the API upload limit, you will need to speak to a Dataverse administrator. Why does my upload of files fail halfway? \u00b6 Dataverse will automatically cease ingest and lock a study when encountering a file which is suitable for tabular processing. The only way to stop this behaviour is to prohibit ingest in the Dataverse configuration, which is probably not possible for many users of the software. To circumvent this, dryad2dataverse attempts to fool Dataverse into not processing the tabular file, by changing the extension or MIME type at upload time. If this doesn\u2019t work and tabular processing starts anyway, the study is forcibly unlocked to allow uploads to continue. This process, unfortunately, is [probably] not foolproof. Why is a file which should be a tabular file not a tabular file? \u00b6 As a direct result of the above, tabular file processing has (hopefully) been eliminated. It\u2019s still possible to create a tabular file by reingesting it. Unless you are are the administrator of a Dataverse installation, you likely don\u2019t have control over what is or is not considered a tabular file. dryad2dataverse attempts to block all tabular file processing, but the process is imperfect. Sic vita. Why does the code use camel case instead of snake case for variables? \u00b6 By the time I realized I should be using snake case, it was too late and I was already consistently using camel case. https://www.python.org/dev/peps/pep-0008/#a-foolish-consistency-is-the-hobgoblin-of-little-minds","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":"Frequently asked questions"},{"location":"faq/#why-is-the-upload-script-dryaddpy-constantly-crashing-with-smtp-errors","text":"If you are using Gmail to send messages about your migration, there are a few potential hurdles. You must enable less secure app access . Even when you do that, it can still crash with no obvious explanation or a mysterious authentication error. In this case, the script may be encountering a Captcha security measure. You can remove this by going to https://accounts.google.com/DisplayUnlockCaptcha before running your script (when logged into the account which you are using, of course). The settings revert back to normal after some period of time of which I am not aware. Daily or weekly updates should be OK, but monthly ones will probably fail with SMTP errors as the service reverts to defaults. Your other option is to not use Gmail. smtplib exceptions will cause a script crash, so if you are experiencing persistent mail problems and still wish to use the script, you may wish disable emailing log messages. This is easily accomplished by commenting out the section starting with elog = email_log( in scripts/dryadd.py . Obviously you can\u2019t do this if you\u2019re using a binary dryadd. Currently email notifications are a mandatory part of the dryadd.py app, but this may be optional and/or more advanced mail handling may be available in later releases. All error messages are written to the log anyway, so if you disable emailing of log messages you can still see them in the transfer log.","title":"Why is the upload script (dryadd.py) constantly crashing with SMTP errors?"},{"location":"faq/#why-is-my-transfer-to-dataverse-not-showing-up-as-published","text":"dryad2dataverse does not publish the dataset. That must still be done via the Dataverse GUI or API. Publication functionality has been omitted by design : File size limits within a default Dataverse installation that do not apply to Dryad, so it\u2019s possible that some files need to be moved with the assistance of a Datverse system administrator Although every attempt has been made to map metadata schemas appropriately, it\u2019s undoubtedly not perfect. A quick once-over by a human can notice any unusual or unforeseen errors Metadata quality standards can vary between Dryad and a Dataverse installations. A manual curation step is sometimes desirable to ensure that all published records meet the same standards.","title":"Why is my transfer to Dataverse not showing up as published?"},{"location":"faq/#but-i-dont-want-to-manually-curate-my-data","text":"It\u2019s possible to publish via the Dataverse API. If you really want to publish automatically, you can obtain a list of unpublished studies from Dataverse and publish them programatically. This is left as an exercise for the reader.","title":"But I don't want to manually curate my data"},{"location":"faq/#why-does-my-large-file-downloadupload-fail","text":"By default, Dataverse limits file sizes to 3 Gb, but that can vary by installation. dryad2dataverse.constants.MAX_UPLOAD contains the value which should correspond to the maximum upload size in Dataverse. If you don\u2019t know what the upload size is, contact the system administrator of your target Dataverse installation to find out. To upload files exceeding the API upload limit, you will need to speak to a Dataverse administrator.","title":"Why does my large file download/upload fail?"},{"location":"faq/#why-does-my-upload-of-files-fail-halfway","text":"Dataverse will automatically cease ingest and lock a study when encountering a file which is suitable for tabular processing. The only way to stop this behaviour is to prohibit ingest in the Dataverse configuration, which is probably not possible for many users of the software. To circumvent this, dryad2dataverse attempts to fool Dataverse into not processing the tabular file, by changing the extension or MIME type at upload time. If this doesn\u2019t work and tabular processing starts anyway, the study is forcibly unlocked to allow uploads to continue. This process, unfortunately, is [probably] not foolproof.","title":"Why does my upload of files fail halfway?"},{"location":"faq/#why-is-a-file-which-should-be-a-tabular-file-not-a-tabular-file","text":"As a direct result of the above, tabular file processing has (hopefully) been eliminated. It\u2019s still possible to create a tabular file by reingesting it. Unless you are are the administrator of a Dataverse installation, you likely don\u2019t have control over what is or is not considered a tabular file. dryad2dataverse attempts to block all tabular file processing, but the process is imperfect. Sic vita.","title":"Why is a file which should be a tabular file not a tabular file?"},{"location":"faq/#why-does-the-code-use-camel-case-instead-of-snake-case-for-variables","text":"By the time I realized I should be using snake case, it was too late and I was already consistently using camel case. https://www.python.org/dev/peps/pep-0008/#a-foolish-consistency-is-the-hobgoblin-of-little-minds","title":"Why does the code use camel case instead of snake case for variables?"},{"location":"installation/","text":"Installation \u00b6 This is not a complete list of installation methods. For a complete guide to Python package installation, please see https://packaging.python.org/tutorials/installing-packages/ . Requirements \u00b6 requests >= 2.21.0 requests-toolbelt >= 0.9.1 nose >= 1.3.7 Actually, it will probably work just fine with earlier versions, but that\u2019s what development started with. Pip from source \u00b6 Download \u00b6 The source code for this project is available at https://github.com/ubc-library-rc/dryad2dataverse To install, first clone the repository: git clone https://github.com/ubc-library-rc/dryad2dataverse.git Intermediate (optional) step: using a virtual environment \u00b6 Depending on your needs, you may wish to keep dryad2dataverse in a virtual environment. This step is completely optional. In this case, you will need to perform the following steps to create your virtual environment. First, create a directory to hold your virtual environment: mkdir -p \\path\\to\\sample_venv Create the virtual environment: python3 -m venv \\path\\to\\sample_venv Finally, enable the virtual environment: source \\path\\to\\sample_venv\\bin activate Creating the virtual environment is not required if you don\u2019t mind having the prerequisites installed. More information on virtual environments can be found on the Python website: https://docs.python.org/3.6/tutorial/venv.html Installation \u00b6 Once you\u2019ve cloned the files and optionally used venv : cd dryad2dataverse pip install .","title":"Installation"},{"location":"installation/#installation","text":"This is not a complete list of installation methods. For a complete guide to Python package installation, please see https://packaging.python.org/tutorials/installing-packages/ .","title":"Installation"},{"location":"installation/#requirements","text":"requests >= 2.21.0 requests-toolbelt >= 0.9.1 nose >= 1.3.7 Actually, it will probably work just fine with earlier versions, but that\u2019s what development started with.","title":"Requirements"},{"location":"installation/#pip-from-source","text":"","title":"Pip from source"},{"location":"installation/#download","text":"The source code for this project is available at https://github.com/ubc-library-rc/dryad2dataverse To install, first clone the repository: git clone https://github.com/ubc-library-rc/dryad2dataverse.git","title":"Download"},{"location":"installation/#intermediate-optional-step-using-a-virtual-environment","text":"Depending on your needs, you may wish to keep dryad2dataverse in a virtual environment. This step is completely optional. In this case, you will need to perform the following steps to create your virtual environment. First, create a directory to hold your virtual environment: mkdir -p \\path\\to\\sample_venv Create the virtual environment: python3 -m venv \\path\\to\\sample_venv Finally, enable the virtual environment: source \\path\\to\\sample_venv\\bin activate Creating the virtual environment is not required if you don\u2019t mind having the prerequisites installed. More information on virtual environments can be found on the Python website: https://docs.python.org/3.6/tutorial/venv.html","title":"Intermediate (optional) step: using a virtual environment"},{"location":"installation/#installation_1","text":"Once you\u2019ve cloned the files and optionally used venv : cd dryad2dataverse pip install .","title":"Installation"},{"location":"reference/","text":"General Reference \u00b6 This page covers material that isn\u2019t automatically generated from the source code, that is, the API reference section. Information regarding specific modules is below. dryad2dataverse.constants \u00b6 And by \u201cconstants\u201d, you should change these as required. This module contains the information that configures all the parameters required to transfer data from Dryad to Dataverse. As \u2018constants\u2019 don\u2019t generally change, there\u2019s a non-zero chance that the name of this module will change. General variables \u00b6 RETRY_STRATEGY + This is a urllib3.util Retry object which controls the connection attemps of a requests.Session object. Not all connections are guaranteed to be successful the first time round, and the Retry object will allow multiple connection attempts before raising an exception. + Default: 10 attempts, with exponentially increased times between attempts. + For more information/a tutorial on how to use the Retry object, please see https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/#retry-on-failure DRYURL + Base URL for the Dryad data repository. + Default = \u2018https://datadryad.org\u2019 It\u2019s unlikely you will ever change this, but Dryad is an open source project, so it\u2019s not out of the realm of possibility that there will be another Dryad-style repository. TMP + Temporary file download location. Note that downloaded files have the potential of being very large, so select a directory which has sufficient space. + Default =\u2019/tmp\u2019 This is configured for *nix style environments. Windows does not, by default, have /tmp directory, for instance. Data transfer variables \u00b6 DVURL + Base URL for dataverse installation + Default = \u2018https://dataverse.scholarsportal.info\u2019 Obviously, if you are not transferring your data to the Scholars Portal , you will need to change this. APIKEY + Dataverse API key for user performing transfer. Sufficient privileges for upload and metadata manipulation must be attached to the user. See Dataverse API documentation for an explanation of the privilege level required. + Default = None To avoid issues, using an API key which has administrator privileges for the target dataverse is the easiest apprach. MAX_UPLOAD + Maximum upload file size in bytes. Files exceeding this size will be ignored. By default, Dataverse has a 3GB upload size limit + Default = 3221225472 Files will not be downloaded or uploaded if their (reported) size exceeds this limit. DV_CONTACT_EMAIL + Dataverse \u201cContact\u201d email address. Required as part of Dataverse metadata. This would generally be the email address of the data set curator + Default= None API uploads to Dataverse fail without a contact email. While dryad2dataverse attempts to read email addresses from Dryad records, they are not required in Dryad. DV_CONTACT_NAME + Dataverse \u201cContact\u201d name. Required as part of Dataverse metadata. Generally the name of the data set curator, whether individual or an organization + Default = None As with contact email addresses, contact names are required in Dataverse, but not in Dryad. NOTAB + File extensions which should have tabular processing disabled. Lower case only. + Dataverse will immediately cease ingest and lock a dataset when encountering a file which can be processed to .tab format. This causes upload crashes unless disabled. + Files may be converted to .tab format after upload using Dataverse\u2019s reingest endpoint: https://guides.dataverse.org/en/latest/api/native-api.html#reingest-a-file + Default = [\u2018.sav\u2019, \u2018.por\u2019, \u2018.zip\u2019, \u2018.csv\u2019, \u2018.tsv\u2019, \u2018.dta\u2019, \u2018.rdata\u2019, \u2018.xslx\u2019] If one of the files in the upload triggers tabular processing the upload will suddenly cease and fail. This behaviour is built into Dataverse (unfortunately), and can be only overcome through workarounds such as double-zipping files, or, in this case, spoofing MIME types and extensions. Because Dataverse\u2019s tabular file processing capabilities are subject to change, this is not an exhaustive list and some files may be processed regardless. See also dryad2dataverse.transfer.Transfer.force_notab_unlock(). Monitoring database variables \u00b6 HOME + Home directory path for user + Default = os.path.expanduser(\u2018~\u2019) Home directory for the user. There is probably no reason to change this. DBASE + Full path for transfer monitoring sqlite3 database + Default = HOME + os.sep + \u2018dryad_dataverse_monitor.sqlite3\u2019 By default, the monitoring/tracking database will be created in the user\u2019s home directory, which is convenient but not necessarily not ideal. The location can also be set on instantiation of dryad2dataverse.monitor.Monitor : eg monitor = dryad2dataverse.monitor.Monitor('/path/to/tracking/directory/databasename.sqlite3')","title":"General Reference"},{"location":"reference/#general-reference","text":"This page covers material that isn\u2019t automatically generated from the source code, that is, the API reference section. Information regarding specific modules is below.","title":"General Reference"},{"location":"reference/#dryad2dataverseconstants","text":"And by \u201cconstants\u201d, you should change these as required. This module contains the information that configures all the parameters required to transfer data from Dryad to Dataverse. As \u2018constants\u2019 don\u2019t generally change, there\u2019s a non-zero chance that the name of this module will change.","title":"dryad2dataverse.constants"},{"location":"reference/#general-variables","text":"RETRY_STRATEGY + This is a urllib3.util Retry object which controls the connection attemps of a requests.Session object. Not all connections are guaranteed to be successful the first time round, and the Retry object will allow multiple connection attempts before raising an exception. + Default: 10 attempts, with exponentially increased times between attempts. + For more information/a tutorial on how to use the Retry object, please see https://findwork.dev/blog/advanced-usage-python-requests-timeouts-retries-hooks/#retry-on-failure DRYURL + Base URL for the Dryad data repository. + Default = \u2018https://datadryad.org\u2019 It\u2019s unlikely you will ever change this, but Dryad is an open source project, so it\u2019s not out of the realm of possibility that there will be another Dryad-style repository. TMP + Temporary file download location. Note that downloaded files have the potential of being very large, so select a directory which has sufficient space. + Default =\u2019/tmp\u2019 This is configured for *nix style environments. Windows does not, by default, have /tmp directory, for instance.","title":"General variables"},{"location":"reference/#data-transfer-variables","text":"DVURL + Base URL for dataverse installation + Default = \u2018https://dataverse.scholarsportal.info\u2019 Obviously, if you are not transferring your data to the Scholars Portal , you will need to change this. APIKEY + Dataverse API key for user performing transfer. Sufficient privileges for upload and metadata manipulation must be attached to the user. See Dataverse API documentation for an explanation of the privilege level required. + Default = None To avoid issues, using an API key which has administrator privileges for the target dataverse is the easiest apprach. MAX_UPLOAD + Maximum upload file size in bytes. Files exceeding this size will be ignored. By default, Dataverse has a 3GB upload size limit + Default = 3221225472 Files will not be downloaded or uploaded if their (reported) size exceeds this limit. DV_CONTACT_EMAIL + Dataverse \u201cContact\u201d email address. Required as part of Dataverse metadata. This would generally be the email address of the data set curator + Default= None API uploads to Dataverse fail without a contact email. While dryad2dataverse attempts to read email addresses from Dryad records, they are not required in Dryad. DV_CONTACT_NAME + Dataverse \u201cContact\u201d name. Required as part of Dataverse metadata. Generally the name of the data set curator, whether individual or an organization + Default = None As with contact email addresses, contact names are required in Dataverse, but not in Dryad. NOTAB + File extensions which should have tabular processing disabled. Lower case only. + Dataverse will immediately cease ingest and lock a dataset when encountering a file which can be processed to .tab format. This causes upload crashes unless disabled. + Files may be converted to .tab format after upload using Dataverse\u2019s reingest endpoint: https://guides.dataverse.org/en/latest/api/native-api.html#reingest-a-file + Default = [\u2018.sav\u2019, \u2018.por\u2019, \u2018.zip\u2019, \u2018.csv\u2019, \u2018.tsv\u2019, \u2018.dta\u2019, \u2018.rdata\u2019, \u2018.xslx\u2019] If one of the files in the upload triggers tabular processing the upload will suddenly cease and fail. This behaviour is built into Dataverse (unfortunately), and can be only overcome through workarounds such as double-zipping files, or, in this case, spoofing MIME types and extensions. Because Dataverse\u2019s tabular file processing capabilities are subject to change, this is not an exhaustive list and some files may be processed regardless. See also dryad2dataverse.transfer.Transfer.force_notab_unlock().","title":"Data transfer variables"},{"location":"reference/#monitoring-database-variables","text":"HOME + Home directory path for user + Default = os.path.expanduser(\u2018~\u2019) Home directory for the user. There is probably no reason to change this. DBASE + Full path for transfer monitoring sqlite3 database + Default = HOME + os.sep + \u2018dryad_dataverse_monitor.sqlite3\u2019 By default, the monitoring/tracking database will be created in the user\u2019s home directory, which is convenient but not necessarily not ideal. The location can also be set on instantiation of dryad2dataverse.monitor.Monitor : eg monitor = dryad2dataverse.monitor.Monitor('/path/to/tracking/directory/databasename.sqlite3')","title":"Monitoring database variables"},{"location":"scripts/","text":"Automated migrator and tracker - dryadd.py \u00b6 While it\u2019s all very nice that there\u2019s code that can migrate Dryad material to Dataverse, many users are not familiar enough with Python/programming or, just as likely, don\u2019t want to have to program things themselves. Anyone transferring from Dryad to Dataverse is likely doing a variant of the same thing, which consists of: Finding new Dryad material, usually from their own institution Moving it to Dataverse and possibly: Checking for updates and handling those automatically Included with dryad2dataverse is a script and possibly binary files which do exactly this. The binary files, if available for your operating system, should not even require a Python installation; they are self-contained programs which will run and monitor the copying process. An important caveat \u00b6 This product will not publish anything in a Dataverse installation (at this time, at least). This is intentional to allow a human-based curatorial step before releasing any data onto an unsuspecting audience. There\u2019s no error like systemic error, so not automatically releasing material should help alleviate this. Requirements \u00b6 Software If you are using the pure Python version of the migrator (ie, dryadd.py ) and you have successfully installed the dryad2dataverse library, the requirements will be filled by default (see the installation document for more details). If using a binary file, it must be supported by your operating system and system architecture (eg. Intel Mac). Hardware You will need sufficient storage space on your system to hold the contents of the largest Dryad record that you are transferring. This is not necessarily a small amount; Dryad studies can range into the tens or hundreds of Gb, which means that a \u201cnormal\u201d /tmp directory will normally not have enough space allocated to it. Other A destination Dataverse must exist, and you should know its short name. The API key must have sufficient privileges to create new studies and upload data. You will need an email address for contact information as this is a required field in Dataverse (but not necessarily in Dryad) and a name to go with it. For example, i_heart_data@test.invalid and Dataverse Support . Note: Use a valid email address (unlike the example) because uploads will also fail if the address is invalid. Information for an email address which sends notifications For this, you need the user name (\u201cuser\u201d from \u201cuser@test.invalid\u201d) The password for this account The smtp server address which sends mail. For example, if using gmail, it\u2019s smtp.gmail.com The port required to send email via SSL. At least one email address to receive update and error notifications. This can be the same as the sender. A place to store your sqlite3 tracking database. A note about GMail Although the script is set up to use GMail by default, it likely won\u2019t work off the bat. You will need to allow less secure app access and possibly deal with a capture as outlined in the FAQ . Miscellaneous This software is still new; it doesn\u2019t actually run as a daemon [yet]. To update, just run the script again at whatever interval you desire, and it will find Dryad material that has been updated since the last run. At this time, the best solution would be to run dryadd.py at predifined intervals using cron . To act as a backup against catastrophic error, the database is automatically copied to $DBASE.bak. Obviously, if you check once a minute this isn\u2019t helpful, but it could be if you update once a month. Usage \u00b6 The implementation is relatively straightforward. Simply supply the required parameters and the software should do the rest. The help menu below is available from the command line by either running the script without inputs or by using the -h switch. usage: dryadd.py [-h] -u URL -k KEY -t TARGET -e USER -r RECIPIENTS [RECIPIENTS ...] -p PWD [--server MAILSERV] [--port PORT] -c CONTACT -n CNAME -i ROR [--tmpfile TMP] [--db DBASE] [--log LOG] Dryad to Dataverse import daemon. All arguments NOT enclosed by square brackets are REQUIRED. Arguments in [square brackets] are not required. The \"optional arguments\" below refers to the use of the option switch, (like -u), meaning \"not a positional argument.\" optional arguments: -h, --help show this help message and exit -u URL, --dv-url URL REQUIRED: Destination dataverse root url. Default: https://dataverse.scholarsportal.info -k KEY, --key KEY REQUIRED: API key for dataverse user -t TARGET, --target TARGET REQUIRED: Target dataverse short name -e USER, --email USER REQUIRED: Username for email address which sends update notifications. ie, the \"user\" portion of \"user@website.invalid\". -r RECIPIENTS [RECIPIENTS ...], --recipient RECIPIENTS [RECIPIENTS ...] REQUIRED: Recipient(s) of email notification. Separate addresses with spaces -p PWD, --pwd PWD REQUIRED: Password for sending email account. Enclose in single quotes to avoid OS errors with special characters. --server MAILSERV Mail server for sending account. Default: smtp.gmail.com --port PORT Mail server port. Default: 465. Mail is sent using SSL. -c CONTACT, --contact CONTACT REQUIRED: Contact email address for Dataverse records. Must pass Dataverse email validation rules (so \"test@test.invalid\" is not acceptable). -n CNAME, --contact-name CNAME REQUIRED: Contact name for Dataverse records -i ROR, --ror ROR REQUIRED: Institutional ROR URL. Eg: \"https://ror.org/03rmrcq20\". This identifies the institution in Dryad repositories. --tmpfile TMP Temporary file location. Default: /tmp) --db DBASE Tracking database location and name. Default: $HOME/dryad_dataverse_monitor.sqlite3 --log LOG Complete path to log. Default: /var/log/dryadd.log","title":"Scripts"},{"location":"scripts/#automated-migrator-and-tracker-dryaddpy","text":"While it\u2019s all very nice that there\u2019s code that can migrate Dryad material to Dataverse, many users are not familiar enough with Python/programming or, just as likely, don\u2019t want to have to program things themselves. Anyone transferring from Dryad to Dataverse is likely doing a variant of the same thing, which consists of: Finding new Dryad material, usually from their own institution Moving it to Dataverse and possibly: Checking for updates and handling those automatically Included with dryad2dataverse is a script and possibly binary files which do exactly this. The binary files, if available for your operating system, should not even require a Python installation; they are self-contained programs which will run and monitor the copying process.","title":"Automated migrator and tracker - dryadd.py"},{"location":"scripts/#an-important-caveat","text":"This product will not publish anything in a Dataverse installation (at this time, at least). This is intentional to allow a human-based curatorial step before releasing any data onto an unsuspecting audience. There\u2019s no error like systemic error, so not automatically releasing material should help alleviate this.","title":"An important caveat"},{"location":"scripts/#requirements","text":"Software If you are using the pure Python version of the migrator (ie, dryadd.py ) and you have successfully installed the dryad2dataverse library, the requirements will be filled by default (see the installation document for more details). If using a binary file, it must be supported by your operating system and system architecture (eg. Intel Mac). Hardware You will need sufficient storage space on your system to hold the contents of the largest Dryad record that you are transferring. This is not necessarily a small amount; Dryad studies can range into the tens or hundreds of Gb, which means that a \u201cnormal\u201d /tmp directory will normally not have enough space allocated to it. Other A destination Dataverse must exist, and you should know its short name. The API key must have sufficient privileges to create new studies and upload data. You will need an email address for contact information as this is a required field in Dataverse (but not necessarily in Dryad) and a name to go with it. For example, i_heart_data@test.invalid and Dataverse Support . Note: Use a valid email address (unlike the example) because uploads will also fail if the address is invalid. Information for an email address which sends notifications For this, you need the user name (\u201cuser\u201d from \u201cuser@test.invalid\u201d) The password for this account The smtp server address which sends mail. For example, if using gmail, it\u2019s smtp.gmail.com The port required to send email via SSL. At least one email address to receive update and error notifications. This can be the same as the sender. A place to store your sqlite3 tracking database. A note about GMail Although the script is set up to use GMail by default, it likely won\u2019t work off the bat. You will need to allow less secure app access and possibly deal with a capture as outlined in the FAQ . Miscellaneous This software is still new; it doesn\u2019t actually run as a daemon [yet]. To update, just run the script again at whatever interval you desire, and it will find Dryad material that has been updated since the last run. At this time, the best solution would be to run dryadd.py at predifined intervals using cron . To act as a backup against catastrophic error, the database is automatically copied to $DBASE.bak. Obviously, if you check once a minute this isn\u2019t helpful, but it could be if you update once a month.","title":"Requirements"},{"location":"scripts/#usage","text":"The implementation is relatively straightforward. Simply supply the required parameters and the software should do the rest. The help menu below is available from the command line by either running the script without inputs or by using the -h switch. usage: dryadd.py [-h] -u URL -k KEY -t TARGET -e USER -r RECIPIENTS [RECIPIENTS ...] -p PWD [--server MAILSERV] [--port PORT] -c CONTACT -n CNAME -i ROR [--tmpfile TMP] [--db DBASE] [--log LOG] Dryad to Dataverse import daemon. All arguments NOT enclosed by square brackets are REQUIRED. Arguments in [square brackets] are not required. The \"optional arguments\" below refers to the use of the option switch, (like -u), meaning \"not a positional argument.\" optional arguments: -h, --help show this help message and exit -u URL, --dv-url URL REQUIRED: Destination dataverse root url. Default: https://dataverse.scholarsportal.info -k KEY, --key KEY REQUIRED: API key for dataverse user -t TARGET, --target TARGET REQUIRED: Target dataverse short name -e USER, --email USER REQUIRED: Username for email address which sends update notifications. ie, the \"user\" portion of \"user@website.invalid\". -r RECIPIENTS [RECIPIENTS ...], --recipient RECIPIENTS [RECIPIENTS ...] REQUIRED: Recipient(s) of email notification. Separate addresses with spaces -p PWD, --pwd PWD REQUIRED: Password for sending email account. Enclose in single quotes to avoid OS errors with special characters. --server MAILSERV Mail server for sending account. Default: smtp.gmail.com --port PORT Mail server port. Default: 465. Mail is sent using SSL. -c CONTACT, --contact CONTACT REQUIRED: Contact email address for Dataverse records. Must pass Dataverse email validation rules (so \"test@test.invalid\" is not acceptable). -n CNAME, --contact-name CNAME REQUIRED: Contact name for Dataverse records -i ROR, --ror ROR REQUIRED: Institutional ROR URL. Eg: \"https://ror.org/03rmrcq20\". This identifies the institution in Dryad repositories. --tmpfile TMP Temporary file location. Default: /tmp) --db DBASE Tracking database location and name. Default: $HOME/dryad_dataverse_monitor.sqlite3 --log LOG Complete path to log. Default: /var/log/dryadd.log","title":"Usage"},{"location":"track/","text":"The tracking database documentation was automatically generated by SchemasSpy and doesn\u2019t fit in nicely with the structure of this documentation. The link below will open in a new page/tab. Tracking database information","title":"Tracking Database Structure"}]}